{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BSBR: Block Sparse Attention with Block Retrieval","text":"<p>BSBR (Block Sparse Attention with Block Retrieval) is a novel attention mechanism for efficient processing of long sequences in transformer architectures. It combines standard attention within chunks and block retrieval between chunks to achieve near-linear complexity while maintaining high model expressivity.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udd04 Efficient Processing: Near-linear complexity in sequence length</li> <li>\ud83e\udde9 Chunk-Based Attention: Standard attention within chunks</li> <li>\ud83d\udd0d Block Retrieval: Efficient information retrieval between chunks</li> <li>\ud83c\udfaf Configurable: Adjustable chunk size and compression</li> <li>\ud83d\udcbe Memory Efficient: Optimized memory usage for long sequences</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\n\n# Model configuration\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    compression_factor=4  # Optional compression\n)\n\n# Input data\ninput_ids = torch.randint(0, 10000, (2, 256))\nattention_mask = torch.ones(2, 256)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code># Install the core package\npip install bsbr\n\n# Install with extras for evaluations and research\npip install \"bsbr[extras]\"\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>User Guide</li> <li>API Reference</li> <li>Examples</li> <li>Research</li> </ul>"},{"location":"#research","title":"Research","text":"<p>BSBR is based on research presented in our paper BSBR: Block Sparse Attention with Block Retrieval for Efficient Long-Context Reasoning. The implementation is inspired by Shengding Hu's blog post Streaming models for efficient long-context reasoning.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guidelines for details.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details. </p>"},{"location":"api/bsbr/","title":"BSBR Module","text":""},{"location":"api/bsbr/#bsbrmodel","title":"BSBRModel","text":"<pre><code>class BSBRModel(nn.Module):\n    \"\"\"Block Sparse Attention with Block Retrieval model.\n\n    A transformer model that combines standard attention within chunks with efficient\n    block retrieval between chunks for processing long sequences.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of chunks for block sparse attention\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n        compression_factor (int, optional): Factor to compress chunk states. Defaults to 1.\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#methods","title":"Methods","text":""},{"location":"api/bsbr/#forward","title":"forward","text":"<pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs of shape (batch_size, seq_len)\n        attention_mask (torch.Tensor, optional): Attention mask of shape (batch_size, seq_len).\n            Defaults to None.\n        **kwargs: Additional arguments passed to the model.\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, seq_len, vocab_size)\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#bsbrattention","title":"BSBRAttention","text":"<pre><code>class BSBRAttention(nn.Module):\n    \"\"\"Block Sparse Attention with Block Retrieval.\n\n    Implements the core attention mechanism combining standard attention within chunks\n    with efficient block retrieval between chunks.\n\n    Args:\n        hidden_dim (int): Hidden dimension of the model\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of chunks for block sparse attention\n        dropout (float): Dropout rate\n        compression_factor (int, optional): Factor to compress chunk states. Defaults to 1.\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#methods_1","title":"Methods","text":""},{"location":"api/bsbr/#forward_1","title":"forward","text":"<pre><code>def forward(\n    self,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the attention mechanism.\n\n    Args:\n        query (torch.Tensor): Query tensor of shape (batch_size, seq_len, hidden_dim)\n        key (torch.Tensor): Key tensor of shape (batch_size, seq_len, hidden_dim)\n        value (torch.Tensor): Value tensor of shape (batch_size, seq_len, hidden_dim)\n        mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        **kwargs: Additional arguments passed to the attention mechanism.\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, seq_len, hidden_dim)\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#bsbrlayer","title":"BSBRLayer","text":"<pre><code>class BSBRLayer(nn.Module):\n    \"\"\"A complete transformer layer with BSBR attention and feed-forward network.\n\n    Args:\n        hidden_dim (int): Hidden dimension of the model\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of chunks for block sparse attention\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n        compression_factor (int, optional): Factor to compress chunk states. Defaults to 1.\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#methods_2","title":"Methods","text":""},{"location":"api/bsbr/#forward_2","title":"forward","text":"<pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the transformer layer.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, hidden_dim)\n        mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        **kwargs: Additional arguments passed to the layer.\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, seq_len, hidden_dim)\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/","title":"BSBR Extras Module","text":"<p>This module contains additional transformer architectures for evaluation and research purposes.</p>"},{"location":"api/bsbr_extras/#standardtransformer","title":"StandardTransformer","text":"<pre><code>class StandardTransformer(nn.Module):\n    \"\"\"Standard transformer with full attention mechanism.\n\n    Implements the classic transformer architecture with O(n\u00b2) complexity.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#lineartransformer","title":"LinearTransformer","text":"<pre><code>class LinearTransformer(nn.Module):\n    \"\"\"Linear complexity transformer using reformulated attention.\n\n    Implements a transformer with O(n) complexity by removing softmax and using\n    the associative property of matrix multiplication.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#deltanet","title":"DeltaNet","text":"<pre><code>class DeltaNet(nn.Module):\n    \"\"\"Enhanced linear transformer with removal component.\n\n    Implements a linear transformer with additional memory management through\n    a removal component.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#slidingwindowtransformer","title":"SlidingWindowTransformer","text":"<pre><code>class SlidingWindowTransformer(nn.Module):\n    \"\"\"Transformer with fixed context window attention.\n\n    Implements a transformer that restricts attention to a fixed window size\n    for O(n\u00b7w) complexity.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        window_size (int): Size of the attention window\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#hopfieldnetwork","title":"HopfieldNetwork","text":"<pre><code>class HopfieldNetwork(nn.Module):\n    \"\"\"Memory-based attention inspired by modern Hopfield Networks.\n\n    Implements a transformer using associative memory-based attention\n    for pattern completion.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#gau","title":"GAU","text":"<pre><code>class GAU(nn.Module):\n    \"\"\"Gated Attention Unit with chunk-based parallelism.\n\n    Implements a transformer using gated attention units with chunk-based\n    parallel processing.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of chunks for parallel processing\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"examples/advanced_usage/","title":"Advanced Usage Examples","text":"<p>This guide demonstrates advanced usage patterns and configurations for the BSBR model.</p>"},{"location":"examples/advanced_usage/#custom-attention-configuration","title":"Custom Attention Configuration","text":"<pre><code>from bsbr import BSBRModel, BSBRAttention\n\n# Create custom attention layer\ncustom_attention = BSBRAttention(\n    hidden_dim=512,\n    num_heads=8,\n    chunk_size=128,\n    dropout=0.1,\n    compression_factor=4  # Enable state compression\n)\n\n# Create model with custom attention\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    attention_layer=custom_attention  # Use custom attention\n)\n</code></pre>"},{"location":"examples/advanced_usage/#memory-efficient-training","title":"Memory-Efficient Training","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Enable gradient checkpointing for memory efficiency\nmodel.gradient_checkpointing_enable()\n\n# Use mixed precision training\nscaler = GradScaler()\n\ndef train_with_mixed_precision(model, dataloader, num_epochs):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            with autocast():\n                outputs = model(batch['input_ids'], batch['attention_mask'])\n                loss = criterion(outputs, batch['labels'])\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n</code></pre>"},{"location":"examples/advanced_usage/#custom-chunking-strategy","title":"Custom Chunking Strategy","text":"<pre><code>from bsbr import BSBRModel\nfrom bsbr.utils.chunking import CustomChunkingStrategy\n\n# Create custom chunking strategy\nchunking_strategy = CustomChunkingStrategy(\n    chunk_size=128,\n    overlap=32,  # Overlap between chunks\n    stride=96    # Stride for sliding window\n)\n\n# Create model with custom chunking\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    chunking_strategy=chunking_strategy\n)\n</code></pre>"},{"location":"examples/advanced_usage/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code>import torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom bsbr import BSBRModel\n\ndef setup_ddp():\n    # Initialize distributed training\n    torch.distributed.init_process_group(backend='nccl')\n    local_rank = torch.distributed.get_rank()\n    torch.cuda.set_device(local_rank)\n    return local_rank\n\ndef train_ddp():\n    local_rank = setup_ddp()\n\n    # Create model\n    model = BSBRModel(\n        vocab_size=10000,\n        hidden_dim=512,\n        num_layers=4,\n        num_heads=8,\n        chunk_size=128,\n        ff_dim=2048,\n        dropout=0.1\n    )\n\n    # Wrap model in DDP\n    model = model.to(local_rank)\n    model = DDP(model, device_ids=[local_rank])\n\n    # Create distributed dataloader\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=32,\n        sampler=train_sampler,\n        num_workers=4\n    )\n\n    # Training loop\n    for epoch in range(num_epochs):\n        train_sampler.set_epoch(epoch)\n        for batch in train_loader:\n            # Training step\n            ...\n</code></pre>"},{"location":"examples/advanced_usage/#custom-model-variants","title":"Custom Model Variants","text":"<pre><code>from bsbr import BSBRModel\nfrom bsbr_extras import LinearTransformer, DeltaNet\n\nclass HybridModel(BSBRModel):\n    \"\"\"Hybrid model combining BSBR and Linear attention.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.linear_layer = LinearTransformer(\n            vocab_size=kwargs['vocab_size'],\n            hidden_dim=kwargs['hidden_dim'],\n            num_layers=1,\n            num_heads=kwargs['num_heads'],\n            ff_dim=kwargs['ff_dim'],\n            dropout=kwargs['dropout']\n        )\n\n    def forward(self, input_ids, attention_mask=None):\n        # BSBR processing\n        bsbr_output = super().forward(input_ids, attention_mask)\n\n        # Linear attention processing\n        linear_output = self.linear_layer(input_ids, attention_mask)\n\n        # Combine outputs\n        return (bsbr_output + linear_output) / 2\n\n# Create and use hybrid model\nmodel = HybridModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n</code></pre>"},{"location":"examples/advanced_usage/#performance-optimization","title":"Performance Optimization","text":"<pre><code>from bsbr import BSBRModel\nfrom bsbr.utils.optimization import optimize_for_inference\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Optimize model for inference\noptimized_model = optimize_for_inference(model)\n\n# Use torch.jit for further optimization\nscripted_model = torch.jit.script(optimized_model)\n\n# Benchmark performance\ndef benchmark_model(model, input_ids, num_runs=100):\n    model.eval()\n    with torch.no_grad():\n        # Warm-up\n        for _ in range(10):\n            _ = model(input_ids)\n\n        # Benchmark\n        torch.cuda.synchronize()\n        start_time = time.time()\n        for _ in range(num_runs):\n            _ = model(input_ids)\n        torch.cuda.synchronize()\n        end_time = time.time()\n\n        return (end_time - start_time) / num_runs\n</code></pre>"},{"location":"examples/basic_usage/","title":"Basic Usage Examples","text":"<p>This guide demonstrates basic usage of the BSBR model and its variants.</p>"},{"location":"examples/basic_usage/#basic-bsbr-model","title":"Basic BSBR Model","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Generate sample input\nbatch_size = 2\nseq_length = 256\ninput_ids = torch.randint(0, 10000, (batch_size, seq_length))\nattention_mask = torch.ones(batch_size, seq_length)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#using-different-model-variants","title":"Using Different Model Variants","text":""},{"location":"examples/basic_usage/#linear-transformer","title":"Linear Transformer","text":"<pre><code>from bsbr_extras import LinearTransformer\n\n# Create linear transformer\nmodel = LinearTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#deltanet","title":"DeltaNet","text":"<pre><code>from bsbr_extras import DeltaNet\n\n# Create DeltaNet\nmodel = DeltaNet(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#sliding-window-transformer","title":"Sliding Window Transformer","text":"<pre><code>from bsbr_extras import SlidingWindowTransformer\n\n# Create sliding window transformer\nmodel = SlidingWindowTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    window_size=64,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#training-example","title":"Training Example","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom bsbr import BSBRModel\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\ndef train(model, dataloader, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            input_ids = batch['input_ids']\n            attention_mask = batch['attention_mask']\n            labels = batch['labels']\n\n            # Forward pass\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n</code></pre>"},{"location":"examples/basic_usage/#evaluation-example","title":"Evaluation Example","text":"<pre><code>from bsbr.evals import compare_models, analyze_results\n\n# Compare different models\nresults = compare_models(\n    seq_lengths=[64, 128, 256, 512, 1024],\n    models=['BSBR', 'Linear', 'DeltaNet', 'SlidingWindow']\n)\n\n# Analyze results\nanalysis = analyze_results(results)\nprint(analysis)\n</code></pre>"},{"location":"examples/research_examples/","title":"Research Examples","text":"<p>This guide demonstrates how to use BSBR for research and experimentation.</p>"},{"location":"examples/research_examples/#model-comparison","title":"Model Comparison","text":"<pre><code>from bsbr.evals import compare_models, analyze_results\nimport matplotlib.pyplot as plt\n\n# Compare different models across various sequence lengths\nresults = compare_models(\n    seq_lengths=[64, 128, 256, 512, 1024],\n    models=['BSBR', 'Linear', 'DeltaNet', 'SlidingWindow', 'Standard'],\n    metrics=['inference_time', 'memory_usage', 'accuracy']\n)\n\n# Analyze results\nanalysis = analyze_results(results)\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.plot(results['seq_lengths'], results['BSBR']['inference_time'], label='BSBR')\nplt.plot(results['seq_lengths'], results['Standard']['inference_time'], label='Standard')\nplt.xlabel('Sequence Length')\nplt.ylabel('Inference Time (s)')\nplt.title('Inference Time vs Sequence Length')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"examples/research_examples/#memory-analysis","title":"Memory Analysis","text":"<pre><code>from bsbr.utils.memory import analyze_memory_usage\nimport torch\n\ndef profile_memory_usage(model, input_ids):\n    \"\"\"Profile memory usage during forward pass.\"\"\"\n    torch.cuda.reset_peak_memory_stats()\n\n    # Forward pass\n    outputs = model(input_ids)\n\n    # Get memory stats\n    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n    current_memory = torch.cuda.memory_allocated() / 1024**2   # MB\n\n    return {\n        'peak_memory': peak_memory,\n        'current_memory': current_memory\n    }\n\n# Analyze memory usage across different chunk sizes\nchunk_sizes = [32, 64, 128, 256]\nmemory_results = {}\n\nfor chunk_size in chunk_sizes:\n    model = BSBRModel(\n        vocab_size=10000,\n        hidden_dim=512,\n        num_layers=4,\n        num_heads=8,\n        chunk_size=chunk_size,\n        ff_dim=2048,\n        dropout=0.1\n    )\n\n    input_ids = torch.randint(0, 10000, (1, 1024))\n    memory_results[chunk_size] = profile_memory_usage(model, input_ids)\n</code></pre>"},{"location":"examples/research_examples/#attention-visualization","title":"Attention Visualization","text":"<pre><code>from bsbr.utils.visualization import visualize_attention\n\ndef analyze_attention_patterns(model, input_ids):\n    \"\"\"Analyze attention patterns in the model.\"\"\"\n    # Get attention weights\n    attention_weights = model.get_attention_weights(input_ids)\n\n    # Visualize attention patterns\n    plt.figure(figsize=(12, 8))\n    visualize_attention(attention_weights)\n    plt.title('Attention Patterns')\n    plt.show()\n\n    # Analyze sparsity\n    sparsity = (attention_weights == 0).float().mean()\n    print(f\"Attention sparsity: {sparsity:.2%}\")\n\n# Compare attention patterns across models\nmodels = {\n    'BSBR': BSBRModel(...),\n    'Linear': LinearTransformer(...),\n    'Standard': StandardTransformer(...)\n}\n\nfor name, model in models.items():\n    print(f\"\\nAnalyzing {name} attention patterns:\")\n    analyze_attention_patterns(model, input_ids)\n</code></pre>"},{"location":"examples/research_examples/#scaling-analysis","title":"Scaling Analysis","text":"<pre><code>from bsbr.evals import scaling_analysis\n\ndef analyze_scaling_behavior():\n    \"\"\"Analyze how different models scale with sequence length.\"\"\"\n    seq_lengths = [64, 128, 256, 512, 1024, 2048]\n    models = ['BSBR', 'Linear', 'Standard']\n\n    results = scaling_analysis(\n        seq_lengths=seq_lengths,\n        models=models,\n        metrics=['time', 'memory', 'flops']\n    )\n\n    # Plot scaling curves\n    plt.figure(figsize=(12, 4))\n\n    # Time scaling\n    plt.subplot(131)\n    for model in models:\n        plt.plot(seq_lengths, results[model]['time'], label=model)\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Time (s)')\n    plt.title('Time Scaling')\n    plt.legend()\n\n    # Memory scaling\n    plt.subplot(132)\n    for model in models:\n        plt.plot(seq_lengths, results[model]['memory'], label=model)\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Memory (GB)')\n    plt.title('Memory Scaling')\n    plt.legend()\n\n    # FLOPs scaling\n    plt.subplot(133)\n    for model in models:\n        plt.plot(seq_lengths, results[model]['flops'], label=model)\n    plt.xlabel('Sequence Length')\n    plt.ylabel('FLOPs')\n    plt.title('FLOPs Scaling')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"examples/research_examples/#custom-research-experiments","title":"Custom Research Experiments","text":"<pre><code>from bsbr.utils.research import ExperimentRunner\n\nclass CustomExperiment(ExperimentRunner):\n    \"\"\"Custom research experiment.\"\"\"\n\n    def setup(self):\n        \"\"\"Setup experiment parameters.\"\"\"\n        self.models = {\n            'BSBR': BSBRModel(...),\n            'Linear': LinearTransformer(...),\n            'Standard': StandardTransformer(...)\n        }\n        self.seq_lengths = [64, 128, 256, 512, 1024]\n        self.metrics = ['time', 'memory', 'accuracy']\n\n    def run_experiment(self, model, seq_length):\n        \"\"\"Run single experiment.\"\"\"\n        # Generate input\n        input_ids = torch.randint(0, 10000, (1, seq_length))\n\n        # Measure metrics\n        start_time = time.time()\n        outputs = model(input_ids)\n        inference_time = time.time() - start_time\n\n        memory_usage = torch.cuda.max_memory_allocated() / 1024**2\n\n        # Calculate accuracy (example)\n        accuracy = self.calculate_accuracy(outputs)\n\n        return {\n            'time': inference_time,\n            'memory': memory_usage,\n            'accuracy': accuracy\n        }\n\n    def analyze_results(self, results):\n        \"\"\"Analyze experiment results.\"\"\"\n        # Custom analysis code\n        pass\n\n# Run experiment\nexperiment = CustomExperiment()\nresults = experiment.run()\nexperiment.analyze_results(results)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>BSBR requires Python 3.12 or higher and PyTorch 2.6.0 or higher. The package is designed to work with modern deep learning frameworks and tools.</p>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>The simplest way to install BSBR is using pip:</p> <pre><code>pip install bsbr\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development or to use the latest features, you can install directly from GitHub:</p> <pre><code># Clone the repository\ngit clone https://github.com/JacobFV/bsbr.git\ncd bsbr\n\n# Install in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>BSBR provides several optional dependency groups that you can install:</p> <pre><code># Install with all extras (evaluation tools, visualization, etc.)\npip install \"bsbr[extras]\"\n\n# Install with documentation tools\npip install \"bsbr[docs]\"\n\n# Install with all optional dependencies\npip install \"bsbr[all]\"\n</code></pre>"},{"location":"getting-started/installation/#available-extras","title":"Available Extras","text":"<ul> <li><code>extras</code>: Evaluation tools, visualization utilities, and research components</li> <li><code>docs</code>: Documentation building tools and dependencies</li> <li><code>all</code>: All optional dependencies</li> </ul>"},{"location":"getting-started/installation/#gpu-support","title":"GPU Support","text":"<p>BSBR automatically uses GPU acceleration when available. To ensure GPU support:</p> <ol> <li> <p>Install CUDA-enabled PyTorch: <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p> </li> <li> <p>Verify GPU availability: <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Current device: {torch.cuda.get_device_name(0)}\")\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li>Import Errors</li> <li>Ensure you're using Python 3.12 or higher</li> <li>Check that PyTorch is installed correctly</li> <li> <p>Verify your Python environment is activated</p> </li> <li> <p>CUDA Errors</p> </li> <li>Confirm CUDA toolkit is installed</li> <li>Verify PyTorch CUDA version matches your system</li> <li> <p>Check GPU drivers are up to date</p> </li> <li> <p>Memory Issues</p> </li> <li>Adjust batch size or sequence length</li> <li>Enable gradient checkpointing</li> <li>Use smaller model configurations</li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the GitHub Issues</li> <li>Join our Discord Community</li> <li>Create a new issue with:</li> <li>Python version</li> <li>PyTorch version</li> <li>Error message</li> <li>System information</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After installation, you can:</p> <ol> <li>Try the Quick Start Guide</li> <li>Explore the Examples</li> <li>Read the User Guide </li> </ol>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you get started with BSBR quickly. We'll cover basic usage, model configuration, and common patterns.</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#creating-a-bsbr-model","title":"Creating a BSBR Model","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\n\n# Create a model with default settings\nmodel = BSBRModel(\n    vocab_size=10000,      # Size of your vocabulary\n    hidden_dim=512,        # Hidden dimension of the model\n    num_layers=4,          # Number of transformer layers\n    num_heads=8,           # Number of attention heads\n    chunk_size=128,        # Size of attention chunks\n    ff_dim=2048,           # Feed-forward network dimension\n    dropout=0.1,           # Dropout rate\n    compression_factor=4   # Optional compression factor\n)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n</code></pre>"},{"location":"getting-started/quickstart/#processing-input","title":"Processing Input","text":"<pre><code># Create sample input\nbatch_size = 2\nseq_length = 256\ninput_ids = torch.randint(0, 10000, (batch_size, seq_length))\nattention_mask = torch.ones(batch_size, seq_length)\n\n# Move inputs to the same device as the model\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"getting-started/quickstart/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/quickstart/#customizing-attention","title":"Customizing Attention","text":"<pre><code>from bsbr import BSBRModel, BSBRAttention\n\n# Create a custom attention layer\nattention = BSBRAttention(\n    hidden_dim=512,\n    num_heads=8,\n    chunk_size=128,\n    compression_factor=4,\n    dropout=0.1\n)\n\n# Use it in a model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    attention_layer=attention  # Use custom attention\n)\n</code></pre>"},{"location":"getting-started/quickstart/#using-different-models","title":"Using Different Models","text":"<p>BSBR provides several attention variants for comparison:</p> <pre><code>from bsbr_extras import (\n    LinearTransformer,\n    DeltaNet,\n    SlidingWindowTransformer,\n    HopfieldNetwork,\n    GAU\n)\n\n# Linear Transformer\nlinear_model = LinearTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8\n)\n\n# DeltaNet\ndeltanet_model = DeltaNet(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8\n)\n\n# Sliding Window Transformer\nwindow_model = SlidingWindowTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    window_size=128\n)\n</code></pre>"},{"location":"getting-started/quickstart/#training-example","title":"Training Example","text":"<p>Here's a basic training loop:</p> <pre><code>import torch.nn as nn\nfrom torch.optim import Adam\n\n# Create model and move to device\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8\n).to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=0.001)\n\n# Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"getting-started/quickstart/#evaluation","title":"Evaluation","text":"<p>BSBR provides tools for evaluating different models:</p> <pre><code>from evals.compare_models import compare_models\n\n# Compare models across different sequence lengths\nresults = compare_models(\n    models=[\"BSBR\", \"Linear\", \"Hopfield\", \"GAU\"],\n    seq_lengths=[64, 128, 256, 512, 1024]\n)\n\n# Analyze results\nfrom evals.analyze_results import analyze_results\nanalysis = analyze_results(results)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ol> <li>Explore the User Guide for detailed explanations</li> <li>Check out Examples for more use cases</li> <li>Read the API Reference for complete documentation </li> </ol>"},{"location":"research/background/","title":"Research Background","text":""},{"location":"research/background/#introduction","title":"Introduction","text":"<p>Block Sparse Attention with Block Retrieval (BSBR) is a novel attention mechanism designed to efficiently process long sequences in transformer models. This document provides the theoretical background and motivation behind the approach.</p>"},{"location":"research/background/#problem-statement","title":"Problem Statement","text":"<p>Traditional transformer models face several challenges when processing long sequences:</p> <ol> <li>Quadratic Complexity: Standard attention mechanisms have O(n\u00b2) complexity in sequence length</li> <li>Memory Usage: Attention matrices grow quadratically with sequence length</li> <li>Information Flow: Long-range dependencies may be difficult to capture</li> <li>Computational Efficiency: Processing long sequences becomes computationally expensive</li> </ol>"},{"location":"research/background/#related-work","title":"Related Work","text":""},{"location":"research/background/#efficient-attention-mechanisms","title":"Efficient Attention Mechanisms","text":"<ol> <li>Linear Attention</li> <li>Reformulates attention to achieve O(n) complexity</li> <li>Uses associative property of matrix multiplication</li> <li> <p>May sacrifice expressiveness for efficiency</p> </li> <li> <p>Sparse Attention</p> </li> <li>Reduces computation by sparsifying attention patterns</li> <li>Various sparsity patterns (sliding window, strided, etc.)</li> <li> <p>Trade-off between sparsity and model capacity</p> </li> <li> <p>Sliding Window Attention</p> </li> <li>Restricts attention to local context</li> <li>O(n\u00b7w) complexity where w is window size</li> <li>May miss long-range dependencies</li> </ol>"},{"location":"research/background/#memory-efficient-approaches","title":"Memory-Efficient Approaches","text":"<ol> <li>Gradient Checkpointing</li> <li>Trades computation for memory</li> <li>Recomputes intermediate activations during backward pass</li> <li> <p>Increases training time</p> </li> <li> <p>State Compression</p> </li> <li>Compresses intermediate states</li> <li>Reduces memory usage at cost of information loss</li> <li>Various compression techniques</li> </ol>"},{"location":"research/background/#bsbr-approach","title":"BSBR Approach","text":""},{"location":"research/background/#core-idea","title":"Core Idea","text":"<p>BSBR combines two key components:</p> <ol> <li>Within-Chunk Attention</li> <li>Standard attention within fixed-size chunks</li> <li>Maintains local context processing</li> <li> <p>O(c\u00b2) complexity where c is chunk size</p> </li> <li> <p>Block Retrieval</p> </li> <li>Efficient retrieval between chunks</li> <li>Uses meta-attention for chunk-level interaction</li> <li>O(n) complexity overall</li> </ol>"},{"location":"research/background/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The attention computation can be expressed as:</p> <pre><code>Attention(Q, K, V) = softmax(QK^T)V\n</code></pre> <p>BSBR decomposes this into:</p> <ol> <li> <p>Within-chunk attention: <pre><code>A_in = softmax(Q_in K_in^T)V_in\n</code></pre></p> </li> <li> <p>Between-chunk attention: <pre><code>A_out = Q_out \u2299 softmax(RH^T)F\n</code></pre></p> </li> </ol> <p>Where: - Q_in, K_in, V_in: Query, Key, Value matrices within chunks - Q_out: Query matrix for between-chunk attention - R, H: Meta queries and keys for chunk-level attention - F: State vectors (flattened K^T\u00b7V for each chunk) - \u2299: Element-wise multiplication</p>"},{"location":"research/background/#advantages","title":"Advantages","text":"<ol> <li>Efficiency</li> <li>Linear complexity in sequence length</li> <li>Memory usage scales linearly</li> <li> <p>Parallel processing within chunks</p> </li> <li> <p>Expressiveness</p> </li> <li>Maintains local context processing</li> <li>Captures long-range dependencies through block retrieval</li> <li> <p>Flexible chunk size selection</p> </li> <li> <p>Memory Management</p> </li> <li>Natural chunking reduces peak memory usage</li> <li>Optional state compression</li> <li>Efficient gradient computation</li> </ol>"},{"location":"research/background/#implementation-details","title":"Implementation Details","text":""},{"location":"research/background/#chunking-strategy","title":"Chunking Strategy","text":"<ol> <li>Fixed-Size Chunks</li> <li>Uniform chunk size</li> <li>Simple implementation</li> <li> <p>Predictable memory usage</p> </li> <li> <p>Overlapping Chunks</p> </li> <li>Overlap between chunks</li> <li>Better context preservation</li> <li> <p>Increased computation</p> </li> <li> <p>Adaptive Chunking</p> </li> <li>Dynamic chunk sizes</li> <li>Content-aware splitting</li> <li>More complex implementation</li> </ol>"},{"location":"research/background/#block-retrieval","title":"Block Retrieval","text":"<ol> <li>Meta-Attention</li> <li>Chunk-level attention mechanism</li> <li>Efficient state compression</li> <li> <p>Flexible retrieval patterns</p> </li> <li> <p>State Compression</p> </li> <li>Optional compression factor</li> <li>Memory-performance trade-off</li> <li> <p>Various compression methods</p> </li> <li> <p>Caching</p> </li> <li>Cache chunk states</li> <li>Reuse for repeated queries</li> <li>Memory overhead</li> </ol>"},{"location":"research/background/#experimental-results","title":"Experimental Results","text":""},{"location":"research/background/#performance-metrics","title":"Performance Metrics","text":"<ol> <li>Computation Time</li> <li>Linear scaling with sequence length</li> <li>Competitive with other efficient methods</li> <li> <p>Better for long sequences</p> </li> <li> <p>Memory Usage</p> </li> <li>Linear memory scaling</li> <li>Lower peak memory</li> <li> <p>Efficient gradient computation</p> </li> <li> <p>Model Quality</p> </li> <li>Comparable to standard attention</li> <li>Better long-range modeling</li> <li>Task-specific advantages</li> </ol>"},{"location":"research/background/#comparison-with-baselines","title":"Comparison with Baselines","text":"<ol> <li>Standard Transformer</li> <li>Better scaling</li> <li>Lower memory usage</li> <li> <p>Similar accuracy</p> </li> <li> <p>Linear Transformer</p> </li> <li>Better expressiveness</li> <li>More stable training</li> <li> <p>Similar efficiency</p> </li> <li> <p>Sliding Window</p> </li> <li>Better long-range modeling</li> <li>More flexible attention</li> <li>Similar locality</li> </ol>"},{"location":"research/background/#future-directions","title":"Future Directions","text":"<ol> <li>Architecture Improvements</li> <li>Adaptive chunking</li> <li>Dynamic compression</li> <li> <p>Hybrid approaches</p> </li> <li> <p>Applications</p> </li> <li>Long document processing</li> <li>Multi-modal tasks</li> <li> <p>Real-time inference</p> </li> <li> <p>Optimization</p> </li> <li>Hardware acceleration</li> <li>Distributed training</li> <li>Quantization </li> </ol>"},{"location":"research/benchmarks/","title":"Research Benchmarks","text":"<p>This document provides detailed benchmark results comparing BSBR with other transformer architectures.</p>"},{"location":"research/benchmarks/#benchmark-setup","title":"Benchmark Setup","text":""},{"location":"research/benchmarks/#environment","title":"Environment","text":"<pre><code>benchmark_config = {\n    'hardware': {\n        'gpu': 'NVIDIA A100 (40GB)',\n        'cpu': '32 cores',\n        'memory': '256GB RAM',\n        'storage': '2TB NVMe SSD'\n    },\n    'software': {\n        'pytorch': '2.6.0',\n        'cuda': '11.8',\n        'python': '3.12',\n        'bsbr': '0.1.1'\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#models","title":"Models","text":"<pre><code>model_configs = {\n    'BSBR': {\n        'hidden_dim': 512,\n        'num_layers': 4,\n        'num_heads': 8,\n        'chunk_size': 128,\n        'ff_dim': 2048,\n        'dropout': 0.1\n    },\n    'Standard': {\n        'hidden_dim': 512,\n        'num_layers': 4,\n        'num_heads': 8,\n        'ff_dim': 2048,\n        'dropout': 0.1\n    },\n    'Linear': {\n        'hidden_dim': 512,\n        'num_layers': 4,\n        'num_heads': 8,\n        'ff_dim': 2048,\n        'dropout': 0.1\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"research/benchmarks/#inference-time","title":"Inference Time","text":"<pre><code>inference_results = {\n    'sequence_lengths': [64, 128, 256, 512, 1024, 2048],\n    'BSBR': {\n        'time': [0.1, 0.15, 0.25, 0.4, 0.7, 1.2],  # seconds\n        'scaling': 'linear'\n    },\n    'Standard': {\n        'time': [0.1, 0.3, 1.0, 3.5, 12.0, 42.0],  # seconds\n        'scaling': 'quadratic'\n    },\n    'Linear': {\n        'time': [0.1, 0.15, 0.25, 0.4, 0.7, 1.2],  # seconds\n        'scaling': 'linear'\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#memory-usage","title":"Memory Usage","text":"<pre><code>memory_results = {\n    'sequence_lengths': [64, 128, 256, 512, 1024, 2048],\n    'BSBR': {\n        'peak_memory': [0.5, 0.8, 1.2, 1.8, 2.5, 3.3],  # GB\n        'gradient_memory': [0.3, 0.5, 0.7, 1.0, 1.4, 1.8]  # GB\n    },\n    'Standard': {\n        'peak_memory': [1.0, 2.0, 4.0, 8.0, 16.0, 32.0],  # GB\n        'gradient_memory': [0.8, 1.6, 3.2, 6.4, 12.8, 25.6]  # GB\n    },\n    'Linear': {\n        'peak_memory': [0.5, 0.8, 1.2, 1.8, 2.5, 3.3],  # GB\n        'gradient_memory': [0.3, 0.5, 0.7, 1.0, 1.4, 1.8]  # GB\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#flops-count","title":"FLOPs Count","text":"<pre><code>flops_results = {\n    'sequence_lengths': [64, 128, 256, 512, 1024, 2048],\n    'BSBR': {\n        'flops': [1e9, 2e9, 4e9, 8e9, 16e9, 32e9],\n        'scaling': 'linear'\n    },\n    'Standard': {\n        'flops': [1e9, 4e9, 16e9, 64e9, 256e9, 1024e9],\n        'scaling': 'quadratic'\n    },\n    'Linear': {\n        'flops': [1e9, 2e9, 4e9, 8e9, 16e9, 32e9],\n        'scaling': 'linear'\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#training-benchmarks","title":"Training Benchmarks","text":""},{"location":"research/benchmarks/#convergence-speed","title":"Convergence Speed","text":"<pre><code>convergence_results = {\n    'BSBR': {\n        'epochs_to_converge': 50,\n        'final_loss': 0.15,\n        'validation_accuracy': 0.92\n    },\n    'Standard': {\n        'epochs_to_converge': 45,\n        'final_loss': 0.18,\n        'validation_accuracy': 0.89\n    },\n    'Linear': {\n        'epochs_to_converge': 55,\n        'final_loss': 0.20,\n        'validation_accuracy': 0.87\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#training-memory","title":"Training Memory","text":"<pre><code>training_memory = {\n    'BSBR': {\n        'peak_memory': 2.5,  # GB\n        'gradient_memory': 1.4,  # GB\n        'activation_memory': 0.8  # GB\n    },\n    'Standard': {\n        'peak_memory': 16.0,  # GB\n        'gradient_memory': 12.8,  # GB\n        'activation_memory': 8.0  # GB\n    },\n    'Linear': {\n        'peak_memory': 2.5,  # GB\n        'gradient_memory': 1.4,  # GB\n        'activation_memory': 0.8  # GB\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#task-specific-benchmarks","title":"Task-Specific Benchmarks","text":""},{"location":"research/benchmarks/#document-classification","title":"Document Classification","text":"<pre><code>document_results = {\n    'sequence_lengths': [512, 1024, 2048],\n    'BSBR': {\n        'accuracy': [0.94, 0.92, 0.89],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    },\n    'Standard': {\n        'accuracy': [0.95, 0.88, 0.75],\n        'inference_time': [1.5, 5.0, 18.0]  # seconds\n    },\n    'Linear': {\n        'accuracy': [0.92, 0.89, 0.85],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#language-modeling","title":"Language Modeling","text":"<pre><code>language_modeling_results = {\n    'sequence_lengths': [512, 1024, 2048],\n    'BSBR': {\n        'perplexity': [15.2, 16.8, 18.5],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    },\n    'Standard': {\n        'perplexity': [14.8, 16.5, 18.2],\n        'inference_time': [1.5, 5.0, 18.0]  # seconds\n    },\n    'Linear': {\n        'perplexity': [15.5, 17.2, 19.0],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#question-answering","title":"Question Answering","text":"<pre><code>qa_results = {\n    'sequence_lengths': [512, 1024, 2048],\n    'BSBR': {\n        'f1_score': [0.82, 0.80, 0.77],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    },\n    'Standard': {\n        'f1_score': [0.83, 0.79, 0.74],\n        'inference_time': [1.5, 5.0, 18.0]  # seconds\n    },\n    'Linear': {\n        'f1_score': [0.81, 0.78, 0.75],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#hardware-utilization","title":"Hardware Utilization","text":""},{"location":"research/benchmarks/#gpu-utilization","title":"GPU Utilization","text":"<pre><code>gpu_utilization = {\n    'BSBR': {\n        'gpu_util': 85,  # percentage\n        'memory_util': 60,  # percentage\n        'power_usage': 250  # watts\n    },\n    'Standard': {\n        'gpu_util': 95,  # percentage\n        'memory_util': 90,  # percentage\n        'power_usage': 300  # watts\n    },\n    'Linear': {\n        'gpu_util': 80,  # percentage\n        'memory_util': 55,  # percentage\n        'power_usage': 230  # watts\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#cpu-utilization","title":"CPU Utilization","text":"<pre><code>cpu_utilization = {\n    'BSBR': {\n        'cpu_util': 40,  # percentage\n        'memory_util': 30,  # percentage\n        'io_util': 20  # percentage\n    },\n    'Standard': {\n        'cpu_util': 50,  # percentage\n        'memory_util': 70,  # percentage\n        'io_util': 30  # percentage\n    },\n    'Linear': {\n        'cpu_util': 35,  # percentage\n        'memory_util': 25,  # percentage\n        'io_util': 15  # percentage\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#summary","title":"Summary","text":""},{"location":"research/benchmarks/#key-findings","title":"Key Findings","text":"<ol> <li>Performance</li> <li>BSBR achieves linear scaling with sequence length</li> <li>Significantly lower memory usage compared to standard attention</li> <li> <p>Competitive inference time with other efficient methods</p> </li> <li> <p>Training</p> </li> <li>Similar convergence speed to standard attention</li> <li>Much lower memory requirements during training</li> <li> <p>Stable training dynamics</p> </li> <li> <p>Task Performance</p> </li> <li>Comparable accuracy to standard attention</li> <li>Better handling of long sequences</li> <li>More efficient inference</li> </ol>"},{"location":"research/benchmarks/#recommendations","title":"Recommendations","text":"<ol> <li>Use Cases</li> <li>Long document processing</li> <li>Memory-constrained environments</li> <li> <p>Real-time applications</p> </li> <li> <p>Configuration</p> </li> <li>Chunk size: 128 for general use</li> <li>Compression factor: 4 for memory efficiency</li> <li> <p>Batch size: 32 for optimal performance</p> </li> <li> <p>Hardware</p> </li> <li>GPU with at least 8GB memory</li> <li>Fast storage for data loading</li> <li>Sufficient CPU cores for preprocessing </li> </ol>"},{"location":"research/experiments/","title":"Research Experiments","text":"<p>This document details the experiments conducted to evaluate BSBR's performance and capabilities.</p>"},{"location":"research/experiments/#experimental-setup","title":"Experimental Setup","text":""},{"location":"research/experiments/#hardware-configuration","title":"Hardware Configuration","text":"<ul> <li>GPU: NVIDIA A100 (40GB)</li> <li>CPU: 32 cores</li> <li>Memory: 256GB RAM</li> <li>Storage: 2TB NVMe SSD</li> </ul>"},{"location":"research/experiments/#software-stack","title":"Software Stack","text":"<ul> <li>PyTorch 2.6.0</li> <li>CUDA 11.8</li> <li>Python 3.12</li> <li>BSBR 0.1.1</li> </ul>"},{"location":"research/experiments/#baseline-models","title":"Baseline Models","text":"<ol> <li>Standard Transformer</li> <li>Full attention mechanism</li> <li>O(n\u00b2) complexity</li> <li> <p>Reference implementation</p> </li> <li> <p>Linear Transformer</p> </li> <li>Linear complexity attention</li> <li>No softmax</li> <li> <p>Efficient implementation</p> </li> <li> <p>Sliding Window Transformer</p> </li> <li>Fixed context window</li> <li>O(n\u00b7w) complexity</li> <li> <p>Local attention</p> </li> <li> <p>DeltaNet</p> </li> <li>Enhanced linear transformer</li> <li>Removal component</li> <li>Memory efficient</li> </ol>"},{"location":"research/experiments/#performance-experiments","title":"Performance Experiments","text":""},{"location":"research/experiments/#scaling-analysis","title":"Scaling Analysis","text":""},{"location":"research/experiments/#sequence-length-scaling","title":"Sequence Length Scaling","text":"<pre><code>seq_lengths = [64, 128, 256, 512, 1024, 2048]\nmetrics = ['inference_time', 'memory_usage', 'flops']\n\nresults = scaling_analysis(\n    seq_lengths=seq_lengths,\n    models=['BSBR', 'Linear', 'Standard'],\n    metrics=metrics\n)\n</code></pre> <p>Results: - BSBR: O(n) scaling - Linear: O(n) scaling - Standard: O(n\u00b2) scaling</p>"},{"location":"research/experiments/#memory-usage","title":"Memory Usage","text":"<pre><code>memory_results = {\n    'BSBR': {\n        'peak_memory': [0.5, 0.8, 1.2, 1.8, 2.5, 3.3],  # GB\n        'gradient_memory': [0.3, 0.5, 0.7, 1.0, 1.4, 1.8]  # GB\n    },\n    'Standard': {\n        'peak_memory': [1.0, 2.0, 4.0, 8.0, 16.0, 32.0],  # GB\n        'gradient_memory': [0.8, 1.6, 3.2, 6.4, 12.8, 25.6]  # GB\n    }\n}\n</code></pre>"},{"location":"research/experiments/#training-experiments","title":"Training Experiments","text":""},{"location":"research/experiments/#convergence-analysis","title":"Convergence Analysis","text":"<pre><code>training_configs = {\n    'BSBR': {\n        'learning_rate': 1e-4,\n        'batch_size': 32,\n        'chunk_size': 128\n    },\n    'Standard': {\n        'learning_rate': 1e-4,\n        'batch_size': 32\n    }\n}\n\nconvergence_results = {\n    'BSBR': {\n        'epochs_to_converge': 50,\n        'final_loss': 0.15,\n        'validation_accuracy': 0.92\n    },\n    'Standard': {\n        'epochs_to_converge': 45,\n        'final_loss': 0.18,\n        'validation_accuracy': 0.89\n    }\n}\n</code></pre>"},{"location":"research/experiments/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code>memory_efficiency = {\n    'BSBR': {\n        'peak_memory': 2.5,  # GB\n        'gradient_memory': 1.4,  # GB\n        'activation_memory': 0.8  # GB\n    },\n    'Standard': {\n        'peak_memory': 16.0,  # GB\n        'gradient_memory': 12.8,  # GB\n        'activation_memory': 8.0  # GB\n    }\n}\n</code></pre>"},{"location":"research/experiments/#model-quality-experiments","title":"Model Quality Experiments","text":""},{"location":"research/experiments/#long-range-dependencies","title":"Long-Range Dependencies","text":""},{"location":"research/experiments/#task-document-classification","title":"Task: Document Classification","text":"<pre><code>dataset_configs = {\n    'short': {'max_length': 512},\n    'medium': {'max_length': 1024},\n    'long': {'max_length': 2048}\n}\n\nresults = {\n    'BSBR': {\n        'short': 0.94,\n        'medium': 0.92,\n        'long': 0.89\n    },\n    'Standard': {\n        'short': 0.95,\n        'medium': 0.88,\n        'long': 0.75\n    }\n}\n</code></pre>"},{"location":"research/experiments/#attention-analysis","title":"Attention Analysis","text":""},{"location":"research/experiments/#sparsity-patterns","title":"Sparsity Patterns","text":"<pre><code>sparsity_results = {\n    'BSBR': {\n        'within_chunk': 0.0,  # Full attention\n        'between_chunk': 0.85,  # Sparse attention\n        'overall': 0.65\n    },\n    'Standard': {\n        'within_chunk': 0.0,\n        'between_chunk': 0.0,\n        'overall': 0.0\n    }\n}\n</code></pre>"},{"location":"research/experiments/#attention-visualization","title":"Attention Visualization","text":"<pre><code>def visualize_attention_patterns():\n    \"\"\"Generate attention heatmaps.\"\"\"\n    models = ['BSBR', 'Standard']\n    seq_lengths = [256, 512, 1024]\n\n    for model in models:\n        for length in seq_lengths:\n            attention_weights = get_attention_weights(model, length)\n            plot_attention_heatmap(attention_weights, f\"{model}_{length}\")\n</code></pre>"},{"location":"research/experiments/#ablation-studies","title":"Ablation Studies","text":""},{"location":"research/experiments/#chunk-size-impact","title":"Chunk Size Impact","text":"<pre><code>chunk_sizes = [32, 64, 128, 256]\nresults = {\n    'inference_time': {\n        32: 0.1,  # seconds\n        64: 0.15,\n        128: 0.2,\n        256: 0.3\n    },\n    'memory_usage': {\n        32: 0.8,  # GB\n        64: 1.2,\n        128: 1.8,\n        256: 2.5\n    },\n    'accuracy': {\n        32: 0.85,\n        64: 0.88,\n        128: 0.92,\n        256: 0.93\n    }\n}\n</code></pre>"},{"location":"research/experiments/#compression-factor-analysis","title":"Compression Factor Analysis","text":"<pre><code>compression_factors = [1, 2, 4, 8]\nresults = {\n    'memory_reduction': {\n        1: 1.0,  # baseline\n        2: 0.6,\n        4: 0.4,\n        8: 0.3\n    },\n    'accuracy_impact': {\n        1: 0.92,  # baseline\n        2: 0.91,\n        4: 0.89,\n        8: 0.85\n    }\n}\n</code></pre>"},{"location":"research/experiments/#real-world-applications","title":"Real-World Applications","text":""},{"location":"research/experiments/#document-processing","title":"Document Processing","text":"<pre><code>document_results = {\n    'BSBR': {\n        'processing_time': 0.5,  # seconds per page\n        'memory_usage': 2.5,  # GB\n        'accuracy': 0.92\n    },\n    'Standard': {\n        'processing_time': 2.0,\n        'memory_usage': 16.0,\n        'accuracy': 0.89\n    }\n}\n</code></pre>"},{"location":"research/experiments/#multi-modal-tasks","title":"Multi-Modal Tasks","text":"<pre><code>multimodal_results = {\n    'BSBR': {\n        'text_accuracy': 0.92,\n        'image_accuracy': 0.88,\n        'joint_accuracy': 0.85\n    },\n    'Standard': {\n        'text_accuracy': 0.89,\n        'image_accuracy': 0.85,\n        'joint_accuracy': 0.82\n    }\n}\n</code></pre>"},{"location":"research/experiments/#conclusions","title":"Conclusions","text":"<ol> <li>Performance</li> <li>Linear scaling with sequence length</li> <li>Significantly lower memory usage</li> <li> <p>Competitive inference time</p> </li> <li> <p>Model Quality</p> </li> <li>Better long-range modeling</li> <li>Comparable accuracy to standard attention</li> <li> <p>More stable training</p> </li> <li> <p>Practical Benefits</p> </li> <li>Efficient document processing</li> <li>Better memory management</li> <li>Flexible architecture </li> </ol>"},{"location":"user-guide/core-concepts/","title":"Core Concepts","text":"<p>This guide explains the fundamental concepts behind BSBR (Block Sparse Attention with Block Retrieval). Understanding these concepts will help you make better use of the library and customize it for your needs.</p>"},{"location":"user-guide/core-concepts/#attention-mechanisms","title":"Attention Mechanisms","text":""},{"location":"user-guide/core-concepts/#standard-transformer-attention","title":"Standard Transformer Attention","text":"<p>The standard transformer attention mechanism computes attention scores between all pairs of tokens in a sequence:</p> <pre><code>Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n</code></pre> <p>This leads to O(n\u00b2) complexity in both computation and memory, where n is the sequence length.</p>"},{"location":"user-guide/core-concepts/#bsbrs-approach","title":"BSBR's Approach","text":"<p>BSBR addresses this scalability issue by combining two types of attention:</p> <ol> <li>Within-chunk attention: Standard attention within fixed-size chunks</li> <li>Between-chunk attention: Efficient block retrieval between chunks</li> </ol> <pre><code>O = Q \u2299 softmax(RH^T \u00b7 M_out)F.repeat(B) + softmax(QK^T \u00b7 M_in)V\n</code></pre> <p>Where: - Q, K, V: Query, Key, Value matrices - R, H: Meta queries and keys for chunk-level attention - F: State vectors (flattened K^T\u00b7V for each chunk) - M_in: Block diagonal mask - M_out: Causal mask for chunk-level attention</p>"},{"location":"user-guide/core-concepts/#chunking-strategy","title":"Chunking Strategy","text":""},{"location":"user-guide/core-concepts/#chunk-size-selection","title":"Chunk Size Selection","text":"<p>The chunk size (B) is a crucial hyperparameter that affects:</p> <ol> <li>Memory Usage: Larger chunks use more memory but provide better local context</li> <li>Computation Time: Smaller chunks are faster but may miss important long-range dependencies</li> <li>Model Expressivity: Chunk size affects how well the model can capture different types of relationships</li> </ol>"},{"location":"user-guide/core-concepts/#chunk-overlap","title":"Chunk Overlap","text":"<p>BSBR supports overlapping chunks to improve information flow between adjacent chunks:</p> <pre><code>model = BSBRModel(\n    chunk_size=128,\n    chunk_overlap=32,  # 25% overlap between chunks\n    ...\n)\n</code></pre>"},{"location":"user-guide/core-concepts/#block-retrieval","title":"Block Retrieval","text":""},{"location":"user-guide/core-concepts/#meta-attention","title":"Meta Attention","text":"<p>Between chunks, BSBR uses a meta-attention mechanism to efficiently retrieve information:</p> <ol> <li>State Compression: Each chunk's information is compressed into a state vector</li> <li>Meta Queries: Special queries that operate at the chunk level</li> <li>Efficient Retrieval: O(n/B) complexity for chunk-level attention</li> </ol>"},{"location":"user-guide/core-concepts/#compression-factor","title":"Compression Factor","text":"<p>The compression factor \u00a9 controls how much information is preserved in chunk states:</p> <pre><code>model = BSBRModel(\n    compression_factor=4,  # Compress chunk states by 4x\n    ...\n)\n</code></pre> <p>Higher compression factors: - Reduce memory usage - Speed up computation - May lose some fine-grained information</p>"},{"location":"user-guide/core-concepts/#memory-management","title":"Memory Management","text":""},{"location":"user-guide/core-concepts/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>BSBR supports gradient checkpointing to trade computation for memory:</p> <pre><code>model = BSBRModel(\n    use_checkpointing=True,  # Enable gradient checkpointing\n    ...\n)\n</code></pre>"},{"location":"user-guide/core-concepts/#memory-efficient-attention","title":"Memory-Efficient Attention","text":"<p>The implementation includes several memory optimizations:</p> <ol> <li>Sparse Attention: Only compute attention for relevant token pairs</li> <li>State Reuse: Reuse chunk states across layers</li> <li>Efficient Masking: Optimized attention masks for causal language modeling</li> </ol>"},{"location":"user-guide/core-concepts/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"user-guide/core-concepts/#computational-complexity","title":"Computational Complexity","text":"<p>BSBR achieves near-linear complexity in sequence length:</p> <ul> <li>Within-chunk: O(n\u00b7B) where B is chunk size</li> <li>Between-chunk: O(n + n\u00b2/B)</li> <li>Overall: O(n) for fixed chunk size</li> </ul>"},{"location":"user-guide/core-concepts/#memory-usage","title":"Memory Usage","text":"<p>Memory consumption scales linearly with sequence length:</p> <ul> <li>Within-chunk: O(n\u00b7B)</li> <li>Between-chunk: O(n/B)</li> <li>State vectors: O(n/c) where c is compression factor</li> </ul>"},{"location":"user-guide/core-concepts/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/core-concepts/#model-configuration","title":"Model Configuration","text":"<p>Recommended configurations for different use cases:</p> <ol> <li> <p>Short Sequences (n &lt; 512):    <pre><code>model = BSBRModel(\n    chunk_size=64,\n    compression_factor=2,\n    ...\n)\n</code></pre></p> </li> <li> <p>Medium Sequences (512 \u2264 n &lt; 2048):    <pre><code>model = BSBRModel(\n    chunk_size=128,\n    compression_factor=4,\n    ...\n)\n</code></pre></p> </li> <li> <p>Long Sequences (n \u2265 2048):    <pre><code>model = BSBRModel(\n    chunk_size=256,\n    compression_factor=8,\n    use_checkpointing=True,\n    ...\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/core-concepts/#training-tips","title":"Training Tips","text":"<ol> <li>Learning Rate: Use slightly higher learning rates than standard transformers</li> <li>Warmup: Longer warmup periods may be needed for very long sequences</li> <li>Gradient Clipping: Monitor gradients and clip if necessary</li> <li>Batch Size: Adjust based on available memory and sequence length</li> </ol>"},{"location":"user-guide/core-concepts/#next-steps","title":"Next Steps","text":"<ol> <li>Check the API Reference for detailed documentation</li> <li>Explore Examples for usage examples</li> <li>See Research for benchmark results </li> </ol>"}]}