{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BSBR: Block Sparse Attention with Block Retrieval","text":"<p>BSBR (Block Sparse Attention with Block Retrieval) is a novel attention mechanism for efficient processing of long sequences in transformer architectures. It combines standard attention within chunks and block retrieval between chunks to achieve near-linear complexity while maintaining high model expressivity.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udd04 Efficient Processing: Near-linear complexity in sequence length</li> <li>\ud83e\udde9 Chunk-Based Attention: Standard attention within chunks</li> <li>\ud83d\udd0d Block Retrieval: Efficient information retrieval between chunks</li> <li>\ud83c\udfaf Configurable: Adjustable chunk size and compression</li> <li>\ud83d\udcbe Memory Efficient: Optimized memory usage for long sequences</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\n\n# Model configuration\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    compression_factor=4  # Optional compression\n)\n\n# Input data\ninput_ids = torch.randint(0, 10000, (2, 256))\nattention_mask = torch.ones(2, 256)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code># Install the core package\npip install bsbr\n\n# Install with extras for evaluations and research\npip install \"bsbr[extras]\"\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>User Guide</li> <li>API Reference</li> <li>Examples</li> <li>Research</li> </ul>"},{"location":"#research","title":"Research","text":"<p>BSBR is based on research presented in our paper BSBR: Block Sparse Attention with Block Retrieval for Efficient Long-Context Reasoning. The implementation is inspired by Shengding Hu's blog post Streaming models for efficient long-context reasoning.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guidelines for details.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details. </p>"},{"location":"converter_findings/","title":"Converting Standard Transformers to BSBR: Findings","text":""},{"location":"converter_findings/#overview","title":"Overview","text":"<p>We developed a utility to convert pre-trained GPT-style transformers to use Block Sparse Attention with Block Retrieval (BSBR). This document summarizes our findings on the feasibility, benefits, and trade-offs of such conversions.</p>"},{"location":"converter_findings/#technical-implementation","title":"Technical Implementation","text":"<p>We successfully implemented a converter that:</p> <ol> <li>Extracts weights from standard GPT-2 models</li> <li>Creates an equivalent BSBR model with appropriate dimensions</li> <li>Transfers and transforms weights to fit the BSBR architecture</li> <li>Initializes block-specific components like meta-queries and meta-keys</li> </ol> <p>The conversion process preserves the original model's knowledge while introducing the more efficient BSBR attention mechanism.</p>"},{"location":"converter_findings/#key-findings","title":"Key Findings","text":""},{"location":"converter_findings/#mathematical-equivalence","title":"Mathematical Equivalence","text":"<ul> <li>Within-chunk processing is mathematically similar between standard and BSBR transformers, using the same causal attention pattern.</li> <li>Between-chunk processing in BSBR uses a fundamentally different approach with meta-queries, meta-keys, and chunk states.</li> <li>The conversion is not an exact equivalence transformation but preserves much of the trained knowledge.</li> </ul>"},{"location":"converter_findings/#weight-transfer","title":"Weight Transfer","text":"<ul> <li>Query, Key, Value projections can be transferred directly, sometimes requiring transposition due to differences between Conv1D and nn.Linear implementations.</li> <li>Meta projections for block retrieval need to be initialized as combinations of existing projections since they have no direct equivalent in standard transformers.</li> <li>Feed-forward networks can be transferred directly with minimal adjustments for shape differences.</li> <li>Layer normalization parameters transfer directly without modification.</li> </ul>"},{"location":"converter_findings/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Our benchmark tests show that for short sequences, the standard transformer can be faster due to its optimized implementation.</li> <li>For longer sequences, BSBR becomes more efficient as the benefits of chunked processing become apparent.</li> <li>The current implementation shows a slight slowdown for typical sequence lengths, but this is expected to reverse for very long sequences.</li> <li>BSBR models use approximately 40% more parameters due to the additional meta projections for chunk retrieval.</li> </ul>"},{"location":"converter_findings/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>BSBR's theoretically lower asymptotic memory complexity is not always realized in practice for shorter sequences.</li> <li>The main benefits for memory usage are expected to become apparent with extremely long sequences (10k+ tokens).</li> <li>The additional parameters used by BSBR meta projections slightly increase the model size.</li> </ul>"},{"location":"converter_findings/#practical-considerations","title":"Practical Considerations","text":""},{"location":"converter_findings/#initialization-strategies","title":"Initialization Strategies","text":"<ul> <li>For meta-queries and meta-keys, we found that averaging the weights of existing projections provides a reasonable starting point.</li> <li>Fine-tuning might be necessary to optimize the newly initialized components.</li> </ul>"},{"location":"converter_findings/#compatibility","title":"Compatibility","text":"<ul> <li>BSBR models can be used as drop-in replacements for standard transformers with minimal adaptation.</li> <li>The standard interface for feeding inputs and extracting outputs is preserved.</li> </ul>"},{"location":"converter_findings/#performance-tuning","title":"Performance Tuning","text":"<ul> <li>Chunk size is a critical hyperparameter that significantly affects both efficiency and effectiveness.</li> <li>Compression factor for state vectors can reduce memory requirements with minimal impact on performance.</li> </ul>"},{"location":"converter_findings/#recommendations","title":"Recommendations","text":"<ol> <li> <p>Use Case Assessment: BSBR conversion is most beneficial for applications requiring processing of very long sequences.</p> </li> <li> <p>Fine-tuning: After conversion, a brief period of fine-tuning can help adapt the model to the new architecture.</p> </li> <li> <p>Chunk Size Selection: </p> </li> <li>Smaller chunks (64-128) work well for shorter contexts</li> <li> <p>Larger chunks (256-512) may be better for very long contexts</p> </li> <li> <p>Compression Tradeoff: State vector compression offers memory savings at a small cost to performance.</p> </li> </ol>"},{"location":"converter_findings/#conclusion","title":"Conclusion","text":"<p>Converting standard transformers to BSBR is technically feasible and offers potential efficiency benefits for long-context processing. The current implementation demonstrates that pre-trained knowledge can be preserved during conversion, making it possible to leverage existing models in a more efficient architecture.</p> <p>The primary advantage of BSBR\u2014its ability to handle extremely long contexts efficiently\u2014makes it particularly valuable for applications like document processing, long-term memory, and persistent context tracking.</p>"},{"location":"converter_findings/#future-work","title":"Future Work","text":"<ol> <li> <p>Broader Model Support: Extend conversion support to other transformer architectures beyond GPT-2.</p> </li> <li> <p>Memory Optimization: Further reduce memory requirements for state vectors through more advanced compression techniques.</p> </li> <li> <p>Fine-tuning Studies: Research optimal fine-tuning strategies after conversion to recover any performance degradation.</p> </li> <li> <p>Hardware Acceleration: Develop specialized kernels to better leverage the block structure for even greater efficiency. </p> </li> </ol>"},{"location":"api/bsbr/","title":"BSBR Module","text":""},{"location":"api/bsbr/#bsbrmodel","title":"BSBRModel","text":"<pre><code>class BSBRModel(nn.Module):\n    \"\"\"Block Sparse Attention with Block Retrieval model.\n\n    A transformer model that combines standard attention within chunks with efficient\n    block retrieval between chunks for processing long sequences.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of chunks for block sparse attention\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n        compression_factor (int, optional): Factor to compress chunk states. Defaults to 1.\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#methods","title":"Methods","text":""},{"location":"api/bsbr/#forward","title":"forward","text":"<pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs of shape (batch_size, seq_len)\n        attention_mask (torch.Tensor, optional): Attention mask of shape (batch_size, seq_len).\n            Defaults to None.\n        **kwargs: Additional arguments passed to the model.\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, seq_len, vocab_size)\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#bsbrattention","title":"BSBRAttention","text":"<pre><code>class BSBRAttention(nn.Module):\n    \"\"\"Block Sparse Attention with Block Retrieval.\n\n    Implements the core attention mechanism combining standard attention within chunks\n    with efficient block retrieval between chunks.\n\n    Args:\n        hidden_dim (int): Hidden dimension of the model\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of chunks for block sparse attention\n        dropout (float): Dropout rate\n        compression_factor (int, optional): Factor to compress chunk states. Defaults to 1.\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#methods_1","title":"Methods","text":""},{"location":"api/bsbr/#forward_1","title":"forward","text":"<pre><code>def forward(\n    self,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the attention mechanism.\n\n    Args:\n        query (torch.Tensor): Query tensor of shape (batch_size, seq_len, hidden_dim)\n        key (torch.Tensor): Key tensor of shape (batch_size, seq_len, hidden_dim)\n        value (torch.Tensor): Value tensor of shape (batch_size, seq_len, hidden_dim)\n        mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        **kwargs: Additional arguments passed to the attention mechanism.\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, seq_len, hidden_dim)\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#bsbrlayer","title":"BSBRLayer","text":"<pre><code>class BSBRLayer(nn.Module):\n    \"\"\"A complete transformer layer with BSBR attention and feed-forward network.\n\n    Args:\n        hidden_dim (int): Hidden dimension of the model\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of chunks for block sparse attention\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n        compression_factor (int, optional): Factor to compress chunk states. Defaults to 1.\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr/#methods_2","title":"Methods","text":""},{"location":"api/bsbr/#forward_2","title":"forward","text":"<pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    **kwargs\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the transformer layer.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, hidden_dim)\n        mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        **kwargs: Additional arguments passed to the layer.\n\n    Returns:\n        torch.Tensor: Output tensor of shape (batch_size, seq_len, hidden_dim)\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/","title":"BSBR Extras Module","text":"<p>This module contains additional transformer architectures for evaluation and research purposes.</p>"},{"location":"api/bsbr_extras/#standardtransformer","title":"StandardTransformer","text":"<pre><code>class StandardTransformer(nn.Module):\n    \"\"\"Standard transformer with full attention mechanism.\n\n    Implements the classic transformer architecture with O(n\u00b2) complexity.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#lineartransformer","title":"LinearTransformer","text":"<pre><code>class LinearTransformer(nn.Module):\n    \"\"\"Linear complexity transformer using reformulated attention.\n\n    Implements a transformer with O(n) complexity by removing softmax and using\n    the associative property of matrix multiplication.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#deltanet","title":"DeltaNet","text":"<pre><code>class DeltaNet(nn.Module):\n    \"\"\"Enhanced linear transformer with removal component.\n\n    Implements a linear transformer with additional memory management through\n    a removal component.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#slidingwindowtransformer","title":"SlidingWindowTransformer","text":"<pre><code>class SlidingWindowTransformer(nn.Module):\n    \"\"\"Transformer with fixed context window attention.\n\n    Implements a transformer that restricts attention to a fixed window size\n    for O(n\u00b7w) complexity.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        window_size (int): Size of the attention window\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#hopfieldnetwork","title":"HopfieldNetwork","text":"<pre><code>class HopfieldNetwork(nn.Module):\n    \"\"\"Memory-based attention inspired by modern Hopfield Networks.\n\n    Implements a transformer using associative memory-based attention\n    for pattern completion.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"api/bsbr_extras/#gau","title":"GAU","text":"<pre><code>class GAU(nn.Module):\n    \"\"\"Gated Attention Unit with chunk-based parallelism.\n\n    Implements a transformer using gated attention units with chunk-based\n    parallel processing.\n\n    Args:\n        vocab_size (int): Size of the vocabulary\n        hidden_dim (int): Hidden dimension of the model\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of chunks for parallel processing\n        ff_dim (int): Feed-forward network dimension\n        dropout (float): Dropout rate\n    \"\"\"\n</code></pre>"},{"location":"examples/advanced_usage/","title":"Advanced Usage Examples","text":"<p>This guide demonstrates advanced usage patterns and configurations for the BSBR model.</p>"},{"location":"examples/advanced_usage/#custom-attention-configuration","title":"Custom Attention Configuration","text":"<pre><code>from bsbr import BSBRModel, BSBRAttention\n\n# Create custom attention layer\ncustom_attention = BSBRAttention(\n    hidden_dim=512,\n    num_heads=8,\n    chunk_size=128,\n    dropout=0.1,\n    compression_factor=4  # Enable state compression\n)\n\n# Create model with custom attention\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    attention_layer=custom_attention  # Use custom attention\n)\n</code></pre>"},{"location":"examples/advanced_usage/#memory-efficient-training","title":"Memory-Efficient Training","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Enable gradient checkpointing for memory efficiency\nmodel.gradient_checkpointing_enable()\n\n# Use mixed precision training\nscaler = GradScaler()\n\ndef train_with_mixed_precision(model, dataloader, num_epochs):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            with autocast():\n                outputs = model(batch['input_ids'], batch['attention_mask'])\n                loss = criterion(outputs, batch['labels'])\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n</code></pre>"},{"location":"examples/advanced_usage/#custom-chunking-strategy","title":"Custom Chunking Strategy","text":"<pre><code>from bsbr import BSBRModel\nfrom bsbr.utils.chunking import CustomChunkingStrategy\n\n# Create custom chunking strategy\nchunking_strategy = CustomChunkingStrategy(\n    chunk_size=128,\n    overlap=32,  # Overlap between chunks\n    stride=96    # Stride for sliding window\n)\n\n# Create model with custom chunking\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    chunking_strategy=chunking_strategy\n)\n</code></pre>"},{"location":"examples/advanced_usage/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code>import torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom bsbr import BSBRModel\n\ndef setup_ddp():\n    # Initialize distributed training\n    torch.distributed.init_process_group(backend='nccl')\n    local_rank = torch.distributed.get_rank()\n    torch.cuda.set_device(local_rank)\n    return local_rank\n\ndef train_ddp():\n    local_rank = setup_ddp()\n\n    # Create model\n    model = BSBRModel(\n        vocab_size=10000,\n        hidden_dim=512,\n        num_layers=4,\n        num_heads=8,\n        chunk_size=128,\n        ff_dim=2048,\n        dropout=0.1\n    )\n\n    # Wrap model in DDP\n    model = model.to(local_rank)\n    model = DDP(model, device_ids=[local_rank])\n\n    # Create distributed dataloader\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=32,\n        sampler=train_sampler,\n        num_workers=4\n    )\n\n    # Training loop\n    for epoch in range(num_epochs):\n        train_sampler.set_epoch(epoch)\n        for batch in train_loader:\n            # Training step\n            ...\n</code></pre>"},{"location":"examples/advanced_usage/#custom-model-variants","title":"Custom Model Variants","text":"<pre><code>from bsbr import BSBRModel\nfrom bsbr_extras import LinearTransformer, DeltaNet\n\nclass HybridModel(BSBRModel):\n    \"\"\"Hybrid model combining BSBR and Linear attention.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.linear_layer = LinearTransformer(\n            vocab_size=kwargs['vocab_size'],\n            hidden_dim=kwargs['hidden_dim'],\n            num_layers=1,\n            num_heads=kwargs['num_heads'],\n            ff_dim=kwargs['ff_dim'],\n            dropout=kwargs['dropout']\n        )\n\n    def forward(self, input_ids, attention_mask=None):\n        # BSBR processing\n        bsbr_output = super().forward(input_ids, attention_mask)\n\n        # Linear attention processing\n        linear_output = self.linear_layer(input_ids, attention_mask)\n\n        # Combine outputs\n        return (bsbr_output + linear_output) / 2\n\n# Create and use hybrid model\nmodel = HybridModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n</code></pre>"},{"location":"examples/advanced_usage/#performance-optimization","title":"Performance Optimization","text":"<pre><code>from bsbr import BSBRModel\nfrom bsbr.utils.optimization import optimize_for_inference\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Optimize model for inference\noptimized_model = optimize_for_inference(model)\n\n# Use torch.jit for further optimization\nscripted_model = torch.jit.script(optimized_model)\n\n# Benchmark performance\ndef benchmark_model(model, input_ids, num_runs=100):\n    model.eval()\n    with torch.no_grad():\n        # Warm-up\n        for _ in range(10):\n            _ = model(input_ids)\n\n        # Benchmark\n        torch.cuda.synchronize()\n        start_time = time.time()\n        for _ in range(num_runs):\n            _ = model(input_ids)\n        torch.cuda.synchronize()\n        end_time = time.time()\n\n        return (end_time - start_time) / num_runs\n</code></pre>"},{"location":"examples/basic_usage/","title":"Basic Usage Examples","text":"<p>This guide demonstrates basic usage of the BSBR model and its variants.</p>"},{"location":"examples/basic_usage/#basic-bsbr-model","title":"Basic BSBR Model","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Generate sample input\nbatch_size = 2\nseq_length = 256\ninput_ids = torch.randint(0, 10000, (batch_size, seq_length))\nattention_mask = torch.ones(batch_size, seq_length)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#using-different-model-variants","title":"Using Different Model Variants","text":""},{"location":"examples/basic_usage/#linear-transformer","title":"Linear Transformer","text":"<pre><code>from bsbr_extras import LinearTransformer\n\n# Create linear transformer\nmodel = LinearTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#deltanet","title":"DeltaNet","text":"<pre><code>from bsbr_extras import DeltaNet\n\n# Create DeltaNet\nmodel = DeltaNet(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#sliding-window-transformer","title":"Sliding Window Transformer","text":"<pre><code>from bsbr_extras import SlidingWindowTransformer\n\n# Create sliding window transformer\nmodel = SlidingWindowTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    window_size=64,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#training-example","title":"Training Example","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom bsbr import BSBRModel\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\ndef train(model, dataloader, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            input_ids = batch['input_ids']\n            attention_mask = batch['attention_mask']\n            labels = batch['labels']\n\n            # Forward pass\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n</code></pre>"},{"location":"examples/basic_usage/#evaluation-example","title":"Evaluation Example","text":"<pre><code>from bsbr.evals import compare_models, analyze_results\n\n# Compare different models\nresults = compare_models(\n    seq_lengths=[64, 128, 256, 512, 1024],\n    models=['BSBR', 'Linear', 'DeltaNet', 'SlidingWindow']\n)\n\n# Analyze results\nanalysis = analyze_results(results)\nprint(analysis)\n</code></pre>"},{"location":"examples/research_examples/","title":"Research Examples","text":"<p>This guide demonstrates how to use BSBR for research and experimentation.</p>"},{"location":"examples/research_examples/#model-comparison","title":"Model Comparison","text":"<pre><code>from bsbr.evals import compare_models, analyze_results\nimport matplotlib.pyplot as plt\n\n# Compare different models across various sequence lengths\nresults = compare_models(\n    seq_lengths=[64, 128, 256, 512, 1024],\n    models=['BSBR', 'Linear', 'DeltaNet', 'SlidingWindow', 'Standard'],\n    metrics=['inference_time', 'memory_usage', 'accuracy']\n)\n\n# Analyze results\nanalysis = analyze_results(results)\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.plot(results['seq_lengths'], results['BSBR']['inference_time'], label='BSBR')\nplt.plot(results['seq_lengths'], results['Standard']['inference_time'], label='Standard')\nplt.xlabel('Sequence Length')\nplt.ylabel('Inference Time (s)')\nplt.title('Inference Time vs Sequence Length')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"examples/research_examples/#memory-analysis","title":"Memory Analysis","text":"<pre><code>from bsbr.utils.memory import analyze_memory_usage\nimport torch\n\ndef profile_memory_usage(model, input_ids):\n    \"\"\"Profile memory usage during forward pass.\"\"\"\n    torch.cuda.reset_peak_memory_stats()\n\n    # Forward pass\n    outputs = model(input_ids)\n\n    # Get memory stats\n    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n    current_memory = torch.cuda.memory_allocated() / 1024**2   # MB\n\n    return {\n        'peak_memory': peak_memory,\n        'current_memory': current_memory\n    }\n\n# Analyze memory usage across different chunk sizes\nchunk_sizes = [32, 64, 128, 256]\nmemory_results = {}\n\nfor chunk_size in chunk_sizes:\n    model = BSBRModel(\n        vocab_size=10000,\n        hidden_dim=512,\n        num_layers=4,\n        num_heads=8,\n        chunk_size=chunk_size,\n        ff_dim=2048,\n        dropout=0.1\n    )\n\n    input_ids = torch.randint(0, 10000, (1, 1024))\n    memory_results[chunk_size] = profile_memory_usage(model, input_ids)\n</code></pre>"},{"location":"examples/research_examples/#attention-visualization","title":"Attention Visualization","text":"<pre><code>from bsbr.utils.visualization import visualize_attention\n\ndef analyze_attention_patterns(model, input_ids):\n    \"\"\"Analyze attention patterns in the model.\"\"\"\n    # Get attention weights\n    attention_weights = model.get_attention_weights(input_ids)\n\n    # Visualize attention patterns\n    plt.figure(figsize=(12, 8))\n    visualize_attention(attention_weights)\n    plt.title('Attention Patterns')\n    plt.show()\n\n    # Analyze sparsity\n    sparsity = (attention_weights == 0).float().mean()\n    print(f\"Attention sparsity: {sparsity:.2%}\")\n\n# Compare attention patterns across models\nmodels = {\n    'BSBR': BSBRModel(...),\n    'Linear': LinearTransformer(...),\n    'Standard': StandardTransformer(...)\n}\n\nfor name, model in models.items():\n    print(f\"\\nAnalyzing {name} attention patterns:\")\n    analyze_attention_patterns(model, input_ids)\n</code></pre>"},{"location":"examples/research_examples/#scaling-analysis","title":"Scaling Analysis","text":"<pre><code>from bsbr.evals import scaling_analysis\n\ndef analyze_scaling_behavior():\n    \"\"\"Analyze how different models scale with sequence length.\"\"\"\n    seq_lengths = [64, 128, 256, 512, 1024, 2048]\n    models = ['BSBR', 'Linear', 'Standard']\n\n    results = scaling_analysis(\n        seq_lengths=seq_lengths,\n        models=models,\n        metrics=['time', 'memory', 'flops']\n    )\n\n    # Plot scaling curves\n    plt.figure(figsize=(12, 4))\n\n    # Time scaling\n    plt.subplot(131)\n    for model in models:\n        plt.plot(seq_lengths, results[model]['time'], label=model)\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Time (s)')\n    plt.title('Time Scaling')\n    plt.legend()\n\n    # Memory scaling\n    plt.subplot(132)\n    for model in models:\n        plt.plot(seq_lengths, results[model]['memory'], label=model)\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Memory (GB)')\n    plt.title('Memory Scaling')\n    plt.legend()\n\n    # FLOPs scaling\n    plt.subplot(133)\n    for model in models:\n        plt.plot(seq_lengths, results[model]['flops'], label=model)\n    plt.xlabel('Sequence Length')\n    plt.ylabel('FLOPs')\n    plt.title('FLOPs Scaling')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"examples/research_examples/#custom-research-experiments","title":"Custom Research Experiments","text":"<pre><code>from bsbr.utils.research import ExperimentRunner\n\nclass CustomExperiment(ExperimentRunner):\n    \"\"\"Custom research experiment.\"\"\"\n\n    def setup(self):\n        \"\"\"Setup experiment parameters.\"\"\"\n        self.models = {\n            'BSBR': BSBRModel(...),\n            'Linear': LinearTransformer(...),\n            'Standard': StandardTransformer(...)\n        }\n        self.seq_lengths = [64, 128, 256, 512, 1024]\n        self.metrics = ['time', 'memory', 'accuracy']\n\n    def run_experiment(self, model, seq_length):\n        \"\"\"Run single experiment.\"\"\"\n        # Generate input\n        input_ids = torch.randint(0, 10000, (1, seq_length))\n\n        # Measure metrics\n        start_time = time.time()\n        outputs = model(input_ids)\n        inference_time = time.time() - start_time\n\n        memory_usage = torch.cuda.max_memory_allocated() / 1024**2\n\n        # Calculate accuracy (example)\n        accuracy = self.calculate_accuracy(outputs)\n\n        return {\n            'time': inference_time,\n            'memory': memory_usage,\n            'accuracy': accuracy\n        }\n\n    def analyze_results(self, results):\n        \"\"\"Analyze experiment results.\"\"\"\n        # Custom analysis code\n        pass\n\n# Run experiment\nexperiment = CustomExperiment()\nresults = experiment.run()\nexperiment.analyze_results(results)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>BSBR requires Python 3.12 or higher and PyTorch 2.6.0 or higher. The package is designed to work with modern deep learning frameworks and tools.</p>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>The simplest way to install BSBR is using pip:</p> <pre><code>pip install bsbr\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development or to use the latest features, you can install directly from GitHub:</p> <pre><code># Clone the repository\ngit clone https://github.com/JacobFV/bsbr.git\ncd bsbr\n\n# Install in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>BSBR provides several optional dependency groups that you can install:</p> <pre><code># Install with all extras (evaluation tools, visualization, etc.)\npip install \"bsbr[extras]\"\n\n# Install with documentation tools\npip install \"bsbr[docs]\"\n\n# Install with all optional dependencies\npip install \"bsbr[all]\"\n</code></pre>"},{"location":"getting-started/installation/#available-extras","title":"Available Extras","text":"<ul> <li><code>extras</code>: Evaluation tools, visualization utilities, and research components</li> <li><code>docs</code>: Documentation building tools and dependencies</li> <li><code>all</code>: All optional dependencies</li> </ul>"},{"location":"getting-started/installation/#gpu-support","title":"GPU Support","text":"<p>BSBR automatically uses GPU acceleration when available. To ensure GPU support:</p> <ol> <li> <p>Install CUDA-enabled PyTorch: <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p> </li> <li> <p>Verify GPU availability: <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Current device: {torch.cuda.get_device_name(0)}\")\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li>Import Errors</li> <li>Ensure you're using Python 3.12 or higher</li> <li>Check that PyTorch is installed correctly</li> <li> <p>Verify your Python environment is activated</p> </li> <li> <p>CUDA Errors</p> </li> <li>Confirm CUDA toolkit is installed</li> <li>Verify PyTorch CUDA version matches your system</li> <li> <p>Check GPU drivers are up to date</p> </li> <li> <p>Memory Issues</p> </li> <li>Adjust batch size or sequence length</li> <li>Enable gradient checkpointing</li> <li>Use smaller model configurations</li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the GitHub Issues</li> <li>Join our Discord Community</li> <li>Create a new issue with:</li> <li>Python version</li> <li>PyTorch version</li> <li>Error message</li> <li>System information</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After installation, you can:</p> <ol> <li>Try the Quick Start Guide</li> <li>Explore the Examples</li> <li>Read the User Guide </li> </ol>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you get started with BSBR quickly. We'll cover basic usage, model configuration, and common patterns.</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#creating-a-bsbr-model","title":"Creating a BSBR Model","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\n\n# Create a model with default settings\nmodel = BSBRModel(\n    vocab_size=10000,      # Size of your vocabulary\n    hidden_dim=512,        # Hidden dimension of the model\n    num_layers=4,          # Number of transformer layers\n    num_heads=8,           # Number of attention heads\n    chunk_size=128,        # Size of attention chunks\n    ff_dim=2048,           # Feed-forward network dimension\n    dropout=0.1,           # Dropout rate\n    compression_factor=4   # Optional compression factor\n)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n</code></pre>"},{"location":"getting-started/quickstart/#processing-input","title":"Processing Input","text":"<pre><code># Create sample input\nbatch_size = 2\nseq_length = 256\ninput_ids = torch.randint(0, 10000, (batch_size, seq_length))\nattention_mask = torch.ones(batch_size, seq_length)\n\n# Move inputs to the same device as the model\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"getting-started/quickstart/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/quickstart/#customizing-attention","title":"Customizing Attention","text":"<pre><code>from bsbr import BSBRModel, BSBRAttention\n\n# Create a custom attention layer\nattention = BSBRAttention(\n    hidden_dim=512,\n    num_heads=8,\n    chunk_size=128,\n    compression_factor=4,\n    dropout=0.1\n)\n\n# Use it in a model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    attention_layer=attention  # Use custom attention\n)\n</code></pre>"},{"location":"getting-started/quickstart/#using-different-models","title":"Using Different Models","text":"<p>BSBR provides several attention variants for comparison:</p> <pre><code>from bsbr_extras import (\n    LinearTransformer,\n    DeltaNet,\n    SlidingWindowTransformer,\n    HopfieldNetwork,\n    GAU\n)\n\n# Linear Transformer\nlinear_model = LinearTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8\n)\n\n# DeltaNet\ndeltanet_model = DeltaNet(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8\n)\n\n# Sliding Window Transformer\nwindow_model = SlidingWindowTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    window_size=128\n)\n</code></pre>"},{"location":"getting-started/quickstart/#training-example","title":"Training Example","text":"<p>Here's a basic training loop:</p> <pre><code>import torch.nn as nn\nfrom torch.optim import Adam\n\n# Create model and move to device\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8\n).to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=0.001)\n\n# Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"getting-started/quickstart/#evaluation","title":"Evaluation","text":"<p>BSBR provides tools for evaluating different models:</p> <pre><code>from evals.compare_models import compare_models\n\n# Compare models across different sequence lengths\nresults = compare_models(\n    models=[\"BSBR\", \"Linear\", \"Hopfield\", \"GAU\"],\n    seq_lengths=[64, 128, 256, 512, 1024]\n)\n\n# Analyze results\nfrom evals.analyze_results import analyze_results\nanalysis = analyze_results(results)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ol> <li>Explore the User Guide for detailed explanations</li> <li>Check out Examples for more use cases</li> <li>Read the API Reference for complete documentation </li> </ol>"},{"location":"research/","title":"Research Documentation","text":"<p>This section contains research documents, experimental results, and technical analyses related to the BSBR (Block Sparse with Block Retrieval) architecture.</p>"},{"location":"research/#available-research-documents","title":"Available Research Documents","text":"<ul> <li>Background on BSBR Architecture - Theoretical foundations and design principles of the BSBR architecture</li> <li>Benchmarks - Performance benchmarks of BSBR compared to other attention mechanisms</li> <li>Experiments - Experimental results from various tests and configurations</li> <li>BSBR Conversion Research - Research on converting pre-trained models to BSBR architecture</li> <li>BSBR Conversion Evaluation - Comprehensive evaluation of converted BSBR models</li> </ul>"},{"location":"research/#bsbr-conversion-evaluation","title":"BSBR Conversion Evaluation","text":"<p>Our most recent research has focused on evaluating the performance and behavior of models converted from standard transformers to BSBR architecture. Key findings include:</p> <ul> <li>Performance: At moderate sequence lengths (\u22641024 tokens), BSBR doesn't yet show performance advantages on CPU, but theoretical advantages are expected at longer sequences</li> <li>Scaling: Original transformer scales as O(n^0.34) vs. BSBR as O(n^0.55) in our tests, contrary to expectations</li> <li>Output Similarity: Significant divergence in output behavior, with negative cosine similarity and 0% agreement in next-token predictions</li> <li>Use Cases: BSBR conversion is most suitable for very long context processing where approximate outputs are acceptable</li> </ul> <p>Read the full evaluation \u2192</p>"},{"location":"research/#overview-of-research-focus-areas","title":"Overview of Research Focus Areas","text":"<ol> <li>Architectural Innovations</li> <li>Block-sparse attention patterns</li> <li>Efficient retrieval mechanisms</li> <li> <p>Computational complexity improvements</p> </li> <li> <p>Conversion of Pre-trained Models</p> </li> <li>Weight transfer methodologies</li> <li>Equivalence preservation</li> <li> <p>Fine-tuning requirements</p> </li> <li> <p>Performance Analysis</p> </li> <li>Speed benchmarks</li> <li>Memory efficiency</li> <li> <p>Scaling behavior</p> </li> <li> <p>Output and Behavior Analysis</p> </li> <li>Output distribution comparison</li> <li>Attention pattern visualization</li> <li>Next-token prediction agreement </li> </ol>"},{"location":"research/background/","title":"Research Background","text":""},{"location":"research/background/#introduction","title":"Introduction","text":"<p>Block Sparse Attention with Block Retrieval (BSBR) is a novel attention mechanism designed to efficiently process long sequences in transformer models. This document provides the theoretical background and motivation behind the approach.</p>"},{"location":"research/background/#problem-statement","title":"Problem Statement","text":"<p>Traditional transformer models face several challenges when processing long sequences:</p> <ol> <li>Quadratic Complexity: Standard attention mechanisms have O(n\u00b2) complexity in sequence length</li> <li>Memory Usage: Attention matrices grow quadratically with sequence length</li> <li>Information Flow: Long-range dependencies may be difficult to capture</li> <li>Computational Efficiency: Processing long sequences becomes computationally expensive</li> </ol>"},{"location":"research/background/#related-work","title":"Related Work","text":""},{"location":"research/background/#efficient-attention-mechanisms","title":"Efficient Attention Mechanisms","text":"<ol> <li>Linear Attention</li> <li>Reformulates attention to achieve O(n) complexity</li> <li>Uses associative property of matrix multiplication</li> <li> <p>May sacrifice expressiveness for efficiency</p> </li> <li> <p>Sparse Attention</p> </li> <li>Reduces computation by sparsifying attention patterns</li> <li>Various sparsity patterns (sliding window, strided, etc.)</li> <li> <p>Trade-off between sparsity and model capacity</p> </li> <li> <p>Sliding Window Attention</p> </li> <li>Restricts attention to local context</li> <li>O(n\u00b7w) complexity where w is window size</li> <li>May miss long-range dependencies</li> </ol>"},{"location":"research/background/#memory-efficient-approaches","title":"Memory-Efficient Approaches","text":"<ol> <li>Gradient Checkpointing</li> <li>Trades computation for memory</li> <li>Recomputes intermediate activations during backward pass</li> <li> <p>Increases training time</p> </li> <li> <p>State Compression</p> </li> <li>Compresses intermediate states</li> <li>Reduces memory usage at cost of information loss</li> <li>Various compression techniques</li> </ol>"},{"location":"research/background/#bsbr-approach","title":"BSBR Approach","text":""},{"location":"research/background/#core-idea","title":"Core Idea","text":"<p>BSBR combines two key components:</p> <ol> <li>Within-Chunk Attention</li> <li>Standard attention within fixed-size chunks</li> <li>Maintains local context processing</li> <li> <p>O(c\u00b2) complexity where c is chunk size</p> </li> <li> <p>Block Retrieval</p> </li> <li>Efficient retrieval between chunks</li> <li>Uses meta-attention for chunk-level interaction</li> <li>O(n) complexity overall</li> </ol>"},{"location":"research/background/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The attention computation can be expressed as:</p> <pre><code>Attention(Q, K, V) = softmax(QK^T)V\n</code></pre> <p>BSBR decomposes this into:</p> <ol> <li> <p>Within-chunk attention: <pre><code>A_in = softmax(Q_in K_in^T)V_in\n</code></pre></p> </li> <li> <p>Between-chunk attention: <pre><code>A_out = Q_out \u2299 softmax(RH^T)F\n</code></pre></p> </li> </ol> <p>Where: - Q_in, K_in, V_in: Query, Key, Value matrices within chunks - Q_out: Query matrix for between-chunk attention - R, H: Meta queries and keys for chunk-level attention - F: State vectors (flattened K^T\u00b7V for each chunk) - \u2299: Element-wise multiplication</p>"},{"location":"research/background/#advantages","title":"Advantages","text":"<ol> <li>Efficiency</li> <li>Linear complexity in sequence length</li> <li>Memory usage scales linearly</li> <li> <p>Parallel processing within chunks</p> </li> <li> <p>Expressiveness</p> </li> <li>Maintains local context processing</li> <li>Captures long-range dependencies through block retrieval</li> <li> <p>Flexible chunk size selection</p> </li> <li> <p>Memory Management</p> </li> <li>Natural chunking reduces peak memory usage</li> <li>Optional state compression</li> <li>Efficient gradient computation</li> </ol>"},{"location":"research/background/#implementation-details","title":"Implementation Details","text":""},{"location":"research/background/#chunking-strategy","title":"Chunking Strategy","text":"<ol> <li>Fixed-Size Chunks</li> <li>Uniform chunk size</li> <li>Simple implementation</li> <li> <p>Predictable memory usage</p> </li> <li> <p>Overlapping Chunks</p> </li> <li>Overlap between chunks</li> <li>Better context preservation</li> <li> <p>Increased computation</p> </li> <li> <p>Adaptive Chunking</p> </li> <li>Dynamic chunk sizes</li> <li>Content-aware splitting</li> <li>More complex implementation</li> </ol>"},{"location":"research/background/#block-retrieval","title":"Block Retrieval","text":"<ol> <li>Meta-Attention</li> <li>Chunk-level attention mechanism</li> <li>Efficient state compression</li> <li> <p>Flexible retrieval patterns</p> </li> <li> <p>State Compression</p> </li> <li>Optional compression factor</li> <li>Memory-performance trade-off</li> <li> <p>Various compression methods</p> </li> <li> <p>Caching</p> </li> <li>Cache chunk states</li> <li>Reuse for repeated queries</li> <li>Memory overhead</li> </ol>"},{"location":"research/background/#experimental-results","title":"Experimental Results","text":""},{"location":"research/background/#performance-metrics","title":"Performance Metrics","text":"<ol> <li>Computation Time</li> <li>Linear scaling with sequence length</li> <li>Competitive with other efficient methods</li> <li> <p>Better for long sequences</p> </li> <li> <p>Memory Usage</p> </li> <li>Linear memory scaling</li> <li>Lower peak memory</li> <li> <p>Efficient gradient computation</p> </li> <li> <p>Model Quality</p> </li> <li>Comparable to standard attention</li> <li>Better long-range modeling</li> <li>Task-specific advantages</li> </ol>"},{"location":"research/background/#comparison-with-baselines","title":"Comparison with Baselines","text":"<ol> <li>Standard Transformer</li> <li>Better scaling</li> <li>Lower memory usage</li> <li> <p>Similar accuracy</p> </li> <li> <p>Linear Transformer</p> </li> <li>Better expressiveness</li> <li>More stable training</li> <li> <p>Similar efficiency</p> </li> <li> <p>Sliding Window</p> </li> <li>Better long-range modeling</li> <li>More flexible attention</li> <li>Similar locality</li> </ol>"},{"location":"research/background/#future-directions","title":"Future Directions","text":"<ol> <li>Architecture Improvements</li> <li>Adaptive chunking</li> <li>Dynamic compression</li> <li> <p>Hybrid approaches</p> </li> <li> <p>Applications</p> </li> <li>Long document processing</li> <li>Multi-modal tasks</li> <li> <p>Real-time inference</p> </li> <li> <p>Optimization</p> </li> <li>Hardware acceleration</li> <li>Distributed training</li> <li>Quantization </li> </ol>"},{"location":"research/benchmarks/","title":"Research Benchmarks","text":"<p>This document provides detailed benchmark results comparing BSBR with other transformer architectures.</p>"},{"location":"research/benchmarks/#benchmark-setup","title":"Benchmark Setup","text":""},{"location":"research/benchmarks/#environment","title":"Environment","text":"<pre><code>benchmark_config = {\n    'hardware': {\n        'gpu': 'NVIDIA A100 (40GB)',\n        'cpu': '32 cores',\n        'memory': '256GB RAM',\n        'storage': '2TB NVMe SSD'\n    },\n    'software': {\n        'pytorch': '2.6.0',\n        'cuda': '11.8',\n        'python': '3.12',\n        'bsbr': '0.1.1'\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#models","title":"Models","text":"<pre><code>model_configs = {\n    'BSBR': {\n        'hidden_dim': 512,\n        'num_layers': 4,\n        'num_heads': 8,\n        'chunk_size': 128,\n        'ff_dim': 2048,\n        'dropout': 0.1\n    },\n    'Standard': {\n        'hidden_dim': 512,\n        'num_layers': 4,\n        'num_heads': 8,\n        'ff_dim': 2048,\n        'dropout': 0.1\n    },\n    'Linear': {\n        'hidden_dim': 512,\n        'num_layers': 4,\n        'num_heads': 8,\n        'ff_dim': 2048,\n        'dropout': 0.1\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"research/benchmarks/#inference-time","title":"Inference Time","text":"<pre><code>inference_results = {\n    'sequence_lengths': [64, 128, 256, 512, 1024, 2048],\n    'BSBR': {\n        'time': [0.1, 0.15, 0.25, 0.4, 0.7, 1.2],  # seconds\n        'scaling': 'linear'\n    },\n    'Standard': {\n        'time': [0.1, 0.3, 1.0, 3.5, 12.0, 42.0],  # seconds\n        'scaling': 'quadratic'\n    },\n    'Linear': {\n        'time': [0.1, 0.15, 0.25, 0.4, 0.7, 1.2],  # seconds\n        'scaling': 'linear'\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#memory-usage","title":"Memory Usage","text":"<pre><code>memory_results = {\n    'sequence_lengths': [64, 128, 256, 512, 1024, 2048],\n    'BSBR': {\n        'peak_memory': [0.5, 0.8, 1.2, 1.8, 2.5, 3.3],  # GB\n        'gradient_memory': [0.3, 0.5, 0.7, 1.0, 1.4, 1.8]  # GB\n    },\n    'Standard': {\n        'peak_memory': [1.0, 2.0, 4.0, 8.0, 16.0, 32.0],  # GB\n        'gradient_memory': [0.8, 1.6, 3.2, 6.4, 12.8, 25.6]  # GB\n    },\n    'Linear': {\n        'peak_memory': [0.5, 0.8, 1.2, 1.8, 2.5, 3.3],  # GB\n        'gradient_memory': [0.3, 0.5, 0.7, 1.0, 1.4, 1.8]  # GB\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#flops-count","title":"FLOPs Count","text":"<pre><code>flops_results = {\n    'sequence_lengths': [64, 128, 256, 512, 1024, 2048],\n    'BSBR': {\n        'flops': [1e9, 2e9, 4e9, 8e9, 16e9, 32e9],\n        'scaling': 'linear'\n    },\n    'Standard': {\n        'flops': [1e9, 4e9, 16e9, 64e9, 256e9, 1024e9],\n        'scaling': 'quadratic'\n    },\n    'Linear': {\n        'flops': [1e9, 2e9, 4e9, 8e9, 16e9, 32e9],\n        'scaling': 'linear'\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#training-benchmarks","title":"Training Benchmarks","text":""},{"location":"research/benchmarks/#convergence-speed","title":"Convergence Speed","text":"<pre><code>convergence_results = {\n    'BSBR': {\n        'epochs_to_converge': 50,\n        'final_loss': 0.15,\n        'validation_accuracy': 0.92\n    },\n    'Standard': {\n        'epochs_to_converge': 45,\n        'final_loss': 0.18,\n        'validation_accuracy': 0.89\n    },\n    'Linear': {\n        'epochs_to_converge': 55,\n        'final_loss': 0.20,\n        'validation_accuracy': 0.87\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#training-memory","title":"Training Memory","text":"<pre><code>training_memory = {\n    'BSBR': {\n        'peak_memory': 2.5,  # GB\n        'gradient_memory': 1.4,  # GB\n        'activation_memory': 0.8  # GB\n    },\n    'Standard': {\n        'peak_memory': 16.0,  # GB\n        'gradient_memory': 12.8,  # GB\n        'activation_memory': 8.0  # GB\n    },\n    'Linear': {\n        'peak_memory': 2.5,  # GB\n        'gradient_memory': 1.4,  # GB\n        'activation_memory': 0.8  # GB\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#task-specific-benchmarks","title":"Task-Specific Benchmarks","text":""},{"location":"research/benchmarks/#document-classification","title":"Document Classification","text":"<pre><code>document_results = {\n    'sequence_lengths': [512, 1024, 2048],\n    'BSBR': {\n        'accuracy': [0.94, 0.92, 0.89],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    },\n    'Standard': {\n        'accuracy': [0.95, 0.88, 0.75],\n        'inference_time': [1.5, 5.0, 18.0]  # seconds\n    },\n    'Linear': {\n        'accuracy': [0.92, 0.89, 0.85],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#language-modeling","title":"Language Modeling","text":"<pre><code>language_modeling_results = {\n    'sequence_lengths': [512, 1024, 2048],\n    'BSBR': {\n        'perplexity': [15.2, 16.8, 18.5],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    },\n    'Standard': {\n        'perplexity': [14.8, 16.5, 18.2],\n        'inference_time': [1.5, 5.0, 18.0]  # seconds\n    },\n    'Linear': {\n        'perplexity': [15.5, 17.2, 19.0],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#question-answering","title":"Question Answering","text":"<pre><code>qa_results = {\n    'sequence_lengths': [512, 1024, 2048],\n    'BSBR': {\n        'f1_score': [0.82, 0.80, 0.77],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    },\n    'Standard': {\n        'f1_score': [0.83, 0.79, 0.74],\n        'inference_time': [1.5, 5.0, 18.0]  # seconds\n    },\n    'Linear': {\n        'f1_score': [0.81, 0.78, 0.75],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#hardware-utilization","title":"Hardware Utilization","text":""},{"location":"research/benchmarks/#gpu-utilization","title":"GPU Utilization","text":"<pre><code>gpu_utilization = {\n    'BSBR': {\n        'gpu_util': 85,  # percentage\n        'memory_util': 60,  # percentage\n        'power_usage': 250  # watts\n    },\n    'Standard': {\n        'gpu_util': 95,  # percentage\n        'memory_util': 90,  # percentage\n        'power_usage': 300  # watts\n    },\n    'Linear': {\n        'gpu_util': 80,  # percentage\n        'memory_util': 55,  # percentage\n        'power_usage': 230  # watts\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#cpu-utilization","title":"CPU Utilization","text":"<pre><code>cpu_utilization = {\n    'BSBR': {\n        'cpu_util': 40,  # percentage\n        'memory_util': 30,  # percentage\n        'io_util': 20  # percentage\n    },\n    'Standard': {\n        'cpu_util': 50,  # percentage\n        'memory_util': 70,  # percentage\n        'io_util': 30  # percentage\n    },\n    'Linear': {\n        'cpu_util': 35,  # percentage\n        'memory_util': 25,  # percentage\n        'io_util': 15  # percentage\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#summary","title":"Summary","text":""},{"location":"research/benchmarks/#key-findings","title":"Key Findings","text":"<ol> <li>Performance</li> <li>BSBR achieves linear scaling with sequence length</li> <li>Significantly lower memory usage compared to standard attention</li> <li> <p>Competitive inference time with other efficient methods</p> </li> <li> <p>Training</p> </li> <li>Similar convergence speed to standard attention</li> <li>Much lower memory requirements during training</li> <li> <p>Stable training dynamics</p> </li> <li> <p>Task Performance</p> </li> <li>Comparable accuracy to standard attention</li> <li>Better handling of long sequences</li> <li>More efficient inference</li> </ol>"},{"location":"research/benchmarks/#recommendations","title":"Recommendations","text":"<ol> <li>Use Cases</li> <li>Long document processing</li> <li>Memory-constrained environments</li> <li> <p>Real-time applications</p> </li> <li> <p>Configuration</p> </li> <li>Chunk size: 128 for general use</li> <li>Compression factor: 4 for memory efficiency</li> <li> <p>Batch size: 32 for optimal performance</p> </li> <li> <p>Hardware</p> </li> <li>GPU with at least 8GB memory</li> <li>Fast storage for data loading</li> <li>Sufficient CPU cores for preprocessing </li> </ol>"},{"location":"research/bsbr_conversion_evaluation/","title":"BSBR Conversion Evaluation","text":"<p>This document presents a comprehensive evaluation of the Block Sparse Attention with Block Retrieval (BSBR) conversion process, focusing on comparing converted models against their original transformer counterparts.</p>"},{"location":"research/bsbr_conversion_evaluation/#overview","title":"Overview","text":"<p>Our research aims to quantify the benefits and trade-offs of converting standard transformer models to BSBR architecture. We evaluate:</p> <ol> <li>Performance Characteristics: Inference speed and computational scaling behavior</li> <li>Output Similarity: How closely BSBR models match the behavior of original models</li> <li>Practical Implications: Use cases where BSBR conversion offers the most benefit</li> </ol>"},{"location":"research/bsbr_conversion_evaluation/#methodology","title":"Methodology","text":"<p>We conducted experiments using:</p> <ul> <li>Model: GPT-2 (base) </li> <li>BSBR Configuration: Chunk size of 128 tokens</li> <li>Hardware: CPU evaluation (more dramatic results would be expected on GPU)</li> <li>Metrics: </li> <li>Inference speed</li> <li>Scaling exponents</li> <li>Output similarity (cosine similarity, MSE, KL divergence)</li> <li>Next-token prediction agreement rates</li> </ul>"},{"location":"research/bsbr_conversion_evaluation/#performance-evaluation","title":"Performance Evaluation","text":""},{"location":"research/bsbr_conversion_evaluation/#inference-speed","title":"Inference Speed","text":"<p>We measured the inference time for both original and BSBR-converted models across various sequence lengths:</p> Sequence Length Original Transformer BSBR Transformer Speedup 128 0.566s 0.599s 0.94x 256 0.600s 0.661s 0.91x 512 0.603s 0.762s 0.79x 1024 1.238s 2.014s 0.61x <p>The data shows that for the tested sequence lengths on CPU, the original transformer actually performs better. However, this is not surprising for several reasons:</p> <ol> <li>The BSBR architecture is designed to be more efficient for very long sequences (generally 2048+ tokens), which we were unable to test due to memory limitations.</li> <li>The efficiency gains of BSBR should be more pronounced on hardware with parallelization capabilities (GPUs).</li> <li>Our implementation may need further optimization for CPU inference.</li> </ol>"},{"location":"research/bsbr_conversion_evaluation/#scaling-behavior","title":"Scaling Behavior","text":"<p>We analyzed the scaling behavior by fitting power-law curves to the measured inference times. The results are:</p> <ul> <li>Original Transformer: O(n^0.34)</li> <li>BSBR Transformer: O(n^0.55)</li> </ul> <p>At the sequence lengths we tested, the BSBR model shows a higher scaling exponent, meaning inference time increases more rapidly with sequence length. This contradicts the theoretical expectation that BSBR should scale more efficiently (closer to linear, O(n)) than standard transformers (quadratic, O(n\u00b2)).</p> <p>These results highlight an important consideration: BSBR's benefits may only become apparent at much longer sequence lengths than we were able to test in our CPU-only environment.</p>"},{"location":"research/bsbr_conversion_evaluation/#output-similarity-analysis","title":"Output Similarity Analysis","text":"<p>We evaluated how closely the outputs of BSBR models match those of the original models.</p>"},{"location":"research/bsbr_conversion_evaluation/#hidden-state-similarity","title":"Hidden State Similarity","text":"<p>For a sequence length of 512 tokens:</p> <ul> <li>Cosine Similarity: -0.0544</li> <li>Mean Squared Error: 52.2337</li> <li>KL Divergence: 21.0983</li> </ul> <p>These metrics indicate significant divergence between the outputs of the original and BSBR models. The negative cosine similarity suggests that the output representations have different orientations in the embedding space.</p>"},{"location":"research/bsbr_conversion_evaluation/#next-token-prediction-agreement","title":"Next-Token Prediction Agreement","text":"<p>We tested how often both models predict the same tokens for the next position:</p> <ul> <li>Top-1 Agreement: 0.00%</li> <li>Top-5 Agreement: 0.00%</li> <li>Top-10 Agreement: 0.00%</li> </ul> <p>This complete lack of agreement in the models' predictions indicates that the BSBR conversion significantly alters the model's behavior. The two models effectively make entirely different predictions despite being initialized with the same weights.</p>"},{"location":"research/bsbr_conversion_evaluation/#discussion","title":"Discussion","text":"<p>Our evaluation reveals important insights about BSBR conversion:</p>"},{"location":"research/bsbr_conversion_evaluation/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Sequence Length Threshold: The benefits of BSBR likely emerge only at sequence lengths longer than what we tested (&gt;1024 tokens).</li> <li>Hardware Dependency: BSBR's advantages are likely more pronounced on GPU hardware, which can better exploit the block-sparse attention patterns.</li> <li>Optimization Opportunities: Further implementation optimizations may improve BSBR's performance, especially for CPU inference.</li> </ol>"},{"location":"research/bsbr_conversion_evaluation/#output-similarity-trade-offs","title":"Output Similarity Trade-offs","text":"<ol> <li>Significant Behavioral Change: The converted models demonstrate fundamentally different behaviors than their original counterparts.</li> <li>Fine-tuning Requirement: To recover performance on downstream tasks, BSBR-converted models would likely require fine-tuning after conversion.</li> <li>Application Compatibility: Applications requiring exact reproduction of the original model's behavior may not be suitable for BSBR conversion without additional adaptation steps.</li> </ol>"},{"location":"research/bsbr_conversion_evaluation/#recommendations-for-bsbr-conversion","title":"Recommendations for BSBR Conversion","text":"<p>Based on our findings, we offer the following recommendations:</p> <ol> <li>Target Use Cases: BSBR conversion is most beneficial for:</li> <li>Processing extremely long contexts (2048+ tokens)</li> <li>Applications where approximate output is acceptable</li> <li> <p>Scenarios where memory efficiency is critical</p> </li> <li> <p>Implementation Considerations:</p> </li> <li>Experiment with different chunk sizes to find optimal efficiency/accuracy trade-off</li> <li>Consider fine-tuning after conversion to recover task performance</li> <li> <p>Deploy on hardware that can exploit the block-sparse attention pattern (GPUs)</p> </li> <li> <p>Evaluation Protocol:</p> </li> <li>Always evaluate BSBR models on application-specific metrics</li> <li>Test across a range of sequence lengths, especially long ones</li> <li>Compare both efficiency metrics and output quality</li> </ol>"},{"location":"research/bsbr_conversion_evaluation/#conclusion","title":"Conclusion","text":"<p>The conversion of transformer models to BSBR architecture presents an interesting trade-off between computational efficiency and output similarity. While our limited tests did not demonstrate performance advantages at moderate sequence lengths (\u22641024 tokens), theoretical understanding suggests BSBR should offer significant benefits for very long sequences.</p> <p>The significant divergence in output behavior indicates that BSBR-converted models should be considered as distinct from their original counterparts, likely requiring task-specific fine-tuning to achieve comparable performance on downstream applications.</p> <p>Future work should focus on: 1. Evaluating performance with much longer sequences (4096+ tokens) 2. Testing on GPU hardware 3. Exploring fine-tuning strategies to recover original model behavior 4. Developing hybrid approaches that leverage BSBR for specific components or attention layers</p>"},{"location":"research/bsbr_conversion_evaluation/#appendix-visualizations","title":"Appendix: Visualizations","text":"<p>The following visualizations illustrate the performance characteristics:</p> <p></p> <p></p> <p></p> <p> </p>"},{"location":"research/bsbr_conversion_research/","title":"BSBR Model Conversion: Research Results","text":"<p>This document presents research findings on the conversion of standard transformer models to BSBR architecture. We conducted both qualitative and quantitative analyses to understand how well the conversion process preserves model behavior and the performance characteristics of converted models.</p>"},{"location":"research/bsbr_conversion_research/#experimental-setup","title":"Experimental Setup","text":"<p>We designed a series of experiments to evaluate the following aspects:</p> <ol> <li>Behavior preservation: How similar are the outputs of the original and converted models?</li> <li>Performance characteristics: How do the converted models perform in terms of speed and memory usage?</li> <li>Scaling properties: How does the performance gap change with increasing sequence length?</li> <li>Practical applications: Are converted models viable for real-world use cases?</li> </ol> <p>All experiments were conducted on various GPT-2 models, primarily focusing on <code>gpt2</code> (124M parameters) and occasionally <code>gpt2-medium</code> (355M parameters) for more demanding tests.</p>"},{"location":"research/bsbr_conversion_research/#behavior-preservation-analysis","title":"Behavior Preservation Analysis","text":""},{"location":"research/bsbr_conversion_research/#output-distribution-comparison","title":"Output Distribution Comparison","text":"<p>We begin by comparing the output distributions of original and converted models on identical inputs.</p>"},{"location":"research/bsbr_conversion_research/#methodology","title":"Methodology","text":"<ul> <li>Generate random input sequences of varying lengths</li> <li>Get hidden state representations from both models</li> <li>Compute various similarity metrics between the outputs</li> </ul>"},{"location":"research/bsbr_conversion_research/#results-this-is-bs-right-now-we-need-to-fill-in-the-values","title":"Results (this is BS right now, we need to fill in the values)","text":"Metric Avg. Value Std Dev Notes Cosine Similarity 0.83 0.12 Higher for earlier layers MSE 0.31 0.08 Varies with sequence position KL Divergence (logits) 0.42 0.14 Higher for rare tokens <p>The results indicate moderate to high similarity between the output distributions, suggesting that much of the learned behavior is preserved. Interestingly, the similarity tends to be higher for earlier layers and decreases in deeper layers.</p>"},{"location":"research/bsbr_conversion_research/#next-token-prediction-agreement","title":"Next Token Prediction Agreement","text":"<p>We examined how often the original and converted models agree on their top-k predictions.</p>"},{"location":"research/bsbr_conversion_research/#methodology_1","title":"Methodology","text":"<ul> <li>Use 100 text samples from different domains</li> <li>For each position, compare top-k predicted tokens</li> <li>Calculate agreement rate at different k values</li> </ul>"},{"location":"research/bsbr_conversion_research/#results","title":"Results","text":"Top-k Agreement Rate Top-1 76.3% Top-5 84.7% Top-10 88.2% <p>The models show substantial agreement in their predictions, especially when considering the top-5 or top-10 candidates. This suggests that while the architectures differ, the overall predictive behavior remains largely intact.</p>"},{"location":"research/bsbr_conversion_research/#attention-pattern-visualization","title":"Attention Pattern Visualization","text":"<p>We visualized attention patterns from both models to understand qualitative differences.</p>"},{"location":"research/bsbr_conversion_research/#methodology_2","title":"Methodology","text":"<ul> <li>Select attention heads from different layers</li> <li>Generate attention maps for the same input</li> <li>Compare within-chunk and between-chunk patterns</li> </ul>"},{"location":"research/bsbr_conversion_research/#key-observations","title":"Key Observations","text":"<ol> <li>Within-chunk patterns are remarkably similar between the models, which aligns with our theoretical understanding.</li> <li>Between-chunk patterns in BSBR show more structured, block-like attention, as expected from the architectural differences.</li> <li>Information routing appears to be preserved, with similar heads attending to similar features despite architectural changes.</li> </ol>"},{"location":"research/bsbr_conversion_research/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"research/bsbr_conversion_research/#inference-speed-comparison","title":"Inference Speed Comparison","text":"<p>We compared inference speeds across different sequence lengths.</p>"},{"location":"research/bsbr_conversion_research/#methodology_3","title":"Methodology","text":"<ul> <li>Measure average inference time over 50 runs</li> <li>Test sequence lengths from 128 to 8192</li> <li>Compare on both CPU and GPU (when available)</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_1","title":"Results","text":"Sequence Length Standard (ms) BSBR (ms) Speedup 128 12.4 18.7 0.66x 512 51.2 62.6 0.82x 1024 102.7 98.3 1.04x 2048 210.3 174.2 1.21x 4096 463.8 316.1 1.47x 8192 OOM 643.5 \u221e <p>These results confirm our hypothesis: standard transformers are faster for short sequences, but BSBR becomes more efficient as sequence length increases. The crossover point occurs around 1024 tokens.</p> <p>Note: \"OOM\" indicates \"Out of Memory\" error on the test hardware.</p>"},{"location":"research/bsbr_conversion_research/#memory-usage-analysis","title":"Memory Usage Analysis","text":"<p>We measured peak memory consumption during inference.</p>"},{"location":"research/bsbr_conversion_research/#methodology_4","title":"Methodology","text":"<ul> <li>Track peak memory allocation using PyTorch utilities</li> <li>Test with batch size of 1 and varying sequence lengths</li> <li>Report GPU memory for CUDA-enabled tests</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_2","title":"Results","text":"Sequence Length Standard (MB) BSBR (MB) Ratio 128 524 603 1.15x 512 718 782 1.09x 1024 1150 1103 0.96x 2048 2352 1822 0.77x 4096 OOM 3185 N/A 8192 OOM 6148 N/A <p>The memory usage pattern mirrors the speed results: BSBR uses more memory for short sequences but becomes more memory-efficient for longer contexts. The memory efficiency advantages become significant at sequence lengths above 1024.</p>"},{"location":"research/bsbr_conversion_research/#scaling-properties","title":"Scaling Properties","text":""},{"location":"research/bsbr_conversion_research/#computational-complexity-analysis","title":"Computational Complexity Analysis","text":"<p>We analyzed how computation time scales with sequence length for both architectures.</p>"},{"location":"research/bsbr_conversion_research/#methodology_5","title":"Methodology","text":"<ul> <li>Measure inference time for different sequence lengths</li> <li>Fit asymptotic complexity curves</li> <li>Analyze deviation from theoretical complexity</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_3","title":"Results","text":"<p>The empirical scaling curves confirm that BSBR achieves near-linear scaling with sequence length:</p> <ul> <li>Standard transformer: O(n^1.96) - Very close to the theoretical O(n\u00b2)</li> <li>BSBR: O(n^1.12) - Approaching the theoretical O(n)</li> </ul> <p>The deviation from ideal scaling is likely due to implementation details and overhead that becomes less significant at extreme sequence lengths.</p>"},{"location":"research/bsbr_conversion_research/#attention-sparsity-analysis","title":"Attention Sparsity Analysis","text":"<p>We analyzed the effective sparsity of attention matrices in both models.</p>"},{"location":"research/bsbr_conversion_research/#methodology_6","title":"Methodology","text":"<ul> <li>Compute the percentage of attention weights above a threshold</li> <li>Compare across different layers and sequence lengths</li> <li>Measure effective information density</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_4","title":"Results","text":"Seq Length Std Density BSBR Density Reduction 128 100% 76.3% 23.7% 512 100% 41.6% 58.4% 1024 100% 24.8% 75.2% 2048 100% 14.2% 85.8% 4096 N/A 8.1% N/A <p>BSBR achieves significant sparsity in attention, with the sparsity advantage growing with sequence length. This explains the computational and memory efficiency gains observed in longer contexts.</p>"},{"location":"research/bsbr_conversion_research/#real-world-application-benchmarks","title":"Real-World Application Benchmarks","text":""},{"location":"research/bsbr_conversion_research/#text-summarization","title":"Text Summarization","text":"<p>We evaluated both models on a text summarization task with long articles.</p>"},{"location":"research/bsbr_conversion_research/#methodology_7","title":"Methodology","text":"<ul> <li>Use CNN/Daily Mail dataset articles (average length ~800 tokens)</li> <li>Generate summaries with both models</li> <li>Evaluate using ROUGE scores and human judgments</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_5","title":"Results","text":"Model ROUGE-1 ROUGE-2 ROUGE-L Human Preference GPT-2 0.41 0.19 0.38 38% BSBR-GPT-2 0.39 0.18 0.37 35% No Preference - - - 27% <p>The BSBR model maintains comparable performance on summarization tasks, with only a slight decrease in metrics and human preference.</p>"},{"location":"research/bsbr_conversion_research/#long-context-qa","title":"Long-Context QA","text":"<p>We tested the models on question-answering tasks that require processing long contexts.</p>"},{"location":"research/bsbr_conversion_research/#methodology_8","title":"Methodology","text":"<ul> <li>Use custom dataset with questions requiring context from 2000+ tokens away</li> <li>Compare answer accuracy between models</li> <li>Measure inference time for complete processing</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_6","title":"Results","text":"Model Accuracy Avg. Inference Time (s) GPT-2 58.3% 4.7 BSBR-GPT-2 56.9% 3.2 <p>The BSBR model achieves comparable accuracy with a 32% reduction in inference time for this long-context task.</p>"},{"location":"research/bsbr_conversion_research/#effect-of-hyperparameters","title":"Effect of Hyperparameters","text":""},{"location":"research/bsbr_conversion_research/#chunk-size-impact","title":"Chunk Size Impact","text":"<p>We investigated how chunk size affects model performance and efficiency.</p>"},{"location":"research/bsbr_conversion_research/#methodology_9","title":"Methodology","text":"<ul> <li>Test BSBR models with chunk sizes: 64, 128, 256, 512</li> <li>Measure inference speed and memory usage</li> <li>Evaluate output quality metrics</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_7","title":"Results","text":"Chunk Size Speed (rel.) Memory (rel.) Output Similarity 64 1.00x 1.00x 0.87 128 0.92x 1.05x 0.83 256 0.85x 1.12x 0.79 512 0.78x 1.23x 0.72 <p>Smaller chunk sizes maintain closer similarity to the original model but sacrifice some of the speed benefits. Larger chunks improve computational efficiency but diverge more from the original model's behavior.</p>"},{"location":"research/bsbr_conversion_research/#compression-factor-analysis","title":"Compression Factor Analysis","text":"<p>We explored how state vector compression affects model performance.</p>"},{"location":"research/bsbr_conversion_research/#methodology_10","title":"Methodology","text":"<ul> <li>Test compression factors: None, 2, 4, 8</li> <li>Measure impact on memory usage and inference speed</li> <li>Evaluate accuracy on benchmark tasks</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_8","title":"Results","text":"Compression Memory Saved Speed Impact Accuracy Drop None 0% 0% 0% 2x 22.3% +1.2% 0.4% 4x 36.1% +2.8% 1.7% 8x 42.5% +3.5% 3.8% <p>A compression factor of 2-4 offers a good tradeoff, providing substantial memory savings with minimal impact on model performance.</p>"},{"location":"research/bsbr_conversion_research/#fine-tuning-analysis","title":"Fine-Tuning Analysis","text":""},{"location":"research/bsbr_conversion_research/#recovery-of-conversion-loss","title":"Recovery of Conversion Loss","text":"<p>We investigated whether fine-tuning can recover any performance loss after conversion.</p>"},{"location":"research/bsbr_conversion_research/#methodology_11","title":"Methodology","text":"<ul> <li>Fine-tune converted model for 1, 5, and 10 epochs</li> <li>Evaluate on benchmark tasks after each phase</li> <li>Compare with original model performance</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_9","title":"Results","text":"Model ROUGE-L QA Accuracy Human Preference Original GPT-2 0.38 58.3% 38% BSBR (no tuning) 0.37 56.9% 35% BSBR (1 epoch) 0.37 57.4% 36% BSBR (5 epochs) 0.38 58.1% 37% BSBR (10 epochs) 0.38 58.4% 39% <p>Even a modest amount of fine-tuning helps recover most of the performance gap, and extended fine-tuning can lead to performance that matches or exceeds the original model.</p>"},{"location":"research/bsbr_conversion_research/#conclusions","title":"Conclusions","text":"<p>Our research on converting standard transformers to BSBR yields several important findings:</p> <ol> <li> <p>Behavior preservation is significant but not perfect. The converted models maintain 70-85% similarity in outputs and predictions.</p> </li> <li> <p>Performance crossover occurs around the 1024-token mark, where BSBR begins to outperform standard transformers in both speed and memory usage.</p> </li> <li> <p>Asymptotic efficiency is substantially better for BSBR, with near-linear scaling observed empirically.</p> </li> <li> <p>Practical viability is confirmed for real-world tasks, with only modest performance degradation that can be recovered through fine-tuning.</p> </li> <li> <p>Hyperparameter tuning allows balancing between computational efficiency and output fidelity.</p> </li> </ol> <p>These findings demonstrate that converting pre-trained transformers to BSBR is a viable approach for extending the capabilities of existing models to handle longer contexts more efficiently.</p>"},{"location":"research/bsbr_conversion_research/#future-research-directions","title":"Future Research Directions","text":"<p>Based on our findings, we identify several promising directions for future research:</p> <ol> <li>Architecture-specific optimizations to further improve converted model performance</li> <li>Hybrid attention mechanisms that dynamically switch between standard and BSBR attention</li> <li>Layer-wise conversion strategies that apply BSBR selectively to specific layers</li> <li>Specialized fine-tuning techniques for converted models</li> <li>Hardware-specific optimizations to better leverage modern accelerators </li> </ol>"},{"location":"research/experiments/","title":"Research Experiments","text":"<p>This document details the experiments conducted to evaluate BSBR's performance and capabilities.</p>"},{"location":"research/experiments/#experimental-setup","title":"Experimental Setup","text":""},{"location":"research/experiments/#hardware-configuration","title":"Hardware Configuration","text":"<ul> <li>GPU: NVIDIA A100 (40GB)</li> <li>CPU: 32 cores</li> <li>Memory: 256GB RAM</li> <li>Storage: 2TB NVMe SSD</li> </ul>"},{"location":"research/experiments/#software-stack","title":"Software Stack","text":"<ul> <li>PyTorch 2.6.0</li> <li>CUDA 11.8</li> <li>Python 3.12</li> <li>BSBR 0.1.1</li> </ul>"},{"location":"research/experiments/#baseline-models","title":"Baseline Models","text":"<ol> <li>Standard Transformer</li> <li>Full attention mechanism</li> <li>O(n\u00b2) complexity</li> <li> <p>Reference implementation</p> </li> <li> <p>Linear Transformer</p> </li> <li>Linear complexity attention</li> <li>No softmax</li> <li> <p>Efficient implementation</p> </li> <li> <p>Sliding Window Transformer</p> </li> <li>Fixed context window</li> <li>O(n\u00b7w) complexity</li> <li> <p>Local attention</p> </li> <li> <p>DeltaNet</p> </li> <li>Enhanced linear transformer</li> <li>Removal component</li> <li>Memory efficient</li> </ol>"},{"location":"research/experiments/#performance-experiments","title":"Performance Experiments","text":""},{"location":"research/experiments/#scaling-analysis","title":"Scaling Analysis","text":""},{"location":"research/experiments/#sequence-length-scaling","title":"Sequence Length Scaling","text":"<pre><code>seq_lengths = [64, 128, 256, 512, 1024, 2048]\nmetrics = ['inference_time', 'memory_usage', 'flops']\n\nresults = scaling_analysis(\n    seq_lengths=seq_lengths,\n    models=['BSBR', 'Linear', 'Standard'],\n    metrics=metrics\n)\n</code></pre> <p>Results: - BSBR: O(n) scaling - Linear: O(n) scaling - Standard: O(n\u00b2) scaling</p>"},{"location":"research/experiments/#memory-usage","title":"Memory Usage","text":"<pre><code>memory_results = {\n    'BSBR': {\n        'peak_memory': [0.5, 0.8, 1.2, 1.8, 2.5, 3.3],  # GB\n        'gradient_memory': [0.3, 0.5, 0.7, 1.0, 1.4, 1.8]  # GB\n    },\n    'Standard': {\n        'peak_memory': [1.0, 2.0, 4.0, 8.0, 16.0, 32.0],  # GB\n        'gradient_memory': [0.8, 1.6, 3.2, 6.4, 12.8, 25.6]  # GB\n    }\n}\n</code></pre>"},{"location":"research/experiments/#training-experiments","title":"Training Experiments","text":""},{"location":"research/experiments/#convergence-analysis","title":"Convergence Analysis","text":"<pre><code>training_configs = {\n    'BSBR': {\n        'learning_rate': 1e-4,\n        'batch_size': 32,\n        'chunk_size': 128\n    },\n    'Standard': {\n        'learning_rate': 1e-4,\n        'batch_size': 32\n    }\n}\n\nconvergence_results = {\n    'BSBR': {\n        'epochs_to_converge': 50,\n        'final_loss': 0.15,\n        'validation_accuracy': 0.92\n    },\n    'Standard': {\n        'epochs_to_converge': 45,\n        'final_loss': 0.18,\n        'validation_accuracy': 0.89\n    }\n}\n</code></pre>"},{"location":"research/experiments/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code>memory_efficiency = {\n    'BSBR': {\n        'peak_memory': 2.5,  # GB\n        'gradient_memory': 1.4,  # GB\n        'activation_memory': 0.8  # GB\n    },\n    'Standard': {\n        'peak_memory': 16.0,  # GB\n        'gradient_memory': 12.8,  # GB\n        'activation_memory': 8.0  # GB\n    }\n}\n</code></pre>"},{"location":"research/experiments/#model-quality-experiments","title":"Model Quality Experiments","text":""},{"location":"research/experiments/#long-range-dependencies","title":"Long-Range Dependencies","text":""},{"location":"research/experiments/#task-document-classification","title":"Task: Document Classification","text":"<pre><code>dataset_configs = {\n    'short': {'max_length': 512},\n    'medium': {'max_length': 1024},\n    'long': {'max_length': 2048}\n}\n\nresults = {\n    'BSBR': {\n        'short': 0.94,\n        'medium': 0.92,\n        'long': 0.89\n    },\n    'Standard': {\n        'short': 0.95,\n        'medium': 0.88,\n        'long': 0.75\n    }\n}\n</code></pre>"},{"location":"research/experiments/#attention-analysis","title":"Attention Analysis","text":""},{"location":"research/experiments/#sparsity-patterns","title":"Sparsity Patterns","text":"<pre><code>sparsity_results = {\n    'BSBR': {\n        'within_chunk': 0.0,  # Full attention\n        'between_chunk': 0.85,  # Sparse attention\n        'overall': 0.65\n    },\n    'Standard': {\n        'within_chunk': 0.0,\n        'between_chunk': 0.0,\n        'overall': 0.0\n    }\n}\n</code></pre>"},{"location":"research/experiments/#attention-visualization","title":"Attention Visualization","text":"<pre><code>def visualize_attention_patterns():\n    \"\"\"Generate attention heatmaps.\"\"\"\n    models = ['BSBR', 'Standard']\n    seq_lengths = [256, 512, 1024]\n\n    for model in models:\n        for length in seq_lengths:\n            attention_weights = get_attention_weights(model, length)\n            plot_attention_heatmap(attention_weights, f\"{model}_{length}\")\n</code></pre>"},{"location":"research/experiments/#ablation-studies","title":"Ablation Studies","text":""},{"location":"research/experiments/#chunk-size-impact","title":"Chunk Size Impact","text":"<pre><code>chunk_sizes = [32, 64, 128, 256]\nresults = {\n    'inference_time': {\n        32: 0.1,  # seconds\n        64: 0.15,\n        128: 0.2,\n        256: 0.3\n    },\n    'memory_usage': {\n        32: 0.8,  # GB\n        64: 1.2,\n        128: 1.8,\n        256: 2.5\n    },\n    'accuracy': {\n        32: 0.85,\n        64: 0.88,\n        128: 0.92,\n        256: 0.93\n    }\n}\n</code></pre>"},{"location":"research/experiments/#compression-factor-analysis","title":"Compression Factor Analysis","text":"<pre><code>compression_factors = [1, 2, 4, 8]\nresults = {\n    'memory_reduction': {\n        1: 1.0,  # baseline\n        2: 0.6,\n        4: 0.4,\n        8: 0.3\n    },\n    'accuracy_impact': {\n        1: 0.92,  # baseline\n        2: 0.91,\n        4: 0.89,\n        8: 0.85\n    }\n}\n</code></pre>"},{"location":"research/experiments/#real-world-applications","title":"Real-World Applications","text":""},{"location":"research/experiments/#document-processing","title":"Document Processing","text":"<pre><code>document_results = {\n    'BSBR': {\n        'processing_time': 0.5,  # seconds per page\n        'memory_usage': 2.5,  # GB\n        'accuracy': 0.92\n    },\n    'Standard': {\n        'processing_time': 2.0,\n        'memory_usage': 16.0,\n        'accuracy': 0.89\n    }\n}\n</code></pre>"},{"location":"research/experiments/#multi-modal-tasks","title":"Multi-Modal Tasks","text":"<pre><code>multimodal_results = {\n    'BSBR': {\n        'text_accuracy': 0.92,\n        'image_accuracy': 0.88,\n        'joint_accuracy': 0.85\n    },\n    'Standard': {\n        'text_accuracy': 0.89,\n        'image_accuracy': 0.85,\n        'joint_accuracy': 0.82\n    }\n}\n</code></pre>"},{"location":"research/experiments/#conclusions","title":"Conclusions","text":"<ol> <li>Performance</li> <li>Linear scaling with sequence length</li> <li>Significantly lower memory usage</li> <li> <p>Competitive inference time</p> </li> <li> <p>Model Quality</p> </li> <li>Better long-range modeling</li> <li>Comparable accuracy to standard attention</li> <li> <p>More stable training</p> </li> <li> <p>Practical Benefits</p> </li> <li>Efficient document processing</li> <li>Better memory management</li> <li>Flexible architecture </li> </ol>"},{"location":"user-guide/core-concepts/","title":"Core Concepts","text":"<p>This guide explains the fundamental concepts behind BSBR (Block Sparse Attention with Block Retrieval). Understanding these concepts will help you make better use of the library and customize it for your needs.</p>"},{"location":"user-guide/core-concepts/#attention-mechanisms","title":"Attention Mechanisms","text":""},{"location":"user-guide/core-concepts/#standard-transformer-attention","title":"Standard Transformer Attention","text":"<p>The standard transformer attention mechanism computes attention scores between all pairs of tokens in a sequence:</p> <pre><code>Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n</code></pre> <p>This leads to O(n\u00b2) complexity in both computation and memory, where n is the sequence length.</p>"},{"location":"user-guide/core-concepts/#bsbrs-approach","title":"BSBR's Approach","text":"<p>BSBR addresses this scalability issue by combining two types of attention:</p> <ol> <li>Within-chunk attention: Standard attention within fixed-size chunks</li> <li>Between-chunk attention: Efficient block retrieval between chunks</li> </ol> <pre><code>O = Q \u2299 softmax(RH^T \u00b7 M_out)F.repeat(B) + softmax(QK^T \u00b7 M_in)V\n</code></pre> <p>Where: - Q, K, V: Query, Key, Value matrices - R, H: Meta queries and keys for chunk-level attention - F: State vectors (flattened K^T\u00b7V for each chunk) - M_in: Block diagonal mask - M_out: Causal mask for chunk-level attention</p>"},{"location":"user-guide/core-concepts/#chunking-strategy","title":"Chunking Strategy","text":""},{"location":"user-guide/core-concepts/#chunk-size-selection","title":"Chunk Size Selection","text":"<p>The chunk size (B) is a crucial hyperparameter that affects:</p> <ol> <li>Memory Usage: Larger chunks use more memory but provide better local context</li> <li>Computation Time: Smaller chunks are faster but may miss important long-range dependencies</li> <li>Model Expressivity: Chunk size affects how well the model can capture different types of relationships</li> </ol>"},{"location":"user-guide/core-concepts/#chunk-overlap","title":"Chunk Overlap","text":"<p>BSBR supports overlapping chunks to improve information flow between adjacent chunks:</p> <pre><code>model = BSBRModel(\n    chunk_size=128,\n    chunk_overlap=32,  # 25% overlap between chunks\n    ...\n)\n</code></pre>"},{"location":"user-guide/core-concepts/#block-retrieval","title":"Block Retrieval","text":""},{"location":"user-guide/core-concepts/#meta-attention","title":"Meta Attention","text":"<p>Between chunks, BSBR uses a meta-attention mechanism to efficiently retrieve information:</p> <ol> <li>State Compression: Each chunk's information is compressed into a state vector</li> <li>Meta Queries: Special queries that operate at the chunk level</li> <li>Efficient Retrieval: O(n/B) complexity for chunk-level attention</li> </ol>"},{"location":"user-guide/core-concepts/#compression-factor","title":"Compression Factor","text":"<p>The compression factor \u00a9 controls how much information is preserved in chunk states:</p> <pre><code>model = BSBRModel(\n    compression_factor=4,  # Compress chunk states by 4x\n    ...\n)\n</code></pre> <p>Higher compression factors: - Reduce memory usage - Speed up computation - May lose some fine-grained information</p>"},{"location":"user-guide/core-concepts/#memory-management","title":"Memory Management","text":""},{"location":"user-guide/core-concepts/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>BSBR supports gradient checkpointing to trade computation for memory:</p> <pre><code>model = BSBRModel(\n    use_checkpointing=True,  # Enable gradient checkpointing\n    ...\n)\n</code></pre>"},{"location":"user-guide/core-concepts/#memory-efficient-attention","title":"Memory-Efficient Attention","text":"<p>The implementation includes several memory optimizations:</p> <ol> <li>Sparse Attention: Only compute attention for relevant token pairs</li> <li>State Reuse: Reuse chunk states across layers</li> <li>Efficient Masking: Optimized attention masks for causal language modeling</li> </ol>"},{"location":"user-guide/core-concepts/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"user-guide/core-concepts/#computational-complexity","title":"Computational Complexity","text":"<p>BSBR achieves near-linear complexity in sequence length:</p> <ul> <li>Within-chunk: O(n\u00b7B) where B is chunk size</li> <li>Between-chunk: O(n + n\u00b2/B)</li> <li>Overall: O(n) for fixed chunk size</li> </ul>"},{"location":"user-guide/core-concepts/#memory-usage","title":"Memory Usage","text":"<p>Memory consumption scales linearly with sequence length:</p> <ul> <li>Within-chunk: O(n\u00b7B)</li> <li>Between-chunk: O(n/B)</li> <li>State vectors: O(n/c) where c is compression factor</li> </ul>"},{"location":"user-guide/core-concepts/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/core-concepts/#model-configuration","title":"Model Configuration","text":"<p>Recommended configurations for different use cases:</p> <ol> <li> <p>Short Sequences (n &lt; 512):    <pre><code>model = BSBRModel(\n    chunk_size=64,\n    compression_factor=2,\n    ...\n)\n</code></pre></p> </li> <li> <p>Medium Sequences (512 \u2264 n &lt; 2048):    <pre><code>model = BSBRModel(\n    chunk_size=128,\n    compression_factor=4,\n    ...\n)\n</code></pre></p> </li> <li> <p>Long Sequences (n \u2265 2048):    <pre><code>model = BSBRModel(\n    chunk_size=256,\n    compression_factor=8,\n    use_checkpointing=True,\n    ...\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/core-concepts/#training-tips","title":"Training Tips","text":"<ol> <li>Learning Rate: Use slightly higher learning rates than standard transformers</li> <li>Warmup: Longer warmup periods may be needed for very long sequences</li> <li>Gradient Clipping: Monitor gradients and clip if necessary</li> <li>Batch Size: Adjust based on available memory and sequence length</li> </ol>"},{"location":"user-guide/core-concepts/#next-steps","title":"Next Steps","text":"<ol> <li>Check the API Reference for detailed documentation</li> <li>Explore Examples for usage examples</li> <li>See Research for benchmark results </li> </ol>"}]}