{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"BSBR: Block Sparse Attention with Block Retrieval","text":"<p>BSBR (Block Sparse Attention with Block Retrieval) is a novel attention mechanism for efficient processing of long sequences in transformer architectures. It combines standard attention within chunks and block retrieval between chunks to achieve near-linear complexity while maintaining high model expressivity.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\udd04 Efficient Processing: Near-linear complexity in sequence length</li> <li>\ud83e\udde9 Chunk-Based Attention: Standard attention within chunks</li> <li>\ud83d\udd0d Block Retrieval: Efficient information retrieval between chunks</li> <li>\ud83c\udfaf Configurable: Adjustable chunk size and compression</li> <li>\ud83d\udcbe Memory Efficient: Optimized memory usage for long sequences</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\n\n# Model configuration\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    compression_factor=4  # Optional compression\n)\n\n# Input data\ninput_ids = torch.randint(0, 10000, (2, 256))\nattention_mask = torch.ones(2, 256)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code># Install the core package\npip install bsbr\n\n# Install with extras for evaluations and research\npip install \"bsbr[extras]\"\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>User Guide</li> <li>API Reference</li> <li>Examples</li> <li>Research</li> </ul>"},{"location":"#research","title":"Research","text":"<p>BSBR is based on research presented in our paper BSBR: Block Sparse Attention with Block Retrieval for Efficient Long-Context Reasoning. The implementation is inspired by Shengding Hu's blog post Streaming models for efficient long-context reasoning.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guidelines for details.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details. </p>"},{"location":"converter_findings/","title":"Converting Standard Transformers to BSBR: Findings","text":""},{"location":"converter_findings/#overview","title":"Overview","text":"<p>We developed a utility to convert pre-trained GPT-style transformers to use Block Sparse Attention with Block Retrieval (BSBR). This document summarizes our findings on the feasibility, benefits, and trade-offs of such conversions.</p>"},{"location":"converter_findings/#technical-implementation","title":"Technical Implementation","text":"<p>We successfully implemented a converter that:</p> <ol> <li>Extracts weights from standard GPT-2 models</li> <li>Creates an equivalent BSBR model with appropriate dimensions</li> <li>Transfers and transforms weights to fit the BSBR architecture</li> <li>Initializes block-specific components like meta-queries and meta-keys</li> </ol> <p>The conversion process preserves the original model's knowledge while introducing the more efficient BSBR attention mechanism.</p>"},{"location":"converter_findings/#key-findings","title":"Key Findings","text":""},{"location":"converter_findings/#mathematical-equivalence","title":"Mathematical Equivalence","text":"<ul> <li>Within-chunk processing is mathematically similar between standard and BSBR transformers, using the same causal attention pattern.</li> <li>Between-chunk processing in BSBR uses a fundamentally different approach with meta-queries, meta-keys, and chunk states.</li> <li>The conversion is not an exact equivalence transformation but preserves much of the trained knowledge.</li> </ul>"},{"location":"converter_findings/#weight-transfer","title":"Weight Transfer","text":"<ul> <li>Query, Key, Value projections can be transferred directly, sometimes requiring transposition due to differences between Conv1D and nn.Linear implementations.</li> <li>Meta projections for block retrieval need to be initialized as combinations of existing projections since they have no direct equivalent in standard transformers.</li> <li>Feed-forward networks can be transferred directly with minimal adjustments for shape differences.</li> <li>Layer normalization parameters transfer directly without modification.</li> </ul>"},{"location":"converter_findings/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Our benchmark tests show that for short sequences, the standard transformer can be faster due to its optimized implementation.</li> <li>For longer sequences, BSBR becomes more efficient as the benefits of chunked processing become apparent.</li> <li>The current implementation shows a slight slowdown for typical sequence lengths, but this is expected to reverse for very long sequences.</li> <li>BSBR models use approximately 40% more parameters due to the additional meta projections for chunk retrieval.</li> </ul>"},{"location":"converter_findings/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>BSBR's theoretically lower asymptotic memory complexity is not always realized in practice for shorter sequences.</li> <li>The main benefits for memory usage are expected to become apparent with extremely long sequences (10k+ tokens).</li> <li>The additional parameters used by BSBR meta projections slightly increase the model size.</li> </ul>"},{"location":"converter_findings/#practical-considerations","title":"Practical Considerations","text":""},{"location":"converter_findings/#initialization-strategies","title":"Initialization Strategies","text":"<ul> <li>For meta-queries and meta-keys, we found that averaging the weights of existing projections provides a reasonable starting point.</li> <li>Fine-tuning might be necessary to optimize the newly initialized components.</li> </ul>"},{"location":"converter_findings/#compatibility","title":"Compatibility","text":"<ul> <li>BSBR models can be used as drop-in replacements for standard transformers with minimal adaptation.</li> <li>The standard interface for feeding inputs and extracting outputs is preserved.</li> </ul>"},{"location":"converter_findings/#performance-tuning","title":"Performance Tuning","text":"<ul> <li>Chunk size is a critical hyperparameter that significantly affects both efficiency and effectiveness.</li> <li>Compression factor for state vectors can reduce memory requirements with minimal impact on performance.</li> </ul>"},{"location":"converter_findings/#recommendations","title":"Recommendations","text":"<ol> <li> <p>Use Case Assessment: BSBR conversion is most beneficial for applications requiring processing of very long sequences.</p> </li> <li> <p>Fine-tuning: After conversion, a brief period of fine-tuning can help adapt the model to the new architecture.</p> </li> <li> <p>Chunk Size Selection: </p> </li> <li>Smaller chunks (64-128) work well for shorter contexts</li> <li> <p>Larger chunks (256-512) may be better for very long contexts</p> </li> <li> <p>Compression Tradeoff: State vector compression offers memory savings at a small cost to performance.</p> </li> </ol>"},{"location":"converter_findings/#conclusion","title":"Conclusion","text":"<p>Converting standard transformers to BSBR is technically feasible and offers potential efficiency benefits for long-context processing. The current implementation demonstrates that pre-trained knowledge can be preserved during conversion, making it possible to leverage existing models in a more efficient architecture.</p> <p>The primary advantage of BSBR\u2014its ability to handle extremely long contexts efficiently\u2014makes it particularly valuable for applications like document processing, long-term memory, and persistent context tracking.</p>"},{"location":"converter_findings/#future-work","title":"Future Work","text":"<ol> <li> <p>Broader Model Support: Extend conversion support to other transformer architectures beyond GPT-2.</p> </li> <li> <p>Memory Optimization: Further reduce memory requirements for state vectors through more advanced compression techniques.</p> </li> <li> <p>Fine-tuning Studies: Research optimal fine-tuning strategies after conversion to recover any performance degradation.</p> </li> <li> <p>Hardware Acceleration: Develop specialized kernels to better leverage the block structure for even greater efficiency. </p> </li> </ol>"},{"location":"api/bsbr/","title":"BSBR Core API","text":""},{"location":"api/bsbr/#bsbr.bsbr","title":"<code>bsbr.bsbr</code>","text":""},{"location":"api/bsbr/#bsbr.bsbr.BSBRModel","title":"<code>BSBRModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Full BSBR model stacking multiple BSBR layers.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Vocabulary size for embedding layer</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_layers</code> <code>int</code> <p>Number of BSBR layers</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>chunk_size</code> <code>int</code> <p>Size of each chunk</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> <code>compression_factor</code> <code>Optional[int]</code> <p>If provided, compresses the state dimension</p> <code>None</code> Source code in <code>src/bsbr/bsbr.py</code> <pre><code>class BSBRModel(nn.Module):\n    \"\"\"\n    Full BSBR model stacking multiple BSBR layers.\n\n    Args:\n        vocab_size (int): Vocabulary size for embedding layer\n        hidden_dim (int): Hidden dimension size\n        num_layers (int): Number of BSBR layers\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of each chunk\n        ff_dim (int): Feed-forward intermediate dimension\n        dropout (float): Dropout probability\n        compression_factor (Optional[int]): If provided, compresses the state dimension\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_dim: int,\n        num_layers: int,\n        num_heads: int,\n        chunk_size: int,\n        ff_dim: int,\n        dropout: float = 0.1,\n        compression_factor: Optional[int] = None\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_encoding = PositionalEncoding(hidden_dim, dropout)\n\n        self.layers = nn.ModuleList([\n            BSBRLayer(\n                hidden_dim=hidden_dim,\n                num_heads=num_heads,\n                chunk_size=chunk_size,\n                ff_dim=ff_dim,\n                dropout=dropout,\n                compression_factor=compression_factor\n            )\n            for _ in range(num_layers)\n        ])\n\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for the full BSBR model.\n\n        Args:\n            input_ids: Token IDs of shape [batch_size, seq_len]\n            attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n        Returns:\n            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n        \"\"\"\n        hidden_states = self.embedding(input_ids)\n        hidden_states = self.pos_encoding(hidden_states)\n\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n\n        hidden_states = self.layer_norm(hidden_states)\n        return hidden_states\n</code></pre>"},{"location":"api/bsbr/#bsbr.bsbr.BSBRModel.forward","title":"<code>forward(input_ids, attention_mask=None)</code>","text":"<p>Forward pass for the full BSBR model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>LongTensor</code> <p>Token IDs of shape [batch_size, seq_len]</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask of shape [batch_size, seq_len]</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> Source code in <code>src/bsbr/bsbr.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for the full BSBR model.\n\n    Args:\n        input_ids: Token IDs of shape [batch_size, seq_len]\n        attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n    Returns:\n        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n    \"\"\"\n    hidden_states = self.embedding(input_ids)\n    hidden_states = self.pos_encoding(hidden_states)\n\n    for layer in self.layers:\n        hidden_states = layer(hidden_states, attention_mask)\n\n    hidden_states = self.layer_norm(hidden_states)\n    return hidden_states\n</code></pre>"},{"location":"api/bsbr/#bsbr.bsbr.BSBRAttention","title":"<code>BSBRAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Block Sparse Attention with Block Retrieval (BSBR) implementation.</p> <p>This attention mechanism splits the input sequence into chunks and processes them in two ways: 1. Within each chunk: Standard attention with softmax 2. Between chunks: Block retrieval using meta queries and keys</p> <p>Parameters:</p> Name Type Description Default <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>chunk_size</code> <code>int</code> <p>Size of each chunk (B)</p> required <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> <code>compression_factor</code> <code>Optional[int]</code> <p>If provided, compresses the state dimension from d_0^2 to d_1^2</p> <code>None</code> Source code in <code>src/bsbr/bsbr.py</code> <pre><code>class BSBRAttention(nn.Module):\n    \"\"\"\n    Block Sparse Attention with Block Retrieval (BSBR) implementation.\n\n    This attention mechanism splits the input sequence into chunks and processes them in two ways:\n    1. Within each chunk: Standard attention with softmax\n    2. Between chunks: Block retrieval using meta queries and keys\n\n    Args:\n        hidden_dim (int): Hidden dimension size\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of each chunk (B)\n        dropout (float): Dropout probability\n        compression_factor (Optional[int]): If provided, compresses the state dimension from d_0^2 to d_1^2\n    \"\"\"\n    def __init__(\n        self,\n        hidden_dim: int,\n        num_heads: int,\n        chunk_size: int,\n        dropout: float = 0.1,\n        compression_factor: Optional[int] = None\n    ):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.num_heads = num_heads\n        self.head_dim = hidden_dim // num_heads\n        self.chunk_size = chunk_size\n        self.dropout = dropout\n\n        # Standard attention projections\n        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n\n        # Meta query and key projections for chunk-level attention\n        self.meta_r_proj = nn.Linear(hidden_dim, hidden_dim)  # R (retriever)\n        self.meta_h_proj = nn.Linear(hidden_dim, hidden_dim)  # H (hash)\n\n        # Optional compression for state vectors\n        self.compression_factor = compression_factor\n        if compression_factor is not None:\n            compressed_dim = hidden_dim // compression_factor\n            # Fix: The compress_proj should map from head_dim*head_dim to compressed_dim\n            state_dim = self.head_dim * self.head_dim\n            self.compress_proj = nn.Linear(state_dim, compressed_dim)\n            self.decompress_proj = nn.Linear(compressed_dim, state_dim)\n\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def _reshape_for_heads(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Reshape input for multi-head attention.\"\"\"\n        batch_size, seq_len, _ = x.size()\n        return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n    def _create_masks(self, seq_len: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Create M_in (block diagonal) and M_out (causal) masks.\"\"\"\n        # Calculate number of chunks\n        num_chunks = math.ceil(seq_len / self.chunk_size)\n\n        # Create block diagonal mask for within-chunk attention\n        m_in = torch.zeros(seq_len, seq_len, device=self.q_proj.weight.device)\n\n        for i in range(num_chunks):\n            start_idx = i * self.chunk_size\n            end_idx = min((i + 1) * self.chunk_size, seq_len)\n            # Create causal mask within each chunk\n            chunk_size = end_idx - start_idx\n            chunk_mask = torch.triu(torch.ones(chunk_size, chunk_size, device=m_in.device), diagonal=0)\n            m_in[start_idx:end_idx, start_idx:end_idx] = chunk_mask\n\n        # Create causal mask for between-chunk attention\n        chunk_indices = torch.arange(num_chunks, device=m_in.device)\n        m_out = torch.triu(torch.ones(num_chunks, num_chunks, device=m_in.device), diagonal=0)\n\n        return m_in, m_out\n\n    def compute_chunk_states(\n        self, \n        keys: torch.Tensor, \n        values: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute F states by flattening K^T\u00b7V for each chunk.\n\n        Args:\n            keys: [batch, num_heads, num_chunks, chunk_size, head_dim]\n            values: [batch, num_heads, num_chunks, chunk_size, head_dim]\n\n        Returns:\n            states: [batch, num_heads, num_chunks, state_dim]\n        \"\"\"\n        batch_size, num_heads, num_chunks, chunk_size, head_dim = keys.size()\n\n        # For each chunk, compute K^T\u00b7V\n        # Reshape to [batch*num_heads*num_chunks, chunk_size, head_dim]\n        keys_flat = keys.reshape(-1, chunk_size, head_dim)\n        values_flat = values.reshape(-1, chunk_size, head_dim)\n\n        # Compute K^T\u00b7V for each chunk\n        # [chunk_size, head_dim] @ [chunk_size, head_dim] -&gt; [head_dim, head_dim]\n        states = torch.bmm(\n            keys_flat.transpose(1, 2),  # [B*H*C, head_dim, chunk_size]\n            values_flat                 # [B*H*C, chunk_size, head_dim]\n        )\n\n        # Flatten the state matrices to vectors\n        states_flat = states.reshape(batch_size, num_heads, num_chunks, -1)\n\n        # Apply optional compression\n        if self.compression_factor is not None:\n            states_flat = self.compress_proj(states_flat)\n\n        return states_flat\n\n    def forward(\n        self, \n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the BSBR attention mechanism.\n\n        Args:\n            hidden_states: Input tensor of shape [batch_size, seq_len, hidden_dim]\n            attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n        Returns:\n            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n        \"\"\"\n        batch_size, seq_len, _ = hidden_states.size()\n        num_chunks = math.ceil(seq_len / self.chunk_size)\n\n        # Create masks for within-chunk and between-chunk attention\n        m_in, m_out = self._create_masks(seq_len)\n        if attention_mask is not None:\n            # Fix: Expand attention_mask to match m_in dimensions\n            attention_mask_expanded = attention_mask.unsqueeze(1).unsqueeze(2).expand(-1, -1, seq_len, -1)\n            m_in = m_in.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n            m_in = m_in * attention_mask_expanded\n\n        # Standard projections for Q, K, V\n        q = self._reshape_for_heads(self.q_proj(hidden_states))  # [batch, num_heads, seq_len, head_dim]\n        k = self._reshape_for_heads(self.k_proj(hidden_states))  # [batch, num_heads, seq_len, head_dim]\n        v = self._reshape_for_heads(self.v_proj(hidden_states))  # [batch, num_heads, seq_len, head_dim]\n\n        # Reshape to chunks\n        # Pad sequence length to be divisible by chunk_size\n        padding = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n        if padding &gt; 0:\n            q = F.pad(q, (0, 0, 0, padding))\n            k = F.pad(k, (0, 0, 0, padding))\n            v = F.pad(v, (0, 0, 0, padding))\n            padded_seq_len = seq_len + padding\n        else:\n            padded_seq_len = seq_len\n\n        # Reshape to chunks [batch, num_heads, num_chunks, chunk_size, head_dim]\n        q_chunks = q.view(batch_size, self.num_heads, num_chunks, self.chunk_size, self.head_dim)\n        k_chunks = k.view(batch_size, self.num_heads, num_chunks, self.chunk_size, self.head_dim)\n        v_chunks = v.view(batch_size, self.num_heads, num_chunks, self.chunk_size, self.head_dim)\n\n        # Meta projections for chunk-level attention\n        # Fix: First, get a representative hidden state for each chunk\n        # Create a padded version of hidden_states for safe reshaping\n        if padding &gt; 0:\n            padded_hidden = F.pad(hidden_states, (0, 0, 0, padding))\n        else:\n            padded_hidden = hidden_states\n\n        # Reshape to [batch, num_chunks, chunk_size, hidden_dim]\n        chunk_hidden = padded_hidden.view(batch_size, num_chunks, self.chunk_size, self.hidden_dim)\n\n        # Get the last token of each chunk as representative\n        # For the final chunk, make sure we don't index beyond valid tokens\n        chunk_repr = []\n        for i in range(num_chunks):\n            if i == num_chunks - 1 and padding &gt; 0:\n                # For the last chunk, get the last valid token\n                last_valid_idx = self.chunk_size - padding - 1\n                if last_valid_idx &lt; 0:  # In case the last chunk is all padding\n                    last_valid_idx = 0\n                repr_i = chunk_hidden[:, i, last_valid_idx, :]\n            else:\n                # For full chunks, get the last token\n                repr_i = chunk_hidden[:, i, -1, :]\n            chunk_repr.append(repr_i)\n\n        # Stack to [batch, num_chunks, hidden_dim]\n        chunk_repr = torch.stack(chunk_repr, dim=1)\n\n        # Project to meta queries and keys\n        r = self.meta_r_proj(chunk_repr)  # [batch, num_chunks, hidden_dim]\n        h = self.meta_h_proj(chunk_repr)  # [batch, num_chunks, hidden_dim]\n\n        # Reshape for multi-head attention\n        r = r.view(batch_size, num_chunks, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, num_chunks, head_dim]\n        h = h.view(batch_size, num_chunks, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, num_chunks, head_dim]\n\n        # Compute chunk states (F)\n        f = self.compute_chunk_states(k_chunks, v_chunks)  # [batch, num_heads, num_chunks, state_dim]\n\n        # Between-chunk attention: softmax(R\u00b7H^T)\u00b7F\n        # Calculate R\u00b7H^T\n        chunk_attn_scores = torch.matmul(r, h.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [batch, num_heads, num_chunks, num_chunks]\n\n        # Apply causal mask for between-chunk attention\n        m_out = m_out.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n        chunk_attn_scores = chunk_attn_scores.masked_fill(m_out == 0, -1e9)\n\n        # Apply softmax\n        chunk_attn_probs = F.softmax(chunk_attn_scores, dim=-1)  # [batch, num_heads, num_chunks, num_chunks]\n        chunk_attn_probs = self.dropout_layer(chunk_attn_probs)\n\n        # Calculate retrieved states\n        retrieved_states = torch.matmul(chunk_attn_probs, f)  # [batch, num_heads, num_chunks, state_dim]\n\n        # Decompress if needed\n        if self.compression_factor is not None:\n            retrieved_states = self.decompress_proj(retrieved_states)\n\n        # Expand retrieved states to match each position in the chunk\n        # First reshape retrieved_states to [batch, num_heads, num_chunks, head_dim, head_dim]\n        retrieved_states = retrieved_states.view(batch_size, self.num_heads, num_chunks, self.head_dim, self.head_dim)\n\n        # Multiply query with retrieved states: q_chunks @ retrieved_states\n        # For each position in each chunk, calculate q @ retrieved_state\n        # [batch, num_heads, num_chunks, chunk_size, head_dim] @ [batch, num_heads, num_chunks, head_dim, head_dim]\n        # -&gt; [batch, num_heads, num_chunks, chunk_size, head_dim]\n        long_term_output = torch.matmul(q_chunks, retrieved_states)\n\n        # Within-chunk attention\n        # Calculate local attention scores\n        local_attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [batch, num_heads, padded_seq_len, padded_seq_len]\n\n        # Fix: Apply the correct block diagonal causal mask\n        # First create a mask for the padded sequence\n        padded_m_in = torch.zeros(padded_seq_len, padded_seq_len, device=m_in.device)\n\n        for i in range(num_chunks):\n            start_idx = i * self.chunk_size\n            end_idx = (i + 1) * self.chunk_size\n            # Create causal mask within each chunk\n            chunk_mask = torch.triu(torch.ones(self.chunk_size, self.chunk_size, device=padded_m_in.device), diagonal=0)\n            padded_m_in[start_idx:end_idx, start_idx:end_idx] = chunk_mask\n\n        # Expand dimensions for broadcasting\n        expanded_mask = padded_m_in.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n\n        # Apply the mask\n        local_attn_scores = local_attn_scores.masked_fill(expanded_mask == 0, -1e9)\n\n        # Apply softmax\n        local_attn_probs = F.softmax(local_attn_scores, dim=-1)\n        local_attn_probs = self.dropout_layer(local_attn_probs)\n\n        # Apply attention\n        local_output = torch.matmul(local_attn_probs, v)  # [batch, num_heads, padded_seq_len, head_dim]\n\n        # Reshape long_term_output to match local_output\n        long_term_output = long_term_output.view(batch_size, self.num_heads, padded_seq_len, self.head_dim)\n\n        # Combine long-term and local outputs\n        output = long_term_output + local_output\n\n        # Remove padding if added\n        if padding &gt; 0:\n            output = output[:, :, :seq_len, :]\n\n        # Reshape back to [batch, seq_len, hidden_dim]\n        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n\n        # Final projection\n        output = self.out_proj(output)\n\n        return output\n</code></pre>"},{"location":"api/bsbr/#bsbr.bsbr.BSBRAttention.compute_chunk_states","title":"<code>compute_chunk_states(keys, values)</code>","text":"<p>Compute F states by flattening K^T\u00b7V for each chunk.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Tensor</code> <p>[batch, num_heads, num_chunks, chunk_size, head_dim]</p> required <code>values</code> <code>Tensor</code> <p>[batch, num_heads, num_chunks, chunk_size, head_dim]</p> required <p>Returns:</p> Name Type Description <code>states</code> <code>Tensor</code> <p>[batch, num_heads, num_chunks, state_dim]</p> Source code in <code>src/bsbr/bsbr.py</code> <pre><code>def compute_chunk_states(\n    self, \n    keys: torch.Tensor, \n    values: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute F states by flattening K^T\u00b7V for each chunk.\n\n    Args:\n        keys: [batch, num_heads, num_chunks, chunk_size, head_dim]\n        values: [batch, num_heads, num_chunks, chunk_size, head_dim]\n\n    Returns:\n        states: [batch, num_heads, num_chunks, state_dim]\n    \"\"\"\n    batch_size, num_heads, num_chunks, chunk_size, head_dim = keys.size()\n\n    # For each chunk, compute K^T\u00b7V\n    # Reshape to [batch*num_heads*num_chunks, chunk_size, head_dim]\n    keys_flat = keys.reshape(-1, chunk_size, head_dim)\n    values_flat = values.reshape(-1, chunk_size, head_dim)\n\n    # Compute K^T\u00b7V for each chunk\n    # [chunk_size, head_dim] @ [chunk_size, head_dim] -&gt; [head_dim, head_dim]\n    states = torch.bmm(\n        keys_flat.transpose(1, 2),  # [B*H*C, head_dim, chunk_size]\n        values_flat                 # [B*H*C, chunk_size, head_dim]\n    )\n\n    # Flatten the state matrices to vectors\n    states_flat = states.reshape(batch_size, num_heads, num_chunks, -1)\n\n    # Apply optional compression\n    if self.compression_factor is not None:\n        states_flat = self.compress_proj(states_flat)\n\n    return states_flat\n</code></pre>"},{"location":"api/bsbr/#bsbr.bsbr.BSBRAttention.forward","title":"<code>forward(hidden_states, attention_mask=None)</code>","text":"<p>Forward pass of the BSBR attention mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_states</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, seq_len, hidden_dim]</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask of shape [batch_size, seq_len]</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> Source code in <code>src/bsbr/bsbr.py</code> <pre><code>def forward(\n    self, \n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the BSBR attention mechanism.\n\n    Args:\n        hidden_states: Input tensor of shape [batch_size, seq_len, hidden_dim]\n        attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n    Returns:\n        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n    \"\"\"\n    batch_size, seq_len, _ = hidden_states.size()\n    num_chunks = math.ceil(seq_len / self.chunk_size)\n\n    # Create masks for within-chunk and between-chunk attention\n    m_in, m_out = self._create_masks(seq_len)\n    if attention_mask is not None:\n        # Fix: Expand attention_mask to match m_in dimensions\n        attention_mask_expanded = attention_mask.unsqueeze(1).unsqueeze(2).expand(-1, -1, seq_len, -1)\n        m_in = m_in.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n        m_in = m_in * attention_mask_expanded\n\n    # Standard projections for Q, K, V\n    q = self._reshape_for_heads(self.q_proj(hidden_states))  # [batch, num_heads, seq_len, head_dim]\n    k = self._reshape_for_heads(self.k_proj(hidden_states))  # [batch, num_heads, seq_len, head_dim]\n    v = self._reshape_for_heads(self.v_proj(hidden_states))  # [batch, num_heads, seq_len, head_dim]\n\n    # Reshape to chunks\n    # Pad sequence length to be divisible by chunk_size\n    padding = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n    if padding &gt; 0:\n        q = F.pad(q, (0, 0, 0, padding))\n        k = F.pad(k, (0, 0, 0, padding))\n        v = F.pad(v, (0, 0, 0, padding))\n        padded_seq_len = seq_len + padding\n    else:\n        padded_seq_len = seq_len\n\n    # Reshape to chunks [batch, num_heads, num_chunks, chunk_size, head_dim]\n    q_chunks = q.view(batch_size, self.num_heads, num_chunks, self.chunk_size, self.head_dim)\n    k_chunks = k.view(batch_size, self.num_heads, num_chunks, self.chunk_size, self.head_dim)\n    v_chunks = v.view(batch_size, self.num_heads, num_chunks, self.chunk_size, self.head_dim)\n\n    # Meta projections for chunk-level attention\n    # Fix: First, get a representative hidden state for each chunk\n    # Create a padded version of hidden_states for safe reshaping\n    if padding &gt; 0:\n        padded_hidden = F.pad(hidden_states, (0, 0, 0, padding))\n    else:\n        padded_hidden = hidden_states\n\n    # Reshape to [batch, num_chunks, chunk_size, hidden_dim]\n    chunk_hidden = padded_hidden.view(batch_size, num_chunks, self.chunk_size, self.hidden_dim)\n\n    # Get the last token of each chunk as representative\n    # For the final chunk, make sure we don't index beyond valid tokens\n    chunk_repr = []\n    for i in range(num_chunks):\n        if i == num_chunks - 1 and padding &gt; 0:\n            # For the last chunk, get the last valid token\n            last_valid_idx = self.chunk_size - padding - 1\n            if last_valid_idx &lt; 0:  # In case the last chunk is all padding\n                last_valid_idx = 0\n            repr_i = chunk_hidden[:, i, last_valid_idx, :]\n        else:\n            # For full chunks, get the last token\n            repr_i = chunk_hidden[:, i, -1, :]\n        chunk_repr.append(repr_i)\n\n    # Stack to [batch, num_chunks, hidden_dim]\n    chunk_repr = torch.stack(chunk_repr, dim=1)\n\n    # Project to meta queries and keys\n    r = self.meta_r_proj(chunk_repr)  # [batch, num_chunks, hidden_dim]\n    h = self.meta_h_proj(chunk_repr)  # [batch, num_chunks, hidden_dim]\n\n    # Reshape for multi-head attention\n    r = r.view(batch_size, num_chunks, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, num_chunks, head_dim]\n    h = h.view(batch_size, num_chunks, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, num_chunks, head_dim]\n\n    # Compute chunk states (F)\n    f = self.compute_chunk_states(k_chunks, v_chunks)  # [batch, num_heads, num_chunks, state_dim]\n\n    # Between-chunk attention: softmax(R\u00b7H^T)\u00b7F\n    # Calculate R\u00b7H^T\n    chunk_attn_scores = torch.matmul(r, h.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [batch, num_heads, num_chunks, num_chunks]\n\n    # Apply causal mask for between-chunk attention\n    m_out = m_out.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n    chunk_attn_scores = chunk_attn_scores.masked_fill(m_out == 0, -1e9)\n\n    # Apply softmax\n    chunk_attn_probs = F.softmax(chunk_attn_scores, dim=-1)  # [batch, num_heads, num_chunks, num_chunks]\n    chunk_attn_probs = self.dropout_layer(chunk_attn_probs)\n\n    # Calculate retrieved states\n    retrieved_states = torch.matmul(chunk_attn_probs, f)  # [batch, num_heads, num_chunks, state_dim]\n\n    # Decompress if needed\n    if self.compression_factor is not None:\n        retrieved_states = self.decompress_proj(retrieved_states)\n\n    # Expand retrieved states to match each position in the chunk\n    # First reshape retrieved_states to [batch, num_heads, num_chunks, head_dim, head_dim]\n    retrieved_states = retrieved_states.view(batch_size, self.num_heads, num_chunks, self.head_dim, self.head_dim)\n\n    # Multiply query with retrieved states: q_chunks @ retrieved_states\n    # For each position in each chunk, calculate q @ retrieved_state\n    # [batch, num_heads, num_chunks, chunk_size, head_dim] @ [batch, num_heads, num_chunks, head_dim, head_dim]\n    # -&gt; [batch, num_heads, num_chunks, chunk_size, head_dim]\n    long_term_output = torch.matmul(q_chunks, retrieved_states)\n\n    # Within-chunk attention\n    # Calculate local attention scores\n    local_attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [batch, num_heads, padded_seq_len, padded_seq_len]\n\n    # Fix: Apply the correct block diagonal causal mask\n    # First create a mask for the padded sequence\n    padded_m_in = torch.zeros(padded_seq_len, padded_seq_len, device=m_in.device)\n\n    for i in range(num_chunks):\n        start_idx = i * self.chunk_size\n        end_idx = (i + 1) * self.chunk_size\n        # Create causal mask within each chunk\n        chunk_mask = torch.triu(torch.ones(self.chunk_size, self.chunk_size, device=padded_m_in.device), diagonal=0)\n        padded_m_in[start_idx:end_idx, start_idx:end_idx] = chunk_mask\n\n    # Expand dimensions for broadcasting\n    expanded_mask = padded_m_in.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n\n    # Apply the mask\n    local_attn_scores = local_attn_scores.masked_fill(expanded_mask == 0, -1e9)\n\n    # Apply softmax\n    local_attn_probs = F.softmax(local_attn_scores, dim=-1)\n    local_attn_probs = self.dropout_layer(local_attn_probs)\n\n    # Apply attention\n    local_output = torch.matmul(local_attn_probs, v)  # [batch, num_heads, padded_seq_len, head_dim]\n\n    # Reshape long_term_output to match local_output\n    long_term_output = long_term_output.view(batch_size, self.num_heads, padded_seq_len, self.head_dim)\n\n    # Combine long-term and local outputs\n    output = long_term_output + local_output\n\n    # Remove padding if added\n    if padding &gt; 0:\n        output = output[:, :, :seq_len, :]\n\n    # Reshape back to [batch, seq_len, hidden_dim]\n    output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n\n    # Final projection\n    output = self.out_proj(output)\n\n    return output\n</code></pre>"},{"location":"api/bsbr/#bsbr.bsbr.BSBRLayer","title":"<code>BSBRLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A single BSBR layer with attention and feed-forward networks.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>chunk_size</code> <code>int</code> <p>Size of each chunk</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> <code>compression_factor</code> <code>Optional[int]</code> <p>If provided, compresses the state dimension</p> <code>None</code> Source code in <code>src/bsbr/bsbr.py</code> <pre><code>class BSBRLayer(nn.Module):\n    \"\"\"\n    A single BSBR layer with attention and feed-forward networks.\n\n    Args:\n        hidden_dim (int): Hidden dimension size\n        num_heads (int): Number of attention heads\n        chunk_size (int): Size of each chunk\n        ff_dim (int): Feed-forward intermediate dimension\n        dropout (float): Dropout probability\n        compression_factor (Optional[int]): If provided, compresses the state dimension\n    \"\"\"\n    def __init__(\n        self,\n        hidden_dim: int,\n        num_heads: int,\n        chunk_size: int,\n        ff_dim: int,\n        dropout: float = 0.1,\n        compression_factor: Optional[int] = None\n    ):\n        super().__init__()\n        self.attention = BSBRAttention(\n            hidden_dim=hidden_dim,\n            num_heads=num_heads,\n            chunk_size=chunk_size,\n            dropout=dropout,\n            compression_factor=compression_factor\n        )\n\n        # Layer normalization\n        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n\n        # Feed-forward network\n        self.ff = nn.Sequential(\n            nn.Linear(hidden_dim, ff_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(ff_dim, hidden_dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass for a single BSBR layer.\"\"\"\n        # Pre-LayerNorm architecture\n        residual = hidden_states\n        hidden_states = self.layer_norm1(hidden_states)\n        hidden_states = self.attention(hidden_states, attention_mask)\n        hidden_states = residual + hidden_states\n\n        # Feed-forward network\n        residual = hidden_states\n        hidden_states = self.layer_norm2(hidden_states)\n        hidden_states = self.ff(hidden_states)\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n</code></pre>"},{"location":"api/bsbr/#bsbr.bsbr.BSBRLayer.forward","title":"<code>forward(hidden_states, attention_mask=None)</code>","text":"<p>Forward pass for a single BSBR layer.</p> Source code in <code>src/bsbr/bsbr.py</code> <pre><code>def forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass for a single BSBR layer.\"\"\"\n    # Pre-LayerNorm architecture\n    residual = hidden_states\n    hidden_states = self.layer_norm1(hidden_states)\n    hidden_states = self.attention(hidden_states, attention_mask)\n    hidden_states = residual + hidden_states\n\n    # Feed-forward network\n    residual = hidden_states\n    hidden_states = self.layer_norm2(hidden_states)\n    hidden_states = self.ff(hidden_states)\n    hidden_states = residual + hidden_states\n\n    return hidden_states\n</code></pre>"},{"location":"api/bsbr_extras/","title":"BSBR Extras API","text":""},{"location":"api/bsbr_extras/#bsbr_extras.standard_transformer.StandardTransformerModel","title":"<code>bsbr_extras.standard_transformer.StandardTransformerModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Full Standard Transformer model stacking multiple Standard Transformer layers.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Vocabulary size for embedding layer</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_layers</code> <code>int</code> <p>Number of transformer layers</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> Source code in <code>src/bsbr_extras/standard_transformer.py</code> <pre><code>class StandardTransformerModel(nn.Module):\n    \"\"\"\n    Full Standard Transformer model stacking multiple Standard Transformer layers.\n\n    Args:\n        vocab_size (int): Vocabulary size for embedding layer\n        hidden_dim (int): Hidden dimension size\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward intermediate dimension\n        dropout (float): Dropout probability\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ff_dim: int,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_encoding = PositionalEncoding(hidden_dim, dropout)\n\n        self.layers = nn.ModuleList([\n            StandardTransformerLayer(\n                hidden_dim=hidden_dim,\n                num_heads=num_heads,\n                ff_dim=ff_dim,\n                dropout=dropout\n            )\n            for _ in range(num_layers)\n        ])\n\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for the full Standard Transformer model.\n\n        Args:\n            input_ids: Token IDs of shape [batch_size, seq_len]\n            attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n        Returns:\n            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n        \"\"\"\n        hidden_states = self.embedding(input_ids)\n        hidden_states = self.pos_encoding(hidden_states)\n\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n\n        hidden_states = self.layer_norm(hidden_states)\n        return hidden_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.standard_transformer.StandardTransformerModel.forward","title":"<code>forward(input_ids, attention_mask=None)</code>","text":"<p>Forward pass for the full Standard Transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>LongTensor</code> <p>Token IDs of shape [batch_size, seq_len]</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask of shape [batch_size, seq_len]</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> Source code in <code>src/bsbr_extras/standard_transformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for the full Standard Transformer model.\n\n    Args:\n        input_ids: Token IDs of shape [batch_size, seq_len]\n        attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n    Returns:\n        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n    \"\"\"\n    hidden_states = self.embedding(input_ids)\n    hidden_states = self.pos_encoding(hidden_states)\n\n    for layer in self.layers:\n        hidden_states = layer(hidden_states, attention_mask)\n\n    hidden_states = self.layer_norm(hidden_states)\n    return hidden_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.linear_transformer.LinearTransformerModel","title":"<code>bsbr_extras.linear_transformer.LinearTransformerModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Full Linear Transformer model stacking multiple Linear Transformer layers.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Vocabulary size for embedding layer</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_layers</code> <code>int</code> <p>Number of LinearTransformer layers</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> Source code in <code>src/bsbr_extras/linear_transformer.py</code> <pre><code>class LinearTransformerModel(nn.Module):\n    \"\"\"\n    Full Linear Transformer model stacking multiple Linear Transformer layers.\n\n    Args:\n        vocab_size (int): Vocabulary size for embedding layer\n        hidden_dim (int): Hidden dimension size\n        num_layers (int): Number of LinearTransformer layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward intermediate dimension\n        dropout (float): Dropout probability\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ff_dim: int,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_encoding = PositionalEncoding(hidden_dim, dropout)\n\n        self.layers = nn.ModuleList([\n            LinearTransformerLayer(\n                hidden_dim=hidden_dim,\n                num_heads=num_heads,\n                ff_dim=ff_dim,\n                dropout=dropout\n            )\n            for _ in range(num_layers)\n        ])\n\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        self.num_layers = num_layers\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        states: Optional[list] = None\n    ) -&gt; Tuple[torch.Tensor, list]:\n        \"\"\"\n        Forward pass for the full Linear Transformer model.\n\n        Args:\n            input_ids: Token IDs of shape [batch_size, seq_len]\n            attention_mask: Optional attention mask of shape [batch_size, seq_len]\n            states: Optional previous state list for each layer\n\n        Returns:\n            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n            new_states: Updated state list for each layer\n        \"\"\"\n        hidden_states = self.embedding(input_ids)\n        hidden_states = self.pos_encoding(hidden_states)\n\n        # Initialize states if not provided\n        if states is None:\n            states = [None] * self.num_layers\n\n        new_states = []\n\n        for i, layer in enumerate(self.layers):\n            hidden_states, new_state = layer(hidden_states, attention_mask, states[i])\n            new_states.append(new_state)\n\n        hidden_states = self.layer_norm(hidden_states)\n\n        return hidden_states, new_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.linear_transformer.LinearTransformerModel.forward","title":"<code>forward(input_ids, attention_mask=None, states=None)</code>","text":"<p>Forward pass for the full Linear Transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>LongTensor</code> <p>Token IDs of shape [batch_size, seq_len]</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask of shape [batch_size, seq_len]</p> <code>None</code> <code>states</code> <code>Optional[list]</code> <p>Optional previous state list for each layer</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> <code>new_states</code> <code>list</code> <p>Updated state list for each layer</p> Source code in <code>src/bsbr_extras/linear_transformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    states: Optional[list] = None\n) -&gt; Tuple[torch.Tensor, list]:\n    \"\"\"\n    Forward pass for the full Linear Transformer model.\n\n    Args:\n        input_ids: Token IDs of shape [batch_size, seq_len]\n        attention_mask: Optional attention mask of shape [batch_size, seq_len]\n        states: Optional previous state list for each layer\n\n    Returns:\n        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n        new_states: Updated state list for each layer\n    \"\"\"\n    hidden_states = self.embedding(input_ids)\n    hidden_states = self.pos_encoding(hidden_states)\n\n    # Initialize states if not provided\n    if states is None:\n        states = [None] * self.num_layers\n\n    new_states = []\n\n    for i, layer in enumerate(self.layers):\n        hidden_states, new_state = layer(hidden_states, attention_mask, states[i])\n        new_states.append(new_state)\n\n    hidden_states = self.layer_norm(hidden_states)\n\n    return hidden_states, new_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.delta_net.DeltaNetModel","title":"<code>bsbr_extras.delta_net.DeltaNetModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Full DeltaNet model stacking multiple DeltaNet layers.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Vocabulary size for embedding layer</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_layers</code> <code>int</code> <p>Number of DeltaNet layers</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <code>beta</code> <code>float</code> <p>Forgetting/update rate parameter (\u03b2 in the paper)</p> <code>0.9</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> Source code in <code>src/bsbr_extras/delta_net.py</code> <pre><code>class DeltaNetModel(nn.Module):\n    \"\"\"\n    Full DeltaNet model stacking multiple DeltaNet layers.\n\n    Args:\n        vocab_size (int): Vocabulary size for embedding layer\n        hidden_dim (int): Hidden dimension size\n        num_layers (int): Number of DeltaNet layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward intermediate dimension\n        beta (float): Forgetting/update rate parameter (\u03b2 in the paper)\n        dropout (float): Dropout probability\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ff_dim: int,\n        beta: float = 0.9,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_encoding = PositionalEncoding(hidden_dim, dropout)\n\n        self.layers = nn.ModuleList([\n            DeltaNetLayer(\n                hidden_dim=hidden_dim,\n                num_heads=num_heads,\n                ff_dim=ff_dim,\n                beta=beta,\n                dropout=dropout\n            )\n            for _ in range(num_layers)\n        ])\n\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        self.num_layers = num_layers\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        states: Optional[list] = None\n    ) -&gt; Tuple[torch.Tensor, list]:\n        \"\"\"\n        Forward pass for the full DeltaNet model.\n\n        Args:\n            input_ids: Token IDs of shape [batch_size, seq_len]\n            attention_mask: Optional attention mask of shape [batch_size, seq_len]\n            states: Optional previous state list for each layer\n\n        Returns:\n            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n            new_states: Updated state list for each layer\n        \"\"\"\n        hidden_states = self.embedding(input_ids)\n        hidden_states = self.pos_encoding(hidden_states)\n\n        # Initialize states if not provided\n        if states is None:\n            states = [None] * self.num_layers\n\n        new_states = []\n\n        for i, layer in enumerate(self.layers):\n            hidden_states, new_state = layer(hidden_states, attention_mask, states[i])\n            new_states.append(new_state)\n\n        hidden_states = self.layer_norm(hidden_states)\n\n        return hidden_states, new_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.delta_net.DeltaNetModel.forward","title":"<code>forward(input_ids, attention_mask=None, states=None)</code>","text":"<p>Forward pass for the full DeltaNet model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>LongTensor</code> <p>Token IDs of shape [batch_size, seq_len]</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask of shape [batch_size, seq_len]</p> <code>None</code> <code>states</code> <code>Optional[list]</code> <p>Optional previous state list for each layer</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> <code>new_states</code> <code>list</code> <p>Updated state list for each layer</p> Source code in <code>src/bsbr_extras/delta_net.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    states: Optional[list] = None\n) -&gt; Tuple[torch.Tensor, list]:\n    \"\"\"\n    Forward pass for the full DeltaNet model.\n\n    Args:\n        input_ids: Token IDs of shape [batch_size, seq_len]\n        attention_mask: Optional attention mask of shape [batch_size, seq_len]\n        states: Optional previous state list for each layer\n\n    Returns:\n        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n        new_states: Updated state list for each layer\n    \"\"\"\n    hidden_states = self.embedding(input_ids)\n    hidden_states = self.pos_encoding(hidden_states)\n\n    # Initialize states if not provided\n    if states is None:\n        states = [None] * self.num_layers\n\n    new_states = []\n\n    for i, layer in enumerate(self.layers):\n        hidden_states, new_state = layer(hidden_states, attention_mask, states[i])\n        new_states.append(new_state)\n\n    hidden_states = self.layer_norm(hidden_states)\n\n    return hidden_states, new_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.gau.GAUModel","title":"<code>bsbr_extras.gau.GAUModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Full Gated Attention Unit model stacking multiple GAU layers.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Vocabulary size for embedding layer</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_layers</code> <code>int</code> <p>Number of GAU layers</p> required <code>chunk_size</code> <code>int</code> <p>Size of chunks for parallel processing</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <code>expansion_factor</code> <code>int</code> <p>Expansion factor for GAU</p> <code>2</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> Source code in <code>src/bsbr_extras/gau.py</code> <pre><code>class GAUModel(nn.Module):\n    \"\"\"\n    Full Gated Attention Unit model stacking multiple GAU layers.\n\n    Args:\n        vocab_size (int): Vocabulary size for embedding layer\n        hidden_dim (int): Hidden dimension size\n        num_layers (int): Number of GAU layers\n        chunk_size (int): Size of chunks for parallel processing\n        ff_dim (int): Feed-forward intermediate dimension\n        expansion_factor (int): Expansion factor for GAU\n        dropout (float): Dropout probability\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_dim: int,\n        num_layers: int,\n        chunk_size: int,\n        ff_dim: int,\n        expansion_factor: int = 2,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_encoding = PositionalEncoding(hidden_dim, dropout)\n\n        self.layers = nn.ModuleList([\n            GAULayer(\n                hidden_dim=hidden_dim,\n                chunk_size=chunk_size,\n                ff_dim=ff_dim,\n                expansion_factor=expansion_factor,\n                dropout=dropout\n            )\n            for _ in range(num_layers)\n        ])\n\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for the full GAU model.\n\n        Args:\n            input_ids: Token IDs of shape [batch_size, seq_len]\n            attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n        Returns:\n            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n        \"\"\"\n        hidden_states = self.embedding(input_ids)\n        hidden_states = self.pos_encoding(hidden_states)\n\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n\n        hidden_states = self.layer_norm(hidden_states)\n        return hidden_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.gau.GAUModel.forward","title":"<code>forward(input_ids, attention_mask=None)</code>","text":"<p>Forward pass for the full GAU model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>LongTensor</code> <p>Token IDs of shape [batch_size, seq_len]</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask of shape [batch_size, seq_len]</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> Source code in <code>src/bsbr_extras/gau.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for the full GAU model.\n\n    Args:\n        input_ids: Token IDs of shape [batch_size, seq_len]\n        attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n    Returns:\n        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n    \"\"\"\n    hidden_states = self.embedding(input_ids)\n    hidden_states = self.pos_encoding(hidden_states)\n\n    for layer in self.layers:\n        hidden_states = layer(hidden_states, attention_mask)\n\n    hidden_states = self.layer_norm(hidden_states)\n    return hidden_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.hopfield_network.HopfieldNetworkModel","title":"<code>bsbr_extras.hopfield_network.HopfieldNetworkModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Full Hopfield Network model stacking multiple Hopfield Network layers.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Vocabulary size for embedding layer</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_layers</code> <code>int</code> <p>Number of Hopfield Network layers</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <code>temperature</code> <code>float</code> <p>Temperature parameter for the Hopfield energy function</p> <code>1.0</code> <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> Source code in <code>src/bsbr_extras/hopfield_network.py</code> <pre><code>class HopfieldNetworkModel(nn.Module):\n    \"\"\"\n    Full Hopfield Network model stacking multiple Hopfield Network layers.\n\n    Args:\n        vocab_size (int): Vocabulary size for embedding layer\n        hidden_dim (int): Hidden dimension size\n        num_layers (int): Number of Hopfield Network layers\n        num_heads (int): Number of attention heads\n        ff_dim (int): Feed-forward intermediate dimension\n        temperature (float): Temperature parameter for the Hopfield energy function\n        dropout (float): Dropout probability\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_dim: int,\n        num_layers: int,\n        num_heads: int,\n        ff_dim: int,\n        temperature: float = 1.0,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_encoding = PositionalEncoding(hidden_dim, dropout)\n\n        self.layers = nn.ModuleList([\n            HopfieldNetworkLayer(\n                hidden_dim=hidden_dim,\n                num_heads=num_heads,\n                ff_dim=ff_dim,\n                temperature=temperature,\n                dropout=dropout\n            )\n            for _ in range(num_layers)\n        ])\n\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n        self.num_layers = num_layers\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        states: Optional[list] = None\n    ) -&gt; Tuple[torch.Tensor, list]:\n        \"\"\"\n        Forward pass for the full Hopfield Network model.\n\n        Args:\n            input_ids: Token IDs of shape [batch_size, seq_len]\n            attention_mask: Optional attention mask of shape [batch_size, seq_len]\n            states: Optional previous states list for each layer\n\n        Returns:\n            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n            new_states: Updated states list for each layer\n        \"\"\"\n        hidden_states = self.embedding(input_ids)\n        hidden_states = self.pos_encoding(hidden_states)\n\n        # Initialize states if not provided\n        if states is None:\n            states = [None] * self.num_layers\n\n        new_states = []\n\n        for i, layer in enumerate(self.layers):\n            hidden_states, new_state = layer(hidden_states, attention_mask, states[i])\n            new_states.append(new_state)\n\n        hidden_states = self.layer_norm(hidden_states)\n\n        return hidden_states, new_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.hopfield_network.HopfieldNetworkModel.forward","title":"<code>forward(input_ids, attention_mask=None, states=None)</code>","text":"<p>Forward pass for the full Hopfield Network model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>LongTensor</code> <p>Token IDs of shape [batch_size, seq_len]</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask of shape [batch_size, seq_len]</p> <code>None</code> <code>states</code> <code>Optional[list]</code> <p>Optional previous states list for each layer</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> <code>new_states</code> <code>list</code> <p>Updated states list for each layer</p> Source code in <code>src/bsbr_extras/hopfield_network.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    states: Optional[list] = None\n) -&gt; Tuple[torch.Tensor, list]:\n    \"\"\"\n    Forward pass for the full Hopfield Network model.\n\n    Args:\n        input_ids: Token IDs of shape [batch_size, seq_len]\n        attention_mask: Optional attention mask of shape [batch_size, seq_len]\n        states: Optional previous states list for each layer\n\n    Returns:\n        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n        new_states: Updated states list for each layer\n    \"\"\"\n    hidden_states = self.embedding(input_ids)\n    hidden_states = self.pos_encoding(hidden_states)\n\n    # Initialize states if not provided\n    if states is None:\n        states = [None] * self.num_layers\n\n    new_states = []\n\n    for i, layer in enumerate(self.layers):\n        hidden_states, new_state = layer(hidden_states, attention_mask, states[i])\n        new_states.append(new_state)\n\n    hidden_states = self.layer_norm(hidden_states)\n\n    return hidden_states, new_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel","title":"<code>bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Full Sliding Window Transformer model stacking multiple transformer layers.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Vocabulary size for embedding layer</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_layers</code> <code>int</code> <p>Number of transformer layers</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>window_size</code> <code>int</code> <p>Size of the attention window</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <code>dropout</code> <code>float</code> <p>Dropout probability</p> <code>0.1</code> Source code in <code>src/bsbr_extras/sliding_window_transformer.py</code> <pre><code>class SlidingWindowTransformerModel(nn.Module):\n    \"\"\"\n    Full Sliding Window Transformer model stacking multiple transformer layers.\n\n    Args:\n        vocab_size (int): Vocabulary size for embedding layer\n        hidden_dim (int): Hidden dimension size\n        num_layers (int): Number of transformer layers\n        num_heads (int): Number of attention heads\n        window_size (int): Size of the attention window\n        ff_dim (int): Feed-forward intermediate dimension\n        dropout (float): Dropout probability\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_dim: int,\n        num_layers: int,\n        num_heads: int,\n        window_size: int,\n        ff_dim: int,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.pos_encoding = PositionalEncoding(hidden_dim, dropout)\n\n        self.layers = nn.ModuleList([\n            SlidingWindowTransformerLayer(\n                hidden_dim=hidden_dim,\n                num_heads=num_heads,\n                window_size=window_size,\n                ff_dim=ff_dim,\n                dropout=dropout\n            )\n            for _ in range(num_layers)\n        ])\n\n        self.layer_norm = nn.LayerNorm(hidden_dim)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for the full Sliding Window Transformer model.\n\n        Args:\n            input_ids: Token IDs of shape [batch_size, seq_len]\n            attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n        Returns:\n            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n        \"\"\"\n        hidden_states = self.embedding(input_ids)\n        hidden_states = self.pos_encoding(hidden_states)\n\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n\n        hidden_states = self.layer_norm(hidden_states)\n        return hidden_states\n</code></pre>"},{"location":"api/bsbr_extras/#bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel.forward","title":"<code>forward(input_ids, attention_mask=None)</code>","text":"<p>Forward pass for the full Sliding Window Transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>LongTensor</code> <p>Token IDs of shape [batch_size, seq_len]</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Optional attention mask of shape [batch_size, seq_len]</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> Source code in <code>src/bsbr_extras/sliding_window_transformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.LongTensor,\n    attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for the full Sliding Window Transformer model.\n\n    Args:\n        input_ids: Token IDs of shape [batch_size, seq_len]\n        attention_mask: Optional attention mask of shape [batch_size, seq_len]\n\n    Returns:\n        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]\n    \"\"\"\n    hidden_states = self.embedding(input_ids)\n    hidden_states = self.pos_encoding(hidden_states)\n\n    for layer in self.layers:\n        hidden_states = layer(hidden_states, attention_mask)\n\n    hidden_states = self.layer_norm(hidden_states)\n    return hidden_states\n</code></pre>"},{"location":"api/bsbr_transformers/","title":"BSBR Transformers API","text":""},{"location":"api/bsbr_transformers/#bsbr_transformers","title":"<code>bsbr_transformers</code>","text":""},{"location":"api/bsbr_transformers/#bsbr_transformers.TransformerToBSBRConverter","title":"<code>TransformerToBSBRConverter</code>","text":"<p>Utility to convert vanilla transformer models (especially GPT-style models) to Block Sparse Attention with Block Retrieval (BSBR) transformers.</p> <p>This converter allows reusing pre-trained weights while benefiting from the efficiency of BSBR for processing longer sequences.</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>class TransformerToBSBRConverter:\n    \"\"\"\n    Utility to convert vanilla transformer models (especially GPT-style models)\n    to Block Sparse Attention with Block Retrieval (BSBR) transformers.\n\n    This converter allows reusing pre-trained weights while benefiting from the\n    efficiency of BSBR for processing longer sequences.\n    \"\"\"\n\n    def __init__(self, chunk_size: int = 128, compression_factor: Optional[int] = None):\n        \"\"\"\n        Initialize the converter with BSBR-specific parameters.\n\n        Args:\n            chunk_size: Size of chunks for BSBR processing\n            compression_factor: Optional factor to compress state vectors\n        \"\"\"\n        self.chunk_size = chunk_size\n        self.compression_factor = compression_factor\n\n    def convert_gpt2_attention_to_bsbr(self, \n                                      gpt_attention: GPT2Attention, \n                                      hidden_dim: int,\n                                      num_heads: int) -&gt; BSBRAttention:\n        \"\"\"\n        Convert a GPT2 attention layer to a BSBR attention layer.\n\n        Args:\n            gpt_attention: The GPT2 attention layer to convert\n            hidden_dim: Hidden dimension size\n            num_heads: Number of attention heads\n\n        Returns:\n            A BSBR attention layer with weights initialized from the GPT2 layer\n        \"\"\"\n        bsbr_attention = BSBRAttention(\n            hidden_dim=hidden_dim,\n            num_heads=num_heads,\n            chunk_size=self.chunk_size,\n            dropout=gpt_attention.attn_dropout.p,\n            compression_factor=self.compression_factor\n        )\n\n        # Handle GPT2's Conv1D vs Linear difference\n        # In GPT2, Conv1D has shape (out_features, in_features) and is transposed compared to nn.Linear\n        # c_attn is a single matrix for q, k, v concatenated, shape is (hidden_dim, 3*hidden_dim)\n\n        # For q, k, v projections, we need to handle the transposed nature of Conv1D vs nn.Linear\n        # The Conv1D weight is already transposed compared to nn.Linear, so we need to transpose it back\n        if hasattr(gpt_attention, 'c_attn'):\n            # Split the weights for Q, K, V\n            qkv_weight = gpt_attention.c_attn.weight\n            qkv_bias = gpt_attention.c_attn.bias\n\n            # Check if qkv_weight has the expected shape (hidden_dim, 3*hidden_dim) or (3*hidden_dim, hidden_dim)\n            if qkv_weight.shape[0] == hidden_dim:\n                # Weights have shape (hidden_dim, 3*hidden_dim)\n                # Need to transpose for nn.Linear which expects (out_features, in_features)\n                q_weight = qkv_weight[:, :hidden_dim].t()\n                k_weight = qkv_weight[:, hidden_dim:2*hidden_dim].t()\n                v_weight = qkv_weight[:, 2*hidden_dim:3*hidden_dim].t()\n\n                # Biases don't need transposition\n                q_bias = qkv_bias[:hidden_dim] if qkv_bias is not None else None\n                k_bias = qkv_bias[hidden_dim:2*hidden_dim] if qkv_bias is not None else None\n                v_bias = qkv_bias[2*hidden_dim:3*hidden_dim] if qkv_bias is not None else None\n            else:\n                # Weights have shape (3*hidden_dim, hidden_dim)\n                q_weight = qkv_weight[:hidden_dim, :]\n                k_weight = qkv_weight[hidden_dim:2*hidden_dim, :]\n                v_weight = qkv_weight[2*hidden_dim:3*hidden_dim, :]\n\n                # Biases\n                q_bias = qkv_bias[:hidden_dim] if qkv_bias is not None else None\n                k_bias = qkv_bias[hidden_dim:2*hidden_dim] if qkv_bias is not None else None\n                v_bias = qkv_bias[2*hidden_dim:3*hidden_dim] if qkv_bias is not None else None\n        else:\n            # If c_attn doesn't exist, try to find separate q, k, v projections\n            q_weight = gpt_attention.q_proj.weight if hasattr(gpt_attention, 'q_proj') else None\n            k_weight = gpt_attention.k_proj.weight if hasattr(gpt_attention, 'k_proj') else None\n            v_weight = gpt_attention.v_proj.weight if hasattr(gpt_attention, 'v_proj') else None\n\n            q_bias = gpt_attention.q_proj.bias if hasattr(gpt_attention, 'q_proj') else None\n            k_bias = gpt_attention.k_proj.bias if hasattr(gpt_attention, 'k_proj') else None\n            v_bias = gpt_attention.v_proj.bias if hasattr(gpt_attention, 'v_proj') else None\n\n        # Copy weights to BSBR attention\n        if q_weight is not None:\n            bsbr_attention.q_proj.weight.data = q_weight\n        if k_weight is not None:\n            bsbr_attention.k_proj.weight.data = k_weight\n        if v_weight is not None:\n            bsbr_attention.v_proj.weight.data = v_weight\n\n        # Copy biases\n        if q_bias is not None:\n            bsbr_attention.q_proj.bias.data = q_bias\n        if k_bias is not None:\n            bsbr_attention.k_proj.bias.data = k_bias\n        if v_bias is not None:\n            bsbr_attention.v_proj.bias.data = v_bias\n\n        # Handle output projection\n        if hasattr(gpt_attention, 'c_proj'):\n            if gpt_attention.c_proj.weight.shape[0] == hidden_dim:\n                # Weight is transposed compared to nn.Linear\n                bsbr_attention.out_proj.weight.data = gpt_attention.c_proj.weight.t()\n            else:\n                bsbr_attention.out_proj.weight.data = gpt_attention.c_proj.weight\n\n            if hasattr(gpt_attention.c_proj, 'bias') and gpt_attention.c_proj.bias is not None:\n                bsbr_attention.out_proj.bias.data = gpt_attention.c_proj.bias\n\n        # Initialize meta projections as combinations of existing projections\n        # This is a heuristic approach to provide a reasonable starting point\n        bsbr_attention.meta_r_proj.weight.data = (bsbr_attention.q_proj.weight.data + \n                                                bsbr_attention.k_proj.weight.data) / 2\n        bsbr_attention.meta_h_proj.weight.data = (bsbr_attention.k_proj.weight.data + \n                                                bsbr_attention.v_proj.weight.data) / 2\n\n        if hasattr(bsbr_attention.q_proj, 'bias') and bsbr_attention.q_proj.bias is not None:\n            bsbr_attention.meta_r_proj.bias.data = (bsbr_attention.q_proj.bias.data + \n                                                  bsbr_attention.k_proj.bias.data) / 2\n            bsbr_attention.meta_h_proj.bias.data = (bsbr_attention.k_proj.bias.data + \n                                                  bsbr_attention.v_proj.bias.data) / 2\n\n        return bsbr_attention\n\n    def convert_gpt2_layer_to_bsbr(self, \n                                  gpt_layer, \n                                  hidden_dim: int,\n                                  num_heads: int,\n                                  ff_dim: int) -&gt; BSBRLayer:\n        \"\"\"\n        Convert a GPT2 layer to a BSBR layer.\n\n        Args:\n            gpt_layer: The GPT2 layer to convert\n            hidden_dim: Hidden dimension size\n            num_heads: Number of attention heads\n            ff_dim: Feed-forward intermediate dimension\n\n        Returns:\n            A BSBR layer with weights initialized from the GPT2 layer\n        \"\"\"\n        # Ensure ff_dim is not None or zero\n        if ff_dim is None or ff_dim == 0:\n            ff_dim = 4 * hidden_dim  # Default to 4x hidden_dim as in standard transformers\n\n        bsbr_layer = BSBRLayer(\n            hidden_dim=hidden_dim,\n            num_heads=num_heads,\n            chunk_size=self.chunk_size,\n            ff_dim=ff_dim,\n            dropout=gpt_layer.attn.attn_dropout.p,\n            compression_factor=self.compression_factor\n        )\n\n        # Convert attention layer\n        bsbr_layer.attention = self.convert_gpt2_attention_to_bsbr(\n            gpt_layer.attn, hidden_dim, num_heads\n        )\n\n        # Copy layer norms\n        bsbr_layer.layer_norm1.weight.data = gpt_layer.ln_1.weight.data\n        bsbr_layer.layer_norm1.bias.data = gpt_layer.ln_1.bias.data\n        bsbr_layer.layer_norm2.weight.data = gpt_layer.ln_2.weight.data\n        bsbr_layer.layer_norm2.bias.data = gpt_layer.ln_2.bias.data\n\n        # Copy feed-forward network\n        # GPT-2 MLP has: c_fc -&gt; gelu -&gt; c_proj\n        # BSBR FF has: Linear -&gt; GELU -&gt; Dropout -&gt; Linear -&gt; Dropout\n\n        # First linear layer (hidden_dim -&gt; ff_dim)\n        if hasattr(gpt_layer.mlp, 'c_fc'):\n            c_fc_weight = gpt_layer.mlp.c_fc.weight\n            c_fc_bias = gpt_layer.mlp.c_fc.bias if hasattr(gpt_layer.mlp.c_fc, 'bias') else None\n\n            # Check c_fc weight shape and transpose if needed\n            # nn.Linear expects weight of shape (out_features, in_features)\n            # For first layer: out_features = ff_dim, in_features = hidden_dim\n            if c_fc_weight.shape[0] == hidden_dim and c_fc_weight.shape[1] == ff_dim:\n                # Weight is transposed compared to nn.Linear expectation, need to transpose\n                c_fc_weight_fixed = c_fc_weight.t()\n            elif c_fc_weight.shape[0] == ff_dim and c_fc_weight.shape[1] == hidden_dim:\n                # Weight already has the correct shape for nn.Linear\n                c_fc_weight_fixed = c_fc_weight\n            else:\n                # Try to reshape or give a reasonable error\n                # This is a heuristic approach\n                if c_fc_weight.numel() == ff_dim * hidden_dim:\n                    c_fc_weight_fixed = c_fc_weight.reshape(ff_dim, hidden_dim)\n                else:\n                    # Initialize with a random weight if shapes don't match\n                    c_fc_weight_fixed = torch.randn(ff_dim, hidden_dim) * 0.02\n                    print(f\"Warning: Had to initialize feedforward weights randomly due to shape mismatch\")\n\n            bsbr_layer.ff[0].weight.data = c_fc_weight_fixed\n            if c_fc_bias is not None:\n                if c_fc_bias.shape[0] == ff_dim:\n                    bsbr_layer.ff[0].bias.data = c_fc_bias\n                else:\n                    # Initialize with zeros if shape doesn't match\n                    bsbr_layer.ff[0].bias.data = torch.zeros(ff_dim)\n\n        # Second linear layer (ff_dim -&gt; hidden_dim)\n        if hasattr(gpt_layer.mlp, 'c_proj'):\n            c_proj_weight = gpt_layer.mlp.c_proj.weight\n            c_proj_bias = gpt_layer.mlp.c_proj.bias if hasattr(gpt_layer.mlp.c_proj, 'bias') else None\n\n            # Check c_proj weight shape and transpose if needed\n            # nn.Linear expects weight of shape (out_features, in_features)\n            # For second layer: out_features = hidden_dim, in_features = ff_dim\n            if c_proj_weight.shape[0] == ff_dim and c_proj_weight.shape[1] == hidden_dim:\n                # Weight is transposed compared to nn.Linear expectation, need to transpose\n                c_proj_weight_fixed = c_proj_weight.t()\n            elif c_proj_weight.shape[0] == hidden_dim and c_proj_weight.shape[1] == ff_dim:\n                # Weight already has the correct shape for nn.Linear\n                c_proj_weight_fixed = c_proj_weight\n            else:\n                # Try to reshape or give a reasonable error\n                # This is a heuristic approach\n                if c_proj_weight.numel() == hidden_dim * ff_dim:\n                    c_proj_weight_fixed = c_proj_weight.reshape(hidden_dim, ff_dim)\n                else:\n                    # Initialize with a random weight if shapes don't match\n                    c_proj_weight_fixed = torch.randn(hidden_dim, ff_dim) * 0.02\n                    print(f\"Warning: Had to initialize feedforward weights randomly due to shape mismatch\")\n\n            bsbr_layer.ff[3].weight.data = c_proj_weight_fixed\n            if c_proj_bias is not None:\n                if c_proj_bias.shape[0] == hidden_dim:\n                    bsbr_layer.ff[3].bias.data = c_proj_bias\n                else:\n                    # Initialize with zeros if shape doesn't match\n                    bsbr_layer.ff[3].bias.data = torch.zeros(hidden_dim)\n\n        return bsbr_layer\n\n    def convert_gpt2_model_to_bsbr(self, gpt_model: GPT2Model) -&gt; BSBRModel:\n        \"\"\"\n        Convert a complete GPT2 model to a BSBR model.\n\n        Args:\n            gpt_model: The GPT2 model to convert\n\n        Returns:\n            A BSBR model with weights initialized from the GPT2 model\n        \"\"\"\n        config = gpt_model.config\n\n        # Extract configuration with fallbacks\n        hidden_size = getattr(config, 'hidden_size', getattr(config, 'n_embd', None))\n        if hidden_size is None:\n            raise ValueError(\"Could not determine hidden_size from model config\")\n\n        num_hidden_layers = getattr(config, 'num_hidden_layers', getattr(config, 'n_layer', None))\n        if num_hidden_layers is None:\n            raise ValueError(\"Could not determine num_hidden_layers from model config\")\n\n        num_attention_heads = getattr(config, 'num_attention_heads', getattr(config, 'n_head', None))\n        if num_attention_heads is None:\n            raise ValueError(\"Could not determine num_attention_heads from model config\")\n\n        vocab_size = getattr(config, 'vocab_size', None)\n        if vocab_size is None:\n            raise ValueError(\"Could not determine vocab_size from model config\")\n\n        # Check for n_inner or infer from hidden_size\n        ff_dim = getattr(config, 'n_inner', None)\n        if ff_dim is None:\n            ff_dim = getattr(config, 'intermediate_size', 4 * hidden_size)\n\n        dropout = getattr(config, 'resid_pdrop', getattr(config, 'hidden_dropout_prob', 0.1))\n\n        # Create a new BSBR model\n        bsbr_model = BSBRModel(\n            vocab_size=vocab_size,\n            hidden_dim=hidden_size,\n            num_layers=num_hidden_layers,\n            num_heads=num_attention_heads,\n            chunk_size=self.chunk_size,\n            ff_dim=ff_dim,\n            dropout=dropout,\n            compression_factor=self.compression_factor\n        )\n\n        # Copy embedding weights\n        bsbr_model.embedding.weight.data = gpt_model.wte.weight.data\n\n        # Copy positional encoding\n        # Note: GPT uses learned positional embeddings while BSBR uses sinusoidal\n        # We'll initialize with sinusoidal but could adapt to use learned if needed\n\n        # Copy transformer layers\n        for i, gpt_layer in enumerate(gpt_model.h):\n            bsbr_model.layers[i] = self.convert_gpt2_layer_to_bsbr(\n                gpt_layer, \n                hidden_size, \n                num_attention_heads,\n                ff_dim\n            )\n\n        # Copy final layer norm\n        bsbr_model.layer_norm.weight.data = gpt_model.ln_f.weight.data\n        bsbr_model.layer_norm.bias.data = gpt_model.ln_f.bias.data\n\n        return bsbr_model\n\n    def convert_pretrained_model(self, model_name_or_path: str) -&gt; BSBRModel:\n        \"\"\"\n        Convert a pre-trained HuggingFace model to a BSBR model.\n\n        Args:\n            model_name_or_path: Name or path of the pre-trained model\n\n        Returns:\n            A BSBR model with weights initialized from the pre-trained model\n        \"\"\"\n        # Load the pre-trained model\n        original_model = AutoModel.from_pretrained(model_name_or_path)\n\n        # Check if it's a GPT2 model\n        if isinstance(original_model, GPT2Model):\n            return self.convert_gpt2_model_to_bsbr(original_model)\n        else:\n            raise ValueError(f\"Model type {type(original_model)} is not supported yet. Only GPT2 models are currently supported.\")\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.TransformerToBSBRConverter.__init__","title":"<code>__init__(chunk_size=128, compression_factor=None)</code>","text":"<p>Initialize the converter with BSBR-specific parameters.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Size of chunks for BSBR processing</p> <code>128</code> <code>compression_factor</code> <code>Optional[int]</code> <p>Optional factor to compress state vectors</p> <code>None</code> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def __init__(self, chunk_size: int = 128, compression_factor: Optional[int] = None):\n    \"\"\"\n    Initialize the converter with BSBR-specific parameters.\n\n    Args:\n        chunk_size: Size of chunks for BSBR processing\n        compression_factor: Optional factor to compress state vectors\n    \"\"\"\n    self.chunk_size = chunk_size\n    self.compression_factor = compression_factor\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.TransformerToBSBRConverter.convert_gpt2_attention_to_bsbr","title":"<code>convert_gpt2_attention_to_bsbr(gpt_attention, hidden_dim, num_heads)</code>","text":"<p>Convert a GPT2 attention layer to a BSBR attention layer.</p> <p>Parameters:</p> Name Type Description Default <code>gpt_attention</code> <code>GPT2Attention</code> <p>The GPT2 attention layer to convert</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <p>Returns:</p> Type Description <code>BSBRAttention</code> <p>A BSBR attention layer with weights initialized from the GPT2 layer</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_gpt2_attention_to_bsbr(self, \n                                  gpt_attention: GPT2Attention, \n                                  hidden_dim: int,\n                                  num_heads: int) -&gt; BSBRAttention:\n    \"\"\"\n    Convert a GPT2 attention layer to a BSBR attention layer.\n\n    Args:\n        gpt_attention: The GPT2 attention layer to convert\n        hidden_dim: Hidden dimension size\n        num_heads: Number of attention heads\n\n    Returns:\n        A BSBR attention layer with weights initialized from the GPT2 layer\n    \"\"\"\n    bsbr_attention = BSBRAttention(\n        hidden_dim=hidden_dim,\n        num_heads=num_heads,\n        chunk_size=self.chunk_size,\n        dropout=gpt_attention.attn_dropout.p,\n        compression_factor=self.compression_factor\n    )\n\n    # Handle GPT2's Conv1D vs Linear difference\n    # In GPT2, Conv1D has shape (out_features, in_features) and is transposed compared to nn.Linear\n    # c_attn is a single matrix for q, k, v concatenated, shape is (hidden_dim, 3*hidden_dim)\n\n    # For q, k, v projections, we need to handle the transposed nature of Conv1D vs nn.Linear\n    # The Conv1D weight is already transposed compared to nn.Linear, so we need to transpose it back\n    if hasattr(gpt_attention, 'c_attn'):\n        # Split the weights for Q, K, V\n        qkv_weight = gpt_attention.c_attn.weight\n        qkv_bias = gpt_attention.c_attn.bias\n\n        # Check if qkv_weight has the expected shape (hidden_dim, 3*hidden_dim) or (3*hidden_dim, hidden_dim)\n        if qkv_weight.shape[0] == hidden_dim:\n            # Weights have shape (hidden_dim, 3*hidden_dim)\n            # Need to transpose for nn.Linear which expects (out_features, in_features)\n            q_weight = qkv_weight[:, :hidden_dim].t()\n            k_weight = qkv_weight[:, hidden_dim:2*hidden_dim].t()\n            v_weight = qkv_weight[:, 2*hidden_dim:3*hidden_dim].t()\n\n            # Biases don't need transposition\n            q_bias = qkv_bias[:hidden_dim] if qkv_bias is not None else None\n            k_bias = qkv_bias[hidden_dim:2*hidden_dim] if qkv_bias is not None else None\n            v_bias = qkv_bias[2*hidden_dim:3*hidden_dim] if qkv_bias is not None else None\n        else:\n            # Weights have shape (3*hidden_dim, hidden_dim)\n            q_weight = qkv_weight[:hidden_dim, :]\n            k_weight = qkv_weight[hidden_dim:2*hidden_dim, :]\n            v_weight = qkv_weight[2*hidden_dim:3*hidden_dim, :]\n\n            # Biases\n            q_bias = qkv_bias[:hidden_dim] if qkv_bias is not None else None\n            k_bias = qkv_bias[hidden_dim:2*hidden_dim] if qkv_bias is not None else None\n            v_bias = qkv_bias[2*hidden_dim:3*hidden_dim] if qkv_bias is not None else None\n    else:\n        # If c_attn doesn't exist, try to find separate q, k, v projections\n        q_weight = gpt_attention.q_proj.weight if hasattr(gpt_attention, 'q_proj') else None\n        k_weight = gpt_attention.k_proj.weight if hasattr(gpt_attention, 'k_proj') else None\n        v_weight = gpt_attention.v_proj.weight if hasattr(gpt_attention, 'v_proj') else None\n\n        q_bias = gpt_attention.q_proj.bias if hasattr(gpt_attention, 'q_proj') else None\n        k_bias = gpt_attention.k_proj.bias if hasattr(gpt_attention, 'k_proj') else None\n        v_bias = gpt_attention.v_proj.bias if hasattr(gpt_attention, 'v_proj') else None\n\n    # Copy weights to BSBR attention\n    if q_weight is not None:\n        bsbr_attention.q_proj.weight.data = q_weight\n    if k_weight is not None:\n        bsbr_attention.k_proj.weight.data = k_weight\n    if v_weight is not None:\n        bsbr_attention.v_proj.weight.data = v_weight\n\n    # Copy biases\n    if q_bias is not None:\n        bsbr_attention.q_proj.bias.data = q_bias\n    if k_bias is not None:\n        bsbr_attention.k_proj.bias.data = k_bias\n    if v_bias is not None:\n        bsbr_attention.v_proj.bias.data = v_bias\n\n    # Handle output projection\n    if hasattr(gpt_attention, 'c_proj'):\n        if gpt_attention.c_proj.weight.shape[0] == hidden_dim:\n            # Weight is transposed compared to nn.Linear\n            bsbr_attention.out_proj.weight.data = gpt_attention.c_proj.weight.t()\n        else:\n            bsbr_attention.out_proj.weight.data = gpt_attention.c_proj.weight\n\n        if hasattr(gpt_attention.c_proj, 'bias') and gpt_attention.c_proj.bias is not None:\n            bsbr_attention.out_proj.bias.data = gpt_attention.c_proj.bias\n\n    # Initialize meta projections as combinations of existing projections\n    # This is a heuristic approach to provide a reasonable starting point\n    bsbr_attention.meta_r_proj.weight.data = (bsbr_attention.q_proj.weight.data + \n                                            bsbr_attention.k_proj.weight.data) / 2\n    bsbr_attention.meta_h_proj.weight.data = (bsbr_attention.k_proj.weight.data + \n                                            bsbr_attention.v_proj.weight.data) / 2\n\n    if hasattr(bsbr_attention.q_proj, 'bias') and bsbr_attention.q_proj.bias is not None:\n        bsbr_attention.meta_r_proj.bias.data = (bsbr_attention.q_proj.bias.data + \n                                              bsbr_attention.k_proj.bias.data) / 2\n        bsbr_attention.meta_h_proj.bias.data = (bsbr_attention.k_proj.bias.data + \n                                              bsbr_attention.v_proj.bias.data) / 2\n\n    return bsbr_attention\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.TransformerToBSBRConverter.convert_gpt2_layer_to_bsbr","title":"<code>convert_gpt2_layer_to_bsbr(gpt_layer, hidden_dim, num_heads, ff_dim)</code>","text":"<p>Convert a GPT2 layer to a BSBR layer.</p> <p>Parameters:</p> Name Type Description Default <code>gpt_layer</code> <p>The GPT2 layer to convert</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <p>Returns:</p> Type Description <code>BSBRLayer</code> <p>A BSBR layer with weights initialized from the GPT2 layer</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_gpt2_layer_to_bsbr(self, \n                              gpt_layer, \n                              hidden_dim: int,\n                              num_heads: int,\n                              ff_dim: int) -&gt; BSBRLayer:\n    \"\"\"\n    Convert a GPT2 layer to a BSBR layer.\n\n    Args:\n        gpt_layer: The GPT2 layer to convert\n        hidden_dim: Hidden dimension size\n        num_heads: Number of attention heads\n        ff_dim: Feed-forward intermediate dimension\n\n    Returns:\n        A BSBR layer with weights initialized from the GPT2 layer\n    \"\"\"\n    # Ensure ff_dim is not None or zero\n    if ff_dim is None or ff_dim == 0:\n        ff_dim = 4 * hidden_dim  # Default to 4x hidden_dim as in standard transformers\n\n    bsbr_layer = BSBRLayer(\n        hidden_dim=hidden_dim,\n        num_heads=num_heads,\n        chunk_size=self.chunk_size,\n        ff_dim=ff_dim,\n        dropout=gpt_layer.attn.attn_dropout.p,\n        compression_factor=self.compression_factor\n    )\n\n    # Convert attention layer\n    bsbr_layer.attention = self.convert_gpt2_attention_to_bsbr(\n        gpt_layer.attn, hidden_dim, num_heads\n    )\n\n    # Copy layer norms\n    bsbr_layer.layer_norm1.weight.data = gpt_layer.ln_1.weight.data\n    bsbr_layer.layer_norm1.bias.data = gpt_layer.ln_1.bias.data\n    bsbr_layer.layer_norm2.weight.data = gpt_layer.ln_2.weight.data\n    bsbr_layer.layer_norm2.bias.data = gpt_layer.ln_2.bias.data\n\n    # Copy feed-forward network\n    # GPT-2 MLP has: c_fc -&gt; gelu -&gt; c_proj\n    # BSBR FF has: Linear -&gt; GELU -&gt; Dropout -&gt; Linear -&gt; Dropout\n\n    # First linear layer (hidden_dim -&gt; ff_dim)\n    if hasattr(gpt_layer.mlp, 'c_fc'):\n        c_fc_weight = gpt_layer.mlp.c_fc.weight\n        c_fc_bias = gpt_layer.mlp.c_fc.bias if hasattr(gpt_layer.mlp.c_fc, 'bias') else None\n\n        # Check c_fc weight shape and transpose if needed\n        # nn.Linear expects weight of shape (out_features, in_features)\n        # For first layer: out_features = ff_dim, in_features = hidden_dim\n        if c_fc_weight.shape[0] == hidden_dim and c_fc_weight.shape[1] == ff_dim:\n            # Weight is transposed compared to nn.Linear expectation, need to transpose\n            c_fc_weight_fixed = c_fc_weight.t()\n        elif c_fc_weight.shape[0] == ff_dim and c_fc_weight.shape[1] == hidden_dim:\n            # Weight already has the correct shape for nn.Linear\n            c_fc_weight_fixed = c_fc_weight\n        else:\n            # Try to reshape or give a reasonable error\n            # This is a heuristic approach\n            if c_fc_weight.numel() == ff_dim * hidden_dim:\n                c_fc_weight_fixed = c_fc_weight.reshape(ff_dim, hidden_dim)\n            else:\n                # Initialize with a random weight if shapes don't match\n                c_fc_weight_fixed = torch.randn(ff_dim, hidden_dim) * 0.02\n                print(f\"Warning: Had to initialize feedforward weights randomly due to shape mismatch\")\n\n        bsbr_layer.ff[0].weight.data = c_fc_weight_fixed\n        if c_fc_bias is not None:\n            if c_fc_bias.shape[0] == ff_dim:\n                bsbr_layer.ff[0].bias.data = c_fc_bias\n            else:\n                # Initialize with zeros if shape doesn't match\n                bsbr_layer.ff[0].bias.data = torch.zeros(ff_dim)\n\n    # Second linear layer (ff_dim -&gt; hidden_dim)\n    if hasattr(gpt_layer.mlp, 'c_proj'):\n        c_proj_weight = gpt_layer.mlp.c_proj.weight\n        c_proj_bias = gpt_layer.mlp.c_proj.bias if hasattr(gpt_layer.mlp.c_proj, 'bias') else None\n\n        # Check c_proj weight shape and transpose if needed\n        # nn.Linear expects weight of shape (out_features, in_features)\n        # For second layer: out_features = hidden_dim, in_features = ff_dim\n        if c_proj_weight.shape[0] == ff_dim and c_proj_weight.shape[1] == hidden_dim:\n            # Weight is transposed compared to nn.Linear expectation, need to transpose\n            c_proj_weight_fixed = c_proj_weight.t()\n        elif c_proj_weight.shape[0] == hidden_dim and c_proj_weight.shape[1] == ff_dim:\n            # Weight already has the correct shape for nn.Linear\n            c_proj_weight_fixed = c_proj_weight\n        else:\n            # Try to reshape or give a reasonable error\n            # This is a heuristic approach\n            if c_proj_weight.numel() == hidden_dim * ff_dim:\n                c_proj_weight_fixed = c_proj_weight.reshape(hidden_dim, ff_dim)\n            else:\n                # Initialize with a random weight if shapes don't match\n                c_proj_weight_fixed = torch.randn(hidden_dim, ff_dim) * 0.02\n                print(f\"Warning: Had to initialize feedforward weights randomly due to shape mismatch\")\n\n        bsbr_layer.ff[3].weight.data = c_proj_weight_fixed\n        if c_proj_bias is not None:\n            if c_proj_bias.shape[0] == hidden_dim:\n                bsbr_layer.ff[3].bias.data = c_proj_bias\n            else:\n                # Initialize with zeros if shape doesn't match\n                bsbr_layer.ff[3].bias.data = torch.zeros(hidden_dim)\n\n    return bsbr_layer\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.TransformerToBSBRConverter.convert_gpt2_model_to_bsbr","title":"<code>convert_gpt2_model_to_bsbr(gpt_model)</code>","text":"<p>Convert a complete GPT2 model to a BSBR model.</p> <p>Parameters:</p> Name Type Description Default <code>gpt_model</code> <code>GPT2Model</code> <p>The GPT2 model to convert</p> required <p>Returns:</p> Type Description <code>BSBRModel</code> <p>A BSBR model with weights initialized from the GPT2 model</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_gpt2_model_to_bsbr(self, gpt_model: GPT2Model) -&gt; BSBRModel:\n    \"\"\"\n    Convert a complete GPT2 model to a BSBR model.\n\n    Args:\n        gpt_model: The GPT2 model to convert\n\n    Returns:\n        A BSBR model with weights initialized from the GPT2 model\n    \"\"\"\n    config = gpt_model.config\n\n    # Extract configuration with fallbacks\n    hidden_size = getattr(config, 'hidden_size', getattr(config, 'n_embd', None))\n    if hidden_size is None:\n        raise ValueError(\"Could not determine hidden_size from model config\")\n\n    num_hidden_layers = getattr(config, 'num_hidden_layers', getattr(config, 'n_layer', None))\n    if num_hidden_layers is None:\n        raise ValueError(\"Could not determine num_hidden_layers from model config\")\n\n    num_attention_heads = getattr(config, 'num_attention_heads', getattr(config, 'n_head', None))\n    if num_attention_heads is None:\n        raise ValueError(\"Could not determine num_attention_heads from model config\")\n\n    vocab_size = getattr(config, 'vocab_size', None)\n    if vocab_size is None:\n        raise ValueError(\"Could not determine vocab_size from model config\")\n\n    # Check for n_inner or infer from hidden_size\n    ff_dim = getattr(config, 'n_inner', None)\n    if ff_dim is None:\n        ff_dim = getattr(config, 'intermediate_size', 4 * hidden_size)\n\n    dropout = getattr(config, 'resid_pdrop', getattr(config, 'hidden_dropout_prob', 0.1))\n\n    # Create a new BSBR model\n    bsbr_model = BSBRModel(\n        vocab_size=vocab_size,\n        hidden_dim=hidden_size,\n        num_layers=num_hidden_layers,\n        num_heads=num_attention_heads,\n        chunk_size=self.chunk_size,\n        ff_dim=ff_dim,\n        dropout=dropout,\n        compression_factor=self.compression_factor\n    )\n\n    # Copy embedding weights\n    bsbr_model.embedding.weight.data = gpt_model.wte.weight.data\n\n    # Copy positional encoding\n    # Note: GPT uses learned positional embeddings while BSBR uses sinusoidal\n    # We'll initialize with sinusoidal but could adapt to use learned if needed\n\n    # Copy transformer layers\n    for i, gpt_layer in enumerate(gpt_model.h):\n        bsbr_model.layers[i] = self.convert_gpt2_layer_to_bsbr(\n            gpt_layer, \n            hidden_size, \n            num_attention_heads,\n            ff_dim\n        )\n\n    # Copy final layer norm\n    bsbr_model.layer_norm.weight.data = gpt_model.ln_f.weight.data\n    bsbr_model.layer_norm.bias.data = gpt_model.ln_f.bias.data\n\n    return bsbr_model\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.TransformerToBSBRConverter.convert_pretrained_model","title":"<code>convert_pretrained_model(model_name_or_path)</code>","text":"<p>Convert a pre-trained HuggingFace model to a BSBR model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Name or path of the pre-trained model</p> required <p>Returns:</p> Type Description <code>BSBRModel</code> <p>A BSBR model with weights initialized from the pre-trained model</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_pretrained_model(self, model_name_or_path: str) -&gt; BSBRModel:\n    \"\"\"\n    Convert a pre-trained HuggingFace model to a BSBR model.\n\n    Args:\n        model_name_or_path: Name or path of the pre-trained model\n\n    Returns:\n        A BSBR model with weights initialized from the pre-trained model\n    \"\"\"\n    # Load the pre-trained model\n    original_model = AutoModel.from_pretrained(model_name_or_path)\n\n    # Check if it's a GPT2 model\n    if isinstance(original_model, GPT2Model):\n        return self.convert_gpt2_model_to_bsbr(original_model)\n    else:\n        raise ValueError(f\"Model type {type(original_model)} is not supported yet. Only GPT2 models are currently supported.\")\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.convert_to_bsbr","title":"<code>convert_to_bsbr(model_name_or_path, chunk_size=128, compression_factor=None)</code>","text":"<p>Convenience function to convert a pre-trained transformer model to BSBR.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Name or path of the pre-trained model</p> required <code>chunk_size</code> <code>int</code> <p>Size of chunks for BSBR processing</p> <code>128</code> <code>compression_factor</code> <code>Optional[int]</code> <p>Optional factor to compress state vectors</p> <code>None</code> <p>Returns:</p> Type Description <code>BSBRModel</code> <p>A BSBR model with weights initialized from the pre-trained model</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_to_bsbr(\n    model_name_or_path: str,\n    chunk_size: int = 128,\n    compression_factor: Optional[int] = None\n) -&gt; BSBRModel:\n    \"\"\"\n    Convenience function to convert a pre-trained transformer model to BSBR.\n\n    Args:\n        model_name_or_path: Name or path of the pre-trained model\n        chunk_size: Size of chunks for BSBR processing\n        compression_factor: Optional factor to compress state vectors\n\n    Returns:\n        A BSBR model with weights initialized from the pre-trained model\n    \"\"\"\n    converter = TransformerToBSBRConverter(\n        chunk_size=chunk_size,\n        compression_factor=compression_factor\n    )\n    return converter.convert_pretrained_model(model_name_or_path) \n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.gpt2_converter","title":"<code>bsbr_transformers.gpt2_converter</code>","text":""},{"location":"api/bsbr_transformers/#bsbr_transformers.gpt2_converter.TransformerToBSBRConverter","title":"<code>TransformerToBSBRConverter</code>","text":"<p>Utility to convert vanilla transformer models (especially GPT-style models) to Block Sparse Attention with Block Retrieval (BSBR) transformers.</p> <p>This converter allows reusing pre-trained weights while benefiting from the efficiency of BSBR for processing longer sequences.</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>class TransformerToBSBRConverter:\n    \"\"\"\n    Utility to convert vanilla transformer models (especially GPT-style models)\n    to Block Sparse Attention with Block Retrieval (BSBR) transformers.\n\n    This converter allows reusing pre-trained weights while benefiting from the\n    efficiency of BSBR for processing longer sequences.\n    \"\"\"\n\n    def __init__(self, chunk_size: int = 128, compression_factor: Optional[int] = None):\n        \"\"\"\n        Initialize the converter with BSBR-specific parameters.\n\n        Args:\n            chunk_size: Size of chunks for BSBR processing\n            compression_factor: Optional factor to compress state vectors\n        \"\"\"\n        self.chunk_size = chunk_size\n        self.compression_factor = compression_factor\n\n    def convert_gpt2_attention_to_bsbr(self, \n                                      gpt_attention: GPT2Attention, \n                                      hidden_dim: int,\n                                      num_heads: int) -&gt; BSBRAttention:\n        \"\"\"\n        Convert a GPT2 attention layer to a BSBR attention layer.\n\n        Args:\n            gpt_attention: The GPT2 attention layer to convert\n            hidden_dim: Hidden dimension size\n            num_heads: Number of attention heads\n\n        Returns:\n            A BSBR attention layer with weights initialized from the GPT2 layer\n        \"\"\"\n        bsbr_attention = BSBRAttention(\n            hidden_dim=hidden_dim,\n            num_heads=num_heads,\n            chunk_size=self.chunk_size,\n            dropout=gpt_attention.attn_dropout.p,\n            compression_factor=self.compression_factor\n        )\n\n        # Handle GPT2's Conv1D vs Linear difference\n        # In GPT2, Conv1D has shape (out_features, in_features) and is transposed compared to nn.Linear\n        # c_attn is a single matrix for q, k, v concatenated, shape is (hidden_dim, 3*hidden_dim)\n\n        # For q, k, v projections, we need to handle the transposed nature of Conv1D vs nn.Linear\n        # The Conv1D weight is already transposed compared to nn.Linear, so we need to transpose it back\n        if hasattr(gpt_attention, 'c_attn'):\n            # Split the weights for Q, K, V\n            qkv_weight = gpt_attention.c_attn.weight\n            qkv_bias = gpt_attention.c_attn.bias\n\n            # Check if qkv_weight has the expected shape (hidden_dim, 3*hidden_dim) or (3*hidden_dim, hidden_dim)\n            if qkv_weight.shape[0] == hidden_dim:\n                # Weights have shape (hidden_dim, 3*hidden_dim)\n                # Need to transpose for nn.Linear which expects (out_features, in_features)\n                q_weight = qkv_weight[:, :hidden_dim].t()\n                k_weight = qkv_weight[:, hidden_dim:2*hidden_dim].t()\n                v_weight = qkv_weight[:, 2*hidden_dim:3*hidden_dim].t()\n\n                # Biases don't need transposition\n                q_bias = qkv_bias[:hidden_dim] if qkv_bias is not None else None\n                k_bias = qkv_bias[hidden_dim:2*hidden_dim] if qkv_bias is not None else None\n                v_bias = qkv_bias[2*hidden_dim:3*hidden_dim] if qkv_bias is not None else None\n            else:\n                # Weights have shape (3*hidden_dim, hidden_dim)\n                q_weight = qkv_weight[:hidden_dim, :]\n                k_weight = qkv_weight[hidden_dim:2*hidden_dim, :]\n                v_weight = qkv_weight[2*hidden_dim:3*hidden_dim, :]\n\n                # Biases\n                q_bias = qkv_bias[:hidden_dim] if qkv_bias is not None else None\n                k_bias = qkv_bias[hidden_dim:2*hidden_dim] if qkv_bias is not None else None\n                v_bias = qkv_bias[2*hidden_dim:3*hidden_dim] if qkv_bias is not None else None\n        else:\n            # If c_attn doesn't exist, try to find separate q, k, v projections\n            q_weight = gpt_attention.q_proj.weight if hasattr(gpt_attention, 'q_proj') else None\n            k_weight = gpt_attention.k_proj.weight if hasattr(gpt_attention, 'k_proj') else None\n            v_weight = gpt_attention.v_proj.weight if hasattr(gpt_attention, 'v_proj') else None\n\n            q_bias = gpt_attention.q_proj.bias if hasattr(gpt_attention, 'q_proj') else None\n            k_bias = gpt_attention.k_proj.bias if hasattr(gpt_attention, 'k_proj') else None\n            v_bias = gpt_attention.v_proj.bias if hasattr(gpt_attention, 'v_proj') else None\n\n        # Copy weights to BSBR attention\n        if q_weight is not None:\n            bsbr_attention.q_proj.weight.data = q_weight\n        if k_weight is not None:\n            bsbr_attention.k_proj.weight.data = k_weight\n        if v_weight is not None:\n            bsbr_attention.v_proj.weight.data = v_weight\n\n        # Copy biases\n        if q_bias is not None:\n            bsbr_attention.q_proj.bias.data = q_bias\n        if k_bias is not None:\n            bsbr_attention.k_proj.bias.data = k_bias\n        if v_bias is not None:\n            bsbr_attention.v_proj.bias.data = v_bias\n\n        # Handle output projection\n        if hasattr(gpt_attention, 'c_proj'):\n            if gpt_attention.c_proj.weight.shape[0] == hidden_dim:\n                # Weight is transposed compared to nn.Linear\n                bsbr_attention.out_proj.weight.data = gpt_attention.c_proj.weight.t()\n            else:\n                bsbr_attention.out_proj.weight.data = gpt_attention.c_proj.weight\n\n            if hasattr(gpt_attention.c_proj, 'bias') and gpt_attention.c_proj.bias is not None:\n                bsbr_attention.out_proj.bias.data = gpt_attention.c_proj.bias\n\n        # Initialize meta projections as combinations of existing projections\n        # This is a heuristic approach to provide a reasonable starting point\n        bsbr_attention.meta_r_proj.weight.data = (bsbr_attention.q_proj.weight.data + \n                                                bsbr_attention.k_proj.weight.data) / 2\n        bsbr_attention.meta_h_proj.weight.data = (bsbr_attention.k_proj.weight.data + \n                                                bsbr_attention.v_proj.weight.data) / 2\n\n        if hasattr(bsbr_attention.q_proj, 'bias') and bsbr_attention.q_proj.bias is not None:\n            bsbr_attention.meta_r_proj.bias.data = (bsbr_attention.q_proj.bias.data + \n                                                  bsbr_attention.k_proj.bias.data) / 2\n            bsbr_attention.meta_h_proj.bias.data = (bsbr_attention.k_proj.bias.data + \n                                                  bsbr_attention.v_proj.bias.data) / 2\n\n        return bsbr_attention\n\n    def convert_gpt2_layer_to_bsbr(self, \n                                  gpt_layer, \n                                  hidden_dim: int,\n                                  num_heads: int,\n                                  ff_dim: int) -&gt; BSBRLayer:\n        \"\"\"\n        Convert a GPT2 layer to a BSBR layer.\n\n        Args:\n            gpt_layer: The GPT2 layer to convert\n            hidden_dim: Hidden dimension size\n            num_heads: Number of attention heads\n            ff_dim: Feed-forward intermediate dimension\n\n        Returns:\n            A BSBR layer with weights initialized from the GPT2 layer\n        \"\"\"\n        # Ensure ff_dim is not None or zero\n        if ff_dim is None or ff_dim == 0:\n            ff_dim = 4 * hidden_dim  # Default to 4x hidden_dim as in standard transformers\n\n        bsbr_layer = BSBRLayer(\n            hidden_dim=hidden_dim,\n            num_heads=num_heads,\n            chunk_size=self.chunk_size,\n            ff_dim=ff_dim,\n            dropout=gpt_layer.attn.attn_dropout.p,\n            compression_factor=self.compression_factor\n        )\n\n        # Convert attention layer\n        bsbr_layer.attention = self.convert_gpt2_attention_to_bsbr(\n            gpt_layer.attn, hidden_dim, num_heads\n        )\n\n        # Copy layer norms\n        bsbr_layer.layer_norm1.weight.data = gpt_layer.ln_1.weight.data\n        bsbr_layer.layer_norm1.bias.data = gpt_layer.ln_1.bias.data\n        bsbr_layer.layer_norm2.weight.data = gpt_layer.ln_2.weight.data\n        bsbr_layer.layer_norm2.bias.data = gpt_layer.ln_2.bias.data\n\n        # Copy feed-forward network\n        # GPT-2 MLP has: c_fc -&gt; gelu -&gt; c_proj\n        # BSBR FF has: Linear -&gt; GELU -&gt; Dropout -&gt; Linear -&gt; Dropout\n\n        # First linear layer (hidden_dim -&gt; ff_dim)\n        if hasattr(gpt_layer.mlp, 'c_fc'):\n            c_fc_weight = gpt_layer.mlp.c_fc.weight\n            c_fc_bias = gpt_layer.mlp.c_fc.bias if hasattr(gpt_layer.mlp.c_fc, 'bias') else None\n\n            # Check c_fc weight shape and transpose if needed\n            # nn.Linear expects weight of shape (out_features, in_features)\n            # For first layer: out_features = ff_dim, in_features = hidden_dim\n            if c_fc_weight.shape[0] == hidden_dim and c_fc_weight.shape[1] == ff_dim:\n                # Weight is transposed compared to nn.Linear expectation, need to transpose\n                c_fc_weight_fixed = c_fc_weight.t()\n            elif c_fc_weight.shape[0] == ff_dim and c_fc_weight.shape[1] == hidden_dim:\n                # Weight already has the correct shape for nn.Linear\n                c_fc_weight_fixed = c_fc_weight\n            else:\n                # Try to reshape or give a reasonable error\n                # This is a heuristic approach\n                if c_fc_weight.numel() == ff_dim * hidden_dim:\n                    c_fc_weight_fixed = c_fc_weight.reshape(ff_dim, hidden_dim)\n                else:\n                    # Initialize with a random weight if shapes don't match\n                    c_fc_weight_fixed = torch.randn(ff_dim, hidden_dim) * 0.02\n                    print(f\"Warning: Had to initialize feedforward weights randomly due to shape mismatch\")\n\n            bsbr_layer.ff[0].weight.data = c_fc_weight_fixed\n            if c_fc_bias is not None:\n                if c_fc_bias.shape[0] == ff_dim:\n                    bsbr_layer.ff[0].bias.data = c_fc_bias\n                else:\n                    # Initialize with zeros if shape doesn't match\n                    bsbr_layer.ff[0].bias.data = torch.zeros(ff_dim)\n\n        # Second linear layer (ff_dim -&gt; hidden_dim)\n        if hasattr(gpt_layer.mlp, 'c_proj'):\n            c_proj_weight = gpt_layer.mlp.c_proj.weight\n            c_proj_bias = gpt_layer.mlp.c_proj.bias if hasattr(gpt_layer.mlp.c_proj, 'bias') else None\n\n            # Check c_proj weight shape and transpose if needed\n            # nn.Linear expects weight of shape (out_features, in_features)\n            # For second layer: out_features = hidden_dim, in_features = ff_dim\n            if c_proj_weight.shape[0] == ff_dim and c_proj_weight.shape[1] == hidden_dim:\n                # Weight is transposed compared to nn.Linear expectation, need to transpose\n                c_proj_weight_fixed = c_proj_weight.t()\n            elif c_proj_weight.shape[0] == hidden_dim and c_proj_weight.shape[1] == ff_dim:\n                # Weight already has the correct shape for nn.Linear\n                c_proj_weight_fixed = c_proj_weight\n            else:\n                # Try to reshape or give a reasonable error\n                # This is a heuristic approach\n                if c_proj_weight.numel() == hidden_dim * ff_dim:\n                    c_proj_weight_fixed = c_proj_weight.reshape(hidden_dim, ff_dim)\n                else:\n                    # Initialize with a random weight if shapes don't match\n                    c_proj_weight_fixed = torch.randn(hidden_dim, ff_dim) * 0.02\n                    print(f\"Warning: Had to initialize feedforward weights randomly due to shape mismatch\")\n\n            bsbr_layer.ff[3].weight.data = c_proj_weight_fixed\n            if c_proj_bias is not None:\n                if c_proj_bias.shape[0] == hidden_dim:\n                    bsbr_layer.ff[3].bias.data = c_proj_bias\n                else:\n                    # Initialize with zeros if shape doesn't match\n                    bsbr_layer.ff[3].bias.data = torch.zeros(hidden_dim)\n\n        return bsbr_layer\n\n    def convert_gpt2_model_to_bsbr(self, gpt_model: GPT2Model) -&gt; BSBRModel:\n        \"\"\"\n        Convert a complete GPT2 model to a BSBR model.\n\n        Args:\n            gpt_model: The GPT2 model to convert\n\n        Returns:\n            A BSBR model with weights initialized from the GPT2 model\n        \"\"\"\n        config = gpt_model.config\n\n        # Extract configuration with fallbacks\n        hidden_size = getattr(config, 'hidden_size', getattr(config, 'n_embd', None))\n        if hidden_size is None:\n            raise ValueError(\"Could not determine hidden_size from model config\")\n\n        num_hidden_layers = getattr(config, 'num_hidden_layers', getattr(config, 'n_layer', None))\n        if num_hidden_layers is None:\n            raise ValueError(\"Could not determine num_hidden_layers from model config\")\n\n        num_attention_heads = getattr(config, 'num_attention_heads', getattr(config, 'n_head', None))\n        if num_attention_heads is None:\n            raise ValueError(\"Could not determine num_attention_heads from model config\")\n\n        vocab_size = getattr(config, 'vocab_size', None)\n        if vocab_size is None:\n            raise ValueError(\"Could not determine vocab_size from model config\")\n\n        # Check for n_inner or infer from hidden_size\n        ff_dim = getattr(config, 'n_inner', None)\n        if ff_dim is None:\n            ff_dim = getattr(config, 'intermediate_size', 4 * hidden_size)\n\n        dropout = getattr(config, 'resid_pdrop', getattr(config, 'hidden_dropout_prob', 0.1))\n\n        # Create a new BSBR model\n        bsbr_model = BSBRModel(\n            vocab_size=vocab_size,\n            hidden_dim=hidden_size,\n            num_layers=num_hidden_layers,\n            num_heads=num_attention_heads,\n            chunk_size=self.chunk_size,\n            ff_dim=ff_dim,\n            dropout=dropout,\n            compression_factor=self.compression_factor\n        )\n\n        # Copy embedding weights\n        bsbr_model.embedding.weight.data = gpt_model.wte.weight.data\n\n        # Copy positional encoding\n        # Note: GPT uses learned positional embeddings while BSBR uses sinusoidal\n        # We'll initialize with sinusoidal but could adapt to use learned if needed\n\n        # Copy transformer layers\n        for i, gpt_layer in enumerate(gpt_model.h):\n            bsbr_model.layers[i] = self.convert_gpt2_layer_to_bsbr(\n                gpt_layer, \n                hidden_size, \n                num_attention_heads,\n                ff_dim\n            )\n\n        # Copy final layer norm\n        bsbr_model.layer_norm.weight.data = gpt_model.ln_f.weight.data\n        bsbr_model.layer_norm.bias.data = gpt_model.ln_f.bias.data\n\n        return bsbr_model\n\n    def convert_pretrained_model(self, model_name_or_path: str) -&gt; BSBRModel:\n        \"\"\"\n        Convert a pre-trained HuggingFace model to a BSBR model.\n\n        Args:\n            model_name_or_path: Name or path of the pre-trained model\n\n        Returns:\n            A BSBR model with weights initialized from the pre-trained model\n        \"\"\"\n        # Load the pre-trained model\n        original_model = AutoModel.from_pretrained(model_name_or_path)\n\n        # Check if it's a GPT2 model\n        if isinstance(original_model, GPT2Model):\n            return self.convert_gpt2_model_to_bsbr(original_model)\n        else:\n            raise ValueError(f\"Model type {type(original_model)} is not supported yet. Only GPT2 models are currently supported.\")\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.gpt2_converter.TransformerToBSBRConverter.__init__","title":"<code>__init__(chunk_size=128, compression_factor=None)</code>","text":"<p>Initialize the converter with BSBR-specific parameters.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>Size of chunks for BSBR processing</p> <code>128</code> <code>compression_factor</code> <code>Optional[int]</code> <p>Optional factor to compress state vectors</p> <code>None</code> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def __init__(self, chunk_size: int = 128, compression_factor: Optional[int] = None):\n    \"\"\"\n    Initialize the converter with BSBR-specific parameters.\n\n    Args:\n        chunk_size: Size of chunks for BSBR processing\n        compression_factor: Optional factor to compress state vectors\n    \"\"\"\n    self.chunk_size = chunk_size\n    self.compression_factor = compression_factor\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.gpt2_converter.TransformerToBSBRConverter.convert_gpt2_attention_to_bsbr","title":"<code>convert_gpt2_attention_to_bsbr(gpt_attention, hidden_dim, num_heads)</code>","text":"<p>Convert a GPT2 attention layer to a BSBR attention layer.</p> <p>Parameters:</p> Name Type Description Default <code>gpt_attention</code> <code>GPT2Attention</code> <p>The GPT2 attention layer to convert</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <p>Returns:</p> Type Description <code>BSBRAttention</code> <p>A BSBR attention layer with weights initialized from the GPT2 layer</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_gpt2_attention_to_bsbr(self, \n                                  gpt_attention: GPT2Attention, \n                                  hidden_dim: int,\n                                  num_heads: int) -&gt; BSBRAttention:\n    \"\"\"\n    Convert a GPT2 attention layer to a BSBR attention layer.\n\n    Args:\n        gpt_attention: The GPT2 attention layer to convert\n        hidden_dim: Hidden dimension size\n        num_heads: Number of attention heads\n\n    Returns:\n        A BSBR attention layer with weights initialized from the GPT2 layer\n    \"\"\"\n    bsbr_attention = BSBRAttention(\n        hidden_dim=hidden_dim,\n        num_heads=num_heads,\n        chunk_size=self.chunk_size,\n        dropout=gpt_attention.attn_dropout.p,\n        compression_factor=self.compression_factor\n    )\n\n    # Handle GPT2's Conv1D vs Linear difference\n    # In GPT2, Conv1D has shape (out_features, in_features) and is transposed compared to nn.Linear\n    # c_attn is a single matrix for q, k, v concatenated, shape is (hidden_dim, 3*hidden_dim)\n\n    # For q, k, v projections, we need to handle the transposed nature of Conv1D vs nn.Linear\n    # The Conv1D weight is already transposed compared to nn.Linear, so we need to transpose it back\n    if hasattr(gpt_attention, 'c_attn'):\n        # Split the weights for Q, K, V\n        qkv_weight = gpt_attention.c_attn.weight\n        qkv_bias = gpt_attention.c_attn.bias\n\n        # Check if qkv_weight has the expected shape (hidden_dim, 3*hidden_dim) or (3*hidden_dim, hidden_dim)\n        if qkv_weight.shape[0] == hidden_dim:\n            # Weights have shape (hidden_dim, 3*hidden_dim)\n            # Need to transpose for nn.Linear which expects (out_features, in_features)\n            q_weight = qkv_weight[:, :hidden_dim].t()\n            k_weight = qkv_weight[:, hidden_dim:2*hidden_dim].t()\n            v_weight = qkv_weight[:, 2*hidden_dim:3*hidden_dim].t()\n\n            # Biases don't need transposition\n            q_bias = qkv_bias[:hidden_dim] if qkv_bias is not None else None\n            k_bias = qkv_bias[hidden_dim:2*hidden_dim] if qkv_bias is not None else None\n            v_bias = qkv_bias[2*hidden_dim:3*hidden_dim] if qkv_bias is not None else None\n        else:\n            # Weights have shape (3*hidden_dim, hidden_dim)\n            q_weight = qkv_weight[:hidden_dim, :]\n            k_weight = qkv_weight[hidden_dim:2*hidden_dim, :]\n            v_weight = qkv_weight[2*hidden_dim:3*hidden_dim, :]\n\n            # Biases\n            q_bias = qkv_bias[:hidden_dim] if qkv_bias is not None else None\n            k_bias = qkv_bias[hidden_dim:2*hidden_dim] if qkv_bias is not None else None\n            v_bias = qkv_bias[2*hidden_dim:3*hidden_dim] if qkv_bias is not None else None\n    else:\n        # If c_attn doesn't exist, try to find separate q, k, v projections\n        q_weight = gpt_attention.q_proj.weight if hasattr(gpt_attention, 'q_proj') else None\n        k_weight = gpt_attention.k_proj.weight if hasattr(gpt_attention, 'k_proj') else None\n        v_weight = gpt_attention.v_proj.weight if hasattr(gpt_attention, 'v_proj') else None\n\n        q_bias = gpt_attention.q_proj.bias if hasattr(gpt_attention, 'q_proj') else None\n        k_bias = gpt_attention.k_proj.bias if hasattr(gpt_attention, 'k_proj') else None\n        v_bias = gpt_attention.v_proj.bias if hasattr(gpt_attention, 'v_proj') else None\n\n    # Copy weights to BSBR attention\n    if q_weight is not None:\n        bsbr_attention.q_proj.weight.data = q_weight\n    if k_weight is not None:\n        bsbr_attention.k_proj.weight.data = k_weight\n    if v_weight is not None:\n        bsbr_attention.v_proj.weight.data = v_weight\n\n    # Copy biases\n    if q_bias is not None:\n        bsbr_attention.q_proj.bias.data = q_bias\n    if k_bias is not None:\n        bsbr_attention.k_proj.bias.data = k_bias\n    if v_bias is not None:\n        bsbr_attention.v_proj.bias.data = v_bias\n\n    # Handle output projection\n    if hasattr(gpt_attention, 'c_proj'):\n        if gpt_attention.c_proj.weight.shape[0] == hidden_dim:\n            # Weight is transposed compared to nn.Linear\n            bsbr_attention.out_proj.weight.data = gpt_attention.c_proj.weight.t()\n        else:\n            bsbr_attention.out_proj.weight.data = gpt_attention.c_proj.weight\n\n        if hasattr(gpt_attention.c_proj, 'bias') and gpt_attention.c_proj.bias is not None:\n            bsbr_attention.out_proj.bias.data = gpt_attention.c_proj.bias\n\n    # Initialize meta projections as combinations of existing projections\n    # This is a heuristic approach to provide a reasonable starting point\n    bsbr_attention.meta_r_proj.weight.data = (bsbr_attention.q_proj.weight.data + \n                                            bsbr_attention.k_proj.weight.data) / 2\n    bsbr_attention.meta_h_proj.weight.data = (bsbr_attention.k_proj.weight.data + \n                                            bsbr_attention.v_proj.weight.data) / 2\n\n    if hasattr(bsbr_attention.q_proj, 'bias') and bsbr_attention.q_proj.bias is not None:\n        bsbr_attention.meta_r_proj.bias.data = (bsbr_attention.q_proj.bias.data + \n                                              bsbr_attention.k_proj.bias.data) / 2\n        bsbr_attention.meta_h_proj.bias.data = (bsbr_attention.k_proj.bias.data + \n                                              bsbr_attention.v_proj.bias.data) / 2\n\n    return bsbr_attention\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.gpt2_converter.TransformerToBSBRConverter.convert_gpt2_layer_to_bsbr","title":"<code>convert_gpt2_layer_to_bsbr(gpt_layer, hidden_dim, num_heads, ff_dim)</code>","text":"<p>Convert a GPT2 layer to a BSBR layer.</p> <p>Parameters:</p> Name Type Description Default <code>gpt_layer</code> <p>The GPT2 layer to convert</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>ff_dim</code> <code>int</code> <p>Feed-forward intermediate dimension</p> required <p>Returns:</p> Type Description <code>BSBRLayer</code> <p>A BSBR layer with weights initialized from the GPT2 layer</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_gpt2_layer_to_bsbr(self, \n                              gpt_layer, \n                              hidden_dim: int,\n                              num_heads: int,\n                              ff_dim: int) -&gt; BSBRLayer:\n    \"\"\"\n    Convert a GPT2 layer to a BSBR layer.\n\n    Args:\n        gpt_layer: The GPT2 layer to convert\n        hidden_dim: Hidden dimension size\n        num_heads: Number of attention heads\n        ff_dim: Feed-forward intermediate dimension\n\n    Returns:\n        A BSBR layer with weights initialized from the GPT2 layer\n    \"\"\"\n    # Ensure ff_dim is not None or zero\n    if ff_dim is None or ff_dim == 0:\n        ff_dim = 4 * hidden_dim  # Default to 4x hidden_dim as in standard transformers\n\n    bsbr_layer = BSBRLayer(\n        hidden_dim=hidden_dim,\n        num_heads=num_heads,\n        chunk_size=self.chunk_size,\n        ff_dim=ff_dim,\n        dropout=gpt_layer.attn.attn_dropout.p,\n        compression_factor=self.compression_factor\n    )\n\n    # Convert attention layer\n    bsbr_layer.attention = self.convert_gpt2_attention_to_bsbr(\n        gpt_layer.attn, hidden_dim, num_heads\n    )\n\n    # Copy layer norms\n    bsbr_layer.layer_norm1.weight.data = gpt_layer.ln_1.weight.data\n    bsbr_layer.layer_norm1.bias.data = gpt_layer.ln_1.bias.data\n    bsbr_layer.layer_norm2.weight.data = gpt_layer.ln_2.weight.data\n    bsbr_layer.layer_norm2.bias.data = gpt_layer.ln_2.bias.data\n\n    # Copy feed-forward network\n    # GPT-2 MLP has: c_fc -&gt; gelu -&gt; c_proj\n    # BSBR FF has: Linear -&gt; GELU -&gt; Dropout -&gt; Linear -&gt; Dropout\n\n    # First linear layer (hidden_dim -&gt; ff_dim)\n    if hasattr(gpt_layer.mlp, 'c_fc'):\n        c_fc_weight = gpt_layer.mlp.c_fc.weight\n        c_fc_bias = gpt_layer.mlp.c_fc.bias if hasattr(gpt_layer.mlp.c_fc, 'bias') else None\n\n        # Check c_fc weight shape and transpose if needed\n        # nn.Linear expects weight of shape (out_features, in_features)\n        # For first layer: out_features = ff_dim, in_features = hidden_dim\n        if c_fc_weight.shape[0] == hidden_dim and c_fc_weight.shape[1] == ff_dim:\n            # Weight is transposed compared to nn.Linear expectation, need to transpose\n            c_fc_weight_fixed = c_fc_weight.t()\n        elif c_fc_weight.shape[0] == ff_dim and c_fc_weight.shape[1] == hidden_dim:\n            # Weight already has the correct shape for nn.Linear\n            c_fc_weight_fixed = c_fc_weight\n        else:\n            # Try to reshape or give a reasonable error\n            # This is a heuristic approach\n            if c_fc_weight.numel() == ff_dim * hidden_dim:\n                c_fc_weight_fixed = c_fc_weight.reshape(ff_dim, hidden_dim)\n            else:\n                # Initialize with a random weight if shapes don't match\n                c_fc_weight_fixed = torch.randn(ff_dim, hidden_dim) * 0.02\n                print(f\"Warning: Had to initialize feedforward weights randomly due to shape mismatch\")\n\n        bsbr_layer.ff[0].weight.data = c_fc_weight_fixed\n        if c_fc_bias is not None:\n            if c_fc_bias.shape[0] == ff_dim:\n                bsbr_layer.ff[0].bias.data = c_fc_bias\n            else:\n                # Initialize with zeros if shape doesn't match\n                bsbr_layer.ff[0].bias.data = torch.zeros(ff_dim)\n\n    # Second linear layer (ff_dim -&gt; hidden_dim)\n    if hasattr(gpt_layer.mlp, 'c_proj'):\n        c_proj_weight = gpt_layer.mlp.c_proj.weight\n        c_proj_bias = gpt_layer.mlp.c_proj.bias if hasattr(gpt_layer.mlp.c_proj, 'bias') else None\n\n        # Check c_proj weight shape and transpose if needed\n        # nn.Linear expects weight of shape (out_features, in_features)\n        # For second layer: out_features = hidden_dim, in_features = ff_dim\n        if c_proj_weight.shape[0] == ff_dim and c_proj_weight.shape[1] == hidden_dim:\n            # Weight is transposed compared to nn.Linear expectation, need to transpose\n            c_proj_weight_fixed = c_proj_weight.t()\n        elif c_proj_weight.shape[0] == hidden_dim and c_proj_weight.shape[1] == ff_dim:\n            # Weight already has the correct shape for nn.Linear\n            c_proj_weight_fixed = c_proj_weight\n        else:\n            # Try to reshape or give a reasonable error\n            # This is a heuristic approach\n            if c_proj_weight.numel() == hidden_dim * ff_dim:\n                c_proj_weight_fixed = c_proj_weight.reshape(hidden_dim, ff_dim)\n            else:\n                # Initialize with a random weight if shapes don't match\n                c_proj_weight_fixed = torch.randn(hidden_dim, ff_dim) * 0.02\n                print(f\"Warning: Had to initialize feedforward weights randomly due to shape mismatch\")\n\n        bsbr_layer.ff[3].weight.data = c_proj_weight_fixed\n        if c_proj_bias is not None:\n            if c_proj_bias.shape[0] == hidden_dim:\n                bsbr_layer.ff[3].bias.data = c_proj_bias\n            else:\n                # Initialize with zeros if shape doesn't match\n                bsbr_layer.ff[3].bias.data = torch.zeros(hidden_dim)\n\n    return bsbr_layer\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.gpt2_converter.TransformerToBSBRConverter.convert_gpt2_model_to_bsbr","title":"<code>convert_gpt2_model_to_bsbr(gpt_model)</code>","text":"<p>Convert a complete GPT2 model to a BSBR model.</p> <p>Parameters:</p> Name Type Description Default <code>gpt_model</code> <code>GPT2Model</code> <p>The GPT2 model to convert</p> required <p>Returns:</p> Type Description <code>BSBRModel</code> <p>A BSBR model with weights initialized from the GPT2 model</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_gpt2_model_to_bsbr(self, gpt_model: GPT2Model) -&gt; BSBRModel:\n    \"\"\"\n    Convert a complete GPT2 model to a BSBR model.\n\n    Args:\n        gpt_model: The GPT2 model to convert\n\n    Returns:\n        A BSBR model with weights initialized from the GPT2 model\n    \"\"\"\n    config = gpt_model.config\n\n    # Extract configuration with fallbacks\n    hidden_size = getattr(config, 'hidden_size', getattr(config, 'n_embd', None))\n    if hidden_size is None:\n        raise ValueError(\"Could not determine hidden_size from model config\")\n\n    num_hidden_layers = getattr(config, 'num_hidden_layers', getattr(config, 'n_layer', None))\n    if num_hidden_layers is None:\n        raise ValueError(\"Could not determine num_hidden_layers from model config\")\n\n    num_attention_heads = getattr(config, 'num_attention_heads', getattr(config, 'n_head', None))\n    if num_attention_heads is None:\n        raise ValueError(\"Could not determine num_attention_heads from model config\")\n\n    vocab_size = getattr(config, 'vocab_size', None)\n    if vocab_size is None:\n        raise ValueError(\"Could not determine vocab_size from model config\")\n\n    # Check for n_inner or infer from hidden_size\n    ff_dim = getattr(config, 'n_inner', None)\n    if ff_dim is None:\n        ff_dim = getattr(config, 'intermediate_size', 4 * hidden_size)\n\n    dropout = getattr(config, 'resid_pdrop', getattr(config, 'hidden_dropout_prob', 0.1))\n\n    # Create a new BSBR model\n    bsbr_model = BSBRModel(\n        vocab_size=vocab_size,\n        hidden_dim=hidden_size,\n        num_layers=num_hidden_layers,\n        num_heads=num_attention_heads,\n        chunk_size=self.chunk_size,\n        ff_dim=ff_dim,\n        dropout=dropout,\n        compression_factor=self.compression_factor\n    )\n\n    # Copy embedding weights\n    bsbr_model.embedding.weight.data = gpt_model.wte.weight.data\n\n    # Copy positional encoding\n    # Note: GPT uses learned positional embeddings while BSBR uses sinusoidal\n    # We'll initialize with sinusoidal but could adapt to use learned if needed\n\n    # Copy transformer layers\n    for i, gpt_layer in enumerate(gpt_model.h):\n        bsbr_model.layers[i] = self.convert_gpt2_layer_to_bsbr(\n            gpt_layer, \n            hidden_size, \n            num_attention_heads,\n            ff_dim\n        )\n\n    # Copy final layer norm\n    bsbr_model.layer_norm.weight.data = gpt_model.ln_f.weight.data\n    bsbr_model.layer_norm.bias.data = gpt_model.ln_f.bias.data\n\n    return bsbr_model\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.gpt2_converter.TransformerToBSBRConverter.convert_pretrained_model","title":"<code>convert_pretrained_model(model_name_or_path)</code>","text":"<p>Convert a pre-trained HuggingFace model to a BSBR model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Name or path of the pre-trained model</p> required <p>Returns:</p> Type Description <code>BSBRModel</code> <p>A BSBR model with weights initialized from the pre-trained model</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_pretrained_model(self, model_name_or_path: str) -&gt; BSBRModel:\n    \"\"\"\n    Convert a pre-trained HuggingFace model to a BSBR model.\n\n    Args:\n        model_name_or_path: Name or path of the pre-trained model\n\n    Returns:\n        A BSBR model with weights initialized from the pre-trained model\n    \"\"\"\n    # Load the pre-trained model\n    original_model = AutoModel.from_pretrained(model_name_or_path)\n\n    # Check if it's a GPT2 model\n    if isinstance(original_model, GPT2Model):\n        return self.convert_gpt2_model_to_bsbr(original_model)\n    else:\n        raise ValueError(f\"Model type {type(original_model)} is not supported yet. Only GPT2 models are currently supported.\")\n</code></pre>"},{"location":"api/bsbr_transformers/#bsbr_transformers.gpt2_converter.convert_to_bsbr","title":"<code>convert_to_bsbr(model_name_or_path, chunk_size=128, compression_factor=None)</code>","text":"<p>Convenience function to convert a pre-trained transformer model to BSBR.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Name or path of the pre-trained model</p> required <code>chunk_size</code> <code>int</code> <p>Size of chunks for BSBR processing</p> <code>128</code> <code>compression_factor</code> <code>Optional[int]</code> <p>Optional factor to compress state vectors</p> <code>None</code> <p>Returns:</p> Type Description <code>BSBRModel</code> <p>A BSBR model with weights initialized from the pre-trained model</p> Source code in <code>src/bsbr_transformers/gpt2_converter.py</code> <pre><code>def convert_to_bsbr(\n    model_name_or_path: str,\n    chunk_size: int = 128,\n    compression_factor: Optional[int] = None\n) -&gt; BSBRModel:\n    \"\"\"\n    Convenience function to convert a pre-trained transformer model to BSBR.\n\n    Args:\n        model_name_or_path: Name or path of the pre-trained model\n        chunk_size: Size of chunks for BSBR processing\n        compression_factor: Optional factor to compress state vectors\n\n    Returns:\n        A BSBR model with weights initialized from the pre-trained model\n    \"\"\"\n    converter = TransformerToBSBRConverter(\n        chunk_size=chunk_size,\n        compression_factor=compression_factor\n    )\n    return converter.convert_pretrained_model(model_name_or_path) \n</code></pre>"},{"location":"examples/advanced_usage/","title":"Advanced Usage Examples","text":"<p>This guide demonstrates advanced usage patterns and configurations for the BSBR model.</p>"},{"location":"examples/advanced_usage/#custom-attention-configuration","title":"Custom Attention Configuration","text":"<pre><code>from bsbr import BSBRModel, BSBRAttention\n\n# Create custom attention layer\ncustom_attention = BSBRAttention(\n    hidden_dim=512,\n    num_heads=8,\n    chunk_size=128,\n    dropout=0.1,\n    compression_factor=4  # Enable state compression\n)\n\n# Create model with custom attention\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    attention_layer=custom_attention  # Use custom attention\n)\n</code></pre>"},{"location":"examples/advanced_usage/#memory-efficient-training","title":"Memory-Efficient Training","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Enable gradient checkpointing for memory efficiency\nmodel.gradient_checkpointing_enable()\n\n# Use mixed precision training\nscaler = GradScaler()\n\ndef train_with_mixed_precision(model, dataloader, num_epochs):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            with autocast():\n                outputs = model(batch['input_ids'], batch['attention_mask'])\n                loss = criterion(outputs, batch['labels'])\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n</code></pre>"},{"location":"examples/advanced_usage/#custom-chunking-strategy","title":"Custom Chunking Strategy","text":"<pre><code>from bsbr import BSBRModel\nfrom bsbr.utils.chunking import CustomChunkingStrategy\n\n# Create custom chunking strategy\nchunking_strategy = CustomChunkingStrategy(\n    chunk_size=128,\n    overlap=32,  # Overlap between chunks\n    stride=96    # Stride for sliding window\n)\n\n# Create model with custom chunking\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    chunking_strategy=chunking_strategy\n)\n</code></pre>"},{"location":"examples/advanced_usage/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code>import torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom bsbr import BSBRModel\n\ndef setup_ddp():\n    # Initialize distributed training\n    torch.distributed.init_process_group(backend='nccl')\n    local_rank = torch.distributed.get_rank()\n    torch.cuda.set_device(local_rank)\n    return local_rank\n\ndef train_ddp():\n    local_rank = setup_ddp()\n\n    # Create model\n    model = BSBRModel(\n        vocab_size=10000,\n        hidden_dim=512,\n        num_layers=4,\n        num_heads=8,\n        chunk_size=128,\n        ff_dim=2048,\n        dropout=0.1\n    )\n\n    # Wrap model in DDP\n    model = model.to(local_rank)\n    model = DDP(model, device_ids=[local_rank])\n\n    # Create distributed dataloader\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=32,\n        sampler=train_sampler,\n        num_workers=4\n    )\n\n    # Training loop\n    for epoch in range(num_epochs):\n        train_sampler.set_epoch(epoch)\n        for batch in train_loader:\n            # Training step\n            ...\n</code></pre>"},{"location":"examples/advanced_usage/#custom-model-variants","title":"Custom Model Variants","text":"<pre><code>from bsbr import BSBRModel\nfrom bsbr_extras import LinearTransformer, DeltaNet\n\nclass HybridModel(BSBRModel):\n    \"\"\"Hybrid model combining BSBR and Linear attention.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.linear_layer = LinearTransformer(\n            vocab_size=kwargs['vocab_size'],\n            hidden_dim=kwargs['hidden_dim'],\n            num_layers=1,\n            num_heads=kwargs['num_heads'],\n            ff_dim=kwargs['ff_dim'],\n            dropout=kwargs['dropout']\n        )\n\n    def forward(self, input_ids, attention_mask=None):\n        # BSBR processing\n        bsbr_output = super().forward(input_ids, attention_mask)\n\n        # Linear attention processing\n        linear_output = self.linear_layer(input_ids, attention_mask)\n\n        # Combine outputs\n        return (bsbr_output + linear_output) / 2\n\n# Create and use hybrid model\nmodel = HybridModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n</code></pre>"},{"location":"examples/advanced_usage/#performance-optimization","title":"Performance Optimization","text":"<pre><code>from bsbr import BSBRModel\nfrom bsbr.utils.optimization import optimize_for_inference\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Optimize model for inference\noptimized_model = optimize_for_inference(model)\n\n# Use torch.jit for further optimization\nscripted_model = torch.jit.script(optimized_model)\n\n# Benchmark performance\ndef benchmark_model(model, input_ids, num_runs=100):\n    model.eval()\n    with torch.no_grad():\n        # Warm-up\n        for _ in range(10):\n            _ = model(input_ids)\n\n        # Benchmark\n        torch.cuda.synchronize()\n        start_time = time.time()\n        for _ in range(num_runs):\n            _ = model(input_ids)\n        torch.cuda.synchronize()\n        end_time = time.time()\n\n        return (end_time - start_time) / num_runs\n</code></pre>"},{"location":"examples/basic_usage/","title":"Basic Usage Examples","text":"<p>This guide demonstrates basic usage of the BSBR model and its variants.</p>"},{"location":"examples/basic_usage/#basic-bsbr-model","title":"Basic BSBR Model","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Generate sample input\nbatch_size = 2\nseq_length = 256\ninput_ids = torch.randint(0, 10000, (batch_size, seq_length))\nattention_mask = torch.ones(batch_size, seq_length)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#using-different-model-variants","title":"Using Different Model Variants","text":""},{"location":"examples/basic_usage/#linear-transformer","title":"Linear Transformer","text":"<pre><code>from bsbr_extras import LinearTransformer\n\n# Create linear transformer\nmodel = LinearTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#deltanet","title":"DeltaNet","text":"<pre><code>from bsbr_extras import DeltaNet\n\n# Create DeltaNet\nmodel = DeltaNet(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#sliding-window-transformer","title":"Sliding Window Transformer","text":"<pre><code>from bsbr_extras import SlidingWindowTransformer\n\n# Create sliding window transformer\nmodel = SlidingWindowTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    window_size=64,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"examples/basic_usage/#training-example","title":"Training Example","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom bsbr import BSBRModel\n\n# Create model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1\n)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\ndef train(model, dataloader, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        for batch in dataloader:\n            input_ids = batch['input_ids']\n            attention_mask = batch['attention_mask']\n            labels = batch['labels']\n\n            # Forward pass\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n</code></pre>"},{"location":"examples/basic_usage/#evaluation-example","title":"Evaluation Example","text":"<pre><code>from bsbr.evals import compare_models, analyze_results\n\n# Compare different models\nresults = compare_models(\n    seq_lengths=[64, 128, 256, 512, 1024],\n    models=['BSBR', 'Linear', 'DeltaNet', 'SlidingWindow']\n)\n\n# Analyze results\nanalysis = analyze_results(results)\nprint(analysis)\n</code></pre>"},{"location":"examples/research_examples/","title":"Research Examples","text":"<p>This guide demonstrates how to use BSBR for research and experimentation.</p>"},{"location":"examples/research_examples/#model-comparison","title":"Model Comparison","text":"<pre><code>from bsbr.evals import compare_models, analyze_results\nimport matplotlib.pyplot as plt\n\n# Compare different models across various sequence lengths\nresults = compare_models(\n    seq_lengths=[64, 128, 256, 512, 1024],\n    models=['BSBR', 'Linear', 'DeltaNet', 'SlidingWindow', 'Standard'],\n    metrics=['inference_time', 'memory_usage', 'accuracy']\n)\n\n# Analyze results\nanalysis = analyze_results(results)\n\n# Plot results\nplt.figure(figsize=(10, 6))\nplt.plot(results['seq_lengths'], results['BSBR']['inference_time'], label='BSBR')\nplt.plot(results['seq_lengths'], results['Standard']['inference_time'], label='Standard')\nplt.xlabel('Sequence Length')\nplt.ylabel('Inference Time (s)')\nplt.title('Inference Time vs Sequence Length')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"examples/research_examples/#memory-analysis","title":"Memory Analysis","text":"<pre><code>from bsbr.utils.memory import analyze_memory_usage\nimport torch\n\ndef profile_memory_usage(model, input_ids):\n    \"\"\"Profile memory usage during forward pass.\"\"\"\n    torch.cuda.reset_peak_memory_stats()\n\n    # Forward pass\n    outputs = model(input_ids)\n\n    # Get memory stats\n    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n    current_memory = torch.cuda.memory_allocated() / 1024**2   # MB\n\n    return {\n        'peak_memory': peak_memory,\n        'current_memory': current_memory\n    }\n\n# Analyze memory usage across different chunk sizes\nchunk_sizes = [32, 64, 128, 256]\nmemory_results = {}\n\nfor chunk_size in chunk_sizes:\n    model = BSBRModel(\n        vocab_size=10000,\n        hidden_dim=512,\n        num_layers=4,\n        num_heads=8,\n        chunk_size=chunk_size,\n        ff_dim=2048,\n        dropout=0.1\n    )\n\n    input_ids = torch.randint(0, 10000, (1, 1024))\n    memory_results[chunk_size] = profile_memory_usage(model, input_ids)\n</code></pre>"},{"location":"examples/research_examples/#attention-visualization","title":"Attention Visualization","text":"<pre><code>from bsbr.utils.visualization import visualize_attention\n\ndef analyze_attention_patterns(model, input_ids):\n    \"\"\"Analyze attention patterns in the model.\"\"\"\n    # Get attention weights\n    attention_weights = model.get_attention_weights(input_ids)\n\n    # Visualize attention patterns\n    plt.figure(figsize=(12, 8))\n    visualize_attention(attention_weights)\n    plt.title('Attention Patterns')\n    plt.show()\n\n    # Analyze sparsity\n    sparsity = (attention_weights == 0).float().mean()\n    print(f\"Attention sparsity: {sparsity:.2%}\")\n\n# Compare attention patterns across models\nmodels = {\n    'BSBR': BSBRModel(...),\n    'Linear': LinearTransformer(...),\n    'Standard': StandardTransformer(...)\n}\n\nfor name, model in models.items():\n    print(f\"\\nAnalyzing {name} attention patterns:\")\n    analyze_attention_patterns(model, input_ids)\n</code></pre>"},{"location":"examples/research_examples/#scaling-analysis","title":"Scaling Analysis","text":"<pre><code>from bsbr.evals import scaling_analysis\n\ndef analyze_scaling_behavior():\n    \"\"\"Analyze how different models scale with sequence length.\"\"\"\n    seq_lengths = [64, 128, 256, 512, 1024, 2048]\n    models = ['BSBR', 'Linear', 'Standard']\n\n    results = scaling_analysis(\n        seq_lengths=seq_lengths,\n        models=models,\n        metrics=['time', 'memory', 'flops']\n    )\n\n    # Plot scaling curves\n    plt.figure(figsize=(12, 4))\n\n    # Time scaling\n    plt.subplot(131)\n    for model in models:\n        plt.plot(seq_lengths, results[model]['time'], label=model)\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Time (s)')\n    plt.title('Time Scaling')\n    plt.legend()\n\n    # Memory scaling\n    plt.subplot(132)\n    for model in models:\n        plt.plot(seq_lengths, results[model]['memory'], label=model)\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Memory (GB)')\n    plt.title('Memory Scaling')\n    plt.legend()\n\n    # FLOPs scaling\n    plt.subplot(133)\n    for model in models:\n        plt.plot(seq_lengths, results[model]['flops'], label=model)\n    plt.xlabel('Sequence Length')\n    plt.ylabel('FLOPs')\n    plt.title('FLOPs Scaling')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"examples/research_examples/#custom-research-experiments","title":"Custom Research Experiments","text":"<pre><code>from bsbr.utils.research import ExperimentRunner\n\nclass CustomExperiment(ExperimentRunner):\n    \"\"\"Custom research experiment.\"\"\"\n\n    def setup(self):\n        \"\"\"Setup experiment parameters.\"\"\"\n        self.models = {\n            'BSBR': BSBRModel(...),\n            'Linear': LinearTransformer(...),\n            'Standard': StandardTransformer(...)\n        }\n        self.seq_lengths = [64, 128, 256, 512, 1024]\n        self.metrics = ['time', 'memory', 'accuracy']\n\n    def run_experiment(self, model, seq_length):\n        \"\"\"Run single experiment.\"\"\"\n        # Generate input\n        input_ids = torch.randint(0, 10000, (1, seq_length))\n\n        # Measure metrics\n        start_time = time.time()\n        outputs = model(input_ids)\n        inference_time = time.time() - start_time\n\n        memory_usage = torch.cuda.max_memory_allocated() / 1024**2\n\n        # Calculate accuracy (example)\n        accuracy = self.calculate_accuracy(outputs)\n\n        return {\n            'time': inference_time,\n            'memory': memory_usage,\n            'accuracy': accuracy\n        }\n\n    def analyze_results(self, results):\n        \"\"\"Analyze experiment results.\"\"\"\n        # Custom analysis code\n        pass\n\n# Run experiment\nexperiment = CustomExperiment()\nresults = experiment.run()\nexperiment.analyze_results(results)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>BSBR requires Python 3.12 or higher and PyTorch 2.6.0 or higher. The package is designed to work with modern deep learning frameworks and tools.</p>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>The simplest way to install BSBR is using pip:</p> <pre><code>pip install bsbr\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development or to use the latest features, you can install directly from GitHub:</p> <pre><code># Clone the repository\ngit clone https://github.com/JacobFV/bsbr.git\ncd bsbr\n\n# Install in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>BSBR provides several optional dependency groups that you can install:</p> <pre><code># Install with all extras (evaluation tools, visualization, etc.)\npip install \"bsbr[extras]\"\n\n# Install with documentation tools\npip install \"bsbr[docs]\"\n\n# Install with all optional dependencies\npip install \"bsbr[all]\"\n</code></pre>"},{"location":"getting-started/installation/#available-extras","title":"Available Extras","text":"<ul> <li><code>extras</code>: Evaluation tools, visualization utilities, and research components</li> <li><code>docs</code>: Documentation building tools and dependencies</li> <li><code>all</code>: All optional dependencies</li> </ul>"},{"location":"getting-started/installation/#gpu-support","title":"GPU Support","text":"<p>BSBR automatically uses GPU acceleration when available. To ensure GPU support:</p> <ol> <li> <p>Install CUDA-enabled PyTorch: <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p> </li> <li> <p>Verify GPU availability: <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Current device: {torch.cuda.get_device_name(0)}\")\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li>Import Errors</li> <li>Ensure you're using Python 3.12 or higher</li> <li>Check that PyTorch is installed correctly</li> <li> <p>Verify your Python environment is activated</p> </li> <li> <p>CUDA Errors</p> </li> <li>Confirm CUDA toolkit is installed</li> <li>Verify PyTorch CUDA version matches your system</li> <li> <p>Check GPU drivers are up to date</p> </li> <li> <p>Memory Issues</p> </li> <li>Adjust batch size or sequence length</li> <li>Enable gradient checkpointing</li> <li>Use smaller model configurations</li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the GitHub Issues</li> <li>Join our Discord Community</li> <li>Create a new issue with:</li> <li>Python version</li> <li>PyTorch version</li> <li>Error message</li> <li>System information</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After installation, you can:</p> <ol> <li>Try the Quick Start Guide</li> <li>Explore the Examples</li> <li>Read the User Guide </li> </ol>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you get started with BSBR quickly. We'll cover basic usage, model configuration, and common patterns.</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#creating-a-bsbr-model","title":"Creating a BSBR Model","text":"<pre><code>import torch\nfrom bsbr import BSBRModel\n\n# Create a model with default settings\nmodel = BSBRModel(\n    vocab_size=10000,      # Size of your vocabulary\n    hidden_dim=512,        # Hidden dimension of the model\n    num_layers=4,          # Number of transformer layers\n    num_heads=8,           # Number of attention heads\n    chunk_size=128,        # Size of attention chunks\n    ff_dim=2048,           # Feed-forward network dimension\n    dropout=0.1,           # Dropout rate\n    compression_factor=4   # Optional compression factor\n)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n</code></pre>"},{"location":"getting-started/quickstart/#processing-input","title":"Processing Input","text":"<pre><code># Create sample input\nbatch_size = 2\nseq_length = 256\ninput_ids = torch.randint(0, 10000, (batch_size, seq_length))\nattention_mask = torch.ones(batch_size, seq_length)\n\n# Move inputs to the same device as the model\ninput_ids = input_ids.to(device)\nattention_mask = attention_mask.to(device)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n</code></pre>"},{"location":"getting-started/quickstart/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/quickstart/#customizing-attention","title":"Customizing Attention","text":"<pre><code>from bsbr import BSBRModel, BSBRAttention\n\n# Create a custom attention layer\nattention = BSBRAttention(\n    hidden_dim=512,\n    num_heads=8,\n    chunk_size=128,\n    compression_factor=4,\n    dropout=0.1\n)\n\n# Use it in a model\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    chunk_size=128,\n    ff_dim=2048,\n    dropout=0.1,\n    attention_layer=attention  # Use custom attention\n)\n</code></pre>"},{"location":"getting-started/quickstart/#using-different-models","title":"Using Different Models","text":"<p>BSBR provides several attention variants for comparison:</p> <pre><code>from bsbr_extras import (\n    LinearTransformer,\n    DeltaNet,\n    SlidingWindowTransformer,\n    HopfieldNetwork,\n    GAU\n)\n\n# Linear Transformer\nlinear_model = LinearTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8\n)\n\n# DeltaNet\ndeltanet_model = DeltaNet(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8\n)\n\n# Sliding Window Transformer\nwindow_model = SlidingWindowTransformer(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8,\n    window_size=128\n)\n</code></pre>"},{"location":"getting-started/quickstart/#training-example","title":"Training Example","text":"<p>Here's a basic training loop:</p> <pre><code>import torch.nn as nn\nfrom torch.optim import Adam\n\n# Create model and move to device\nmodel = BSBRModel(\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=8\n).to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=0.001)\n\n# Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"getting-started/quickstart/#evaluation","title":"Evaluation","text":"<p>BSBR provides tools for evaluating different models:</p> <pre><code>from evals.compare_models import compare_models\n\n# Compare models across different sequence lengths\nresults = compare_models(\n    models=[\"BSBR\", \"Linear\", \"Hopfield\", \"GAU\"],\n    seq_lengths=[64, 128, 256, 512, 1024]\n)\n\n# Analyze results\nfrom evals.analyze_results import analyze_results\nanalysis = analyze_results(results)\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ol> <li>Explore the User Guide for detailed explanations</li> <li>Check out Examples for more use cases</li> <li>Read the API Reference for complete documentation </li> </ol>"},{"location":"research/","title":"Research Documentation","text":"<p>This section contains research documents, experimental results, and technical analyses related to the BSBR (Block Sparse with Block Retrieval) architecture.</p>"},{"location":"research/#available-research-documents","title":"Available Research Documents","text":"<ul> <li>Background on BSBR Architecture - Theoretical foundations and design principles of the BSBR architecture</li> <li>Benchmarks - Performance benchmarks of BSBR compared to other attention mechanisms</li> <li>Experiments - Experimental results from various tests and configurations</li> <li>BSBR Conversion Research - Research on converting pre-trained models to BSBR architecture</li> <li>BSBR Conversion Evaluation - Comprehensive evaluation of converted BSBR models</li> </ul>"},{"location":"research/#bsbr-conversion-evaluation","title":"BSBR Conversion Evaluation","text":"<p>Our most recent research has focused on evaluating the performance and behavior of models converted from standard transformers to BSBR architecture. Key findings include:</p> <ul> <li>Performance: At moderate sequence lengths (\u22641024 tokens), BSBR doesn't yet show performance advantages on CPU, but theoretical advantages are expected at longer sequences</li> <li>Scaling: Original transformer scales as O(n^0.34) vs. BSBR as O(n^0.55) in our tests, contrary to expectations</li> <li>Output Similarity: Significant divergence in output behavior, with negative cosine similarity and 0% agreement in next-token predictions</li> <li>Use Cases: BSBR conversion is most suitable for very long context processing where approximate outputs are acceptable</li> </ul> <p>Read the full evaluation \u2192</p>"},{"location":"research/#overview-of-research-focus-areas","title":"Overview of Research Focus Areas","text":"<ol> <li>Architectural Innovations</li> <li>Block-sparse attention patterns</li> <li>Efficient retrieval mechanisms</li> <li> <p>Computational complexity improvements</p> </li> <li> <p>Conversion of Pre-trained Models</p> </li> <li>Weight transfer methodologies</li> <li>Equivalence preservation</li> <li> <p>Fine-tuning requirements</p> </li> <li> <p>Performance Analysis</p> </li> <li>Speed benchmarks</li> <li>Memory efficiency</li> <li> <p>Scaling behavior</p> </li> <li> <p>Output and Behavior Analysis</p> </li> <li>Output distribution comparison</li> <li>Attention pattern visualization</li> <li>Next-token prediction agreement </li> </ol>"},{"location":"research/background/","title":"Research Background","text":""},{"location":"research/background/#introduction","title":"Introduction","text":"<p>Block Sparse Attention with Block Retrieval (BSBR) is a novel attention mechanism designed to efficiently process long sequences in transformer models. This document provides the theoretical background and motivation behind the approach.</p>"},{"location":"research/background/#problem-statement","title":"Problem Statement","text":"<p>Traditional transformer models face several challenges when processing long sequences:</p> <ol> <li>Quadratic Complexity: Standard attention mechanisms have O(n\u00b2) complexity in sequence length</li> <li>Memory Usage: Attention matrices grow quadratically with sequence length</li> <li>Information Flow: Long-range dependencies may be difficult to capture</li> <li>Computational Efficiency: Processing long sequences becomes computationally expensive</li> </ol>"},{"location":"research/background/#related-work","title":"Related Work","text":""},{"location":"research/background/#efficient-attention-mechanisms","title":"Efficient Attention Mechanisms","text":"<ol> <li>Linear Attention</li> <li>Reformulates attention to achieve O(n) complexity</li> <li>Uses associative property of matrix multiplication</li> <li> <p>May sacrifice expressiveness for efficiency</p> </li> <li> <p>Sparse Attention</p> </li> <li>Reduces computation by sparsifying attention patterns</li> <li>Various sparsity patterns (sliding window, strided, etc.)</li> <li> <p>Trade-off between sparsity and model capacity</p> </li> <li> <p>Sliding Window Attention</p> </li> <li>Restricts attention to local context</li> <li>O(n\u00b7w) complexity where w is window size</li> <li>May miss long-range dependencies</li> </ol>"},{"location":"research/background/#memory-efficient-approaches","title":"Memory-Efficient Approaches","text":"<ol> <li>Gradient Checkpointing</li> <li>Trades computation for memory</li> <li>Recomputes intermediate activations during backward pass</li> <li> <p>Increases training time</p> </li> <li> <p>State Compression</p> </li> <li>Compresses intermediate states</li> <li>Reduces memory usage at cost of information loss</li> <li>Various compression techniques</li> </ol>"},{"location":"research/background/#bsbr-approach","title":"BSBR Approach","text":""},{"location":"research/background/#core-idea","title":"Core Idea","text":"<p>BSBR combines two key components:</p> <ol> <li>Within-Chunk Attention</li> <li>Standard attention within fixed-size chunks</li> <li>Maintains local context processing</li> <li> <p>O(c\u00b2) complexity where c is chunk size</p> </li> <li> <p>Block Retrieval</p> </li> <li>Efficient retrieval between chunks</li> <li>Uses meta-attention for chunk-level interaction</li> <li>O(n) complexity overall</li> </ol>"},{"location":"research/background/#mathematical-formulation","title":"Mathematical Formulation","text":"<p>The attention computation can be expressed as:</p> <pre><code>Attention(Q, K, V) = softmax(QK^T)V\n</code></pre> <p>BSBR decomposes this into:</p> <ol> <li> <p>Within-chunk attention: <pre><code>A_in = softmax(Q_in K_in^T)V_in\n</code></pre></p> </li> <li> <p>Between-chunk attention: <pre><code>A_out = Q_out \u2299 softmax(RH^T)F\n</code></pre></p> </li> </ol> <p>Where: - Q_in, K_in, V_in: Query, Key, Value matrices within chunks - Q_out: Query matrix for between-chunk attention - R, H: Meta queries and keys for chunk-level attention - F: State vectors (flattened K^T\u00b7V for each chunk) - \u2299: Element-wise multiplication</p>"},{"location":"research/background/#advantages","title":"Advantages","text":"<ol> <li>Efficiency</li> <li>Linear complexity in sequence length</li> <li>Memory usage scales linearly</li> <li> <p>Parallel processing within chunks</p> </li> <li> <p>Expressiveness</p> </li> <li>Maintains local context processing</li> <li>Captures long-range dependencies through block retrieval</li> <li> <p>Flexible chunk size selection</p> </li> <li> <p>Memory Management</p> </li> <li>Natural chunking reduces peak memory usage</li> <li>Optional state compression</li> <li>Efficient gradient computation</li> </ol>"},{"location":"research/background/#implementation-details","title":"Implementation Details","text":""},{"location":"research/background/#chunking-strategy","title":"Chunking Strategy","text":"<ol> <li>Fixed-Size Chunks</li> <li>Uniform chunk size</li> <li>Simple implementation</li> <li> <p>Predictable memory usage</p> </li> <li> <p>Overlapping Chunks</p> </li> <li>Overlap between chunks</li> <li>Better context preservation</li> <li> <p>Increased computation</p> </li> <li> <p>Adaptive Chunking</p> </li> <li>Dynamic chunk sizes</li> <li>Content-aware splitting</li> <li>More complex implementation</li> </ol>"},{"location":"research/background/#block-retrieval","title":"Block Retrieval","text":"<ol> <li>Meta-Attention</li> <li>Chunk-level attention mechanism</li> <li>Efficient state compression</li> <li> <p>Flexible retrieval patterns</p> </li> <li> <p>State Compression</p> </li> <li>Optional compression factor</li> <li>Memory-performance trade-off</li> <li> <p>Various compression methods</p> </li> <li> <p>Caching</p> </li> <li>Cache chunk states</li> <li>Reuse for repeated queries</li> <li>Memory overhead</li> </ol>"},{"location":"research/background/#experimental-results","title":"Experimental Results","text":""},{"location":"research/background/#performance-metrics","title":"Performance Metrics","text":"<ol> <li>Computation Time</li> <li>Linear scaling with sequence length</li> <li>Competitive with other efficient methods</li> <li> <p>Better for long sequences</p> </li> <li> <p>Memory Usage</p> </li> <li>Linear memory scaling</li> <li>Lower peak memory</li> <li> <p>Efficient gradient computation</p> </li> <li> <p>Model Quality</p> </li> <li>Comparable to standard attention</li> <li>Better long-range modeling</li> <li>Task-specific advantages</li> </ol>"},{"location":"research/background/#comparison-with-baselines","title":"Comparison with Baselines","text":"<ol> <li>Standard Transformer</li> <li>Better scaling</li> <li>Lower memory usage</li> <li> <p>Similar accuracy</p> </li> <li> <p>Linear Transformer</p> </li> <li>Better expressiveness</li> <li>More stable training</li> <li> <p>Similar efficiency</p> </li> <li> <p>Sliding Window</p> </li> <li>Better long-range modeling</li> <li>More flexible attention</li> <li>Similar locality</li> </ol>"},{"location":"research/background/#future-directions","title":"Future Directions","text":"<ol> <li>Architecture Improvements</li> <li>Adaptive chunking</li> <li>Dynamic compression</li> <li> <p>Hybrid approaches</p> </li> <li> <p>Applications</p> </li> <li>Long document processing</li> <li>Multi-modal tasks</li> <li> <p>Real-time inference</p> </li> <li> <p>Optimization</p> </li> <li>Hardware acceleration</li> <li>Distributed training</li> <li>Quantization </li> </ol>"},{"location":"research/benchmarks/","title":"Research Benchmarks","text":"<p>This document provides detailed benchmark results comparing BSBR with other transformer architectures based on the experiments run in the <code>research/architecture_comparisons/</code> directory.</p>"},{"location":"research/benchmarks/#benchmark-setup","title":"Benchmark Setup","text":""},{"location":"research/benchmarks/#environment","title":"Environment","text":"<ul> <li>Hardware: CPU (Intel Core i9)</li> <li>Software: Python 3.12, PyTorch, Transformers, etc. (see <code>requirements.txt</code>)</li> <li>BSBR Version: 0.1.2</li> </ul>"},{"location":"research/benchmarks/#models-compared","title":"Models Compared","text":"<p>Models were configured with comparable hyperparameters (hidden dim: 256, heads: 4, layers: 2 where applicable) for evaluation.</p> <ol> <li>BSBR (Block Sparse Attention with Block Retrieval)</li> <li>Standard Transformer</li> <li>Linear Transformer</li> <li>DeltaNet</li> <li>Sliding Window Transformer</li> <li>Hopfield Network</li> <li>GAU (Gated Attention Unit)</li> </ol>"},{"location":"research/benchmarks/#parameter-counts","title":"Parameter Counts","text":"Model Parameters (Millions) Relative to Base (Standard) BSBR 6.0M 1.66x Standard 3.6M 1.0x Linear 3.6M 1.0x DeltaNet 3.6M 1.0x SlidingWindow 3.6M 1.0x Hopfield 3.6M 1.0x GAU 4.4M 1.22x <p>Data source: <code>research/architecture_comparisons/results/comparison_results.json</code></p>"},{"location":"research/benchmarks/#performance-benchmarks-cpu","title":"Performance Benchmarks (CPU)","text":"<p>Results are based on runs with sequence lengths [64, 128, 256, 512, 1024]. Data source: <code>research/architecture_comparisons/results/comparison_results.json</code></p>"},{"location":"research/benchmarks/#inference-time-seconds","title":"Inference Time (seconds)","text":"Model n=64 n=128 n=256 n=512 n=1024 BSBR 0.462 0.560 0.753 1.570 3.092 Linear 1.570 2.742 4.896 8.879 17.322 DeltaNet 8.085 13.31 23.71 46.166 92.276 Standard 0.254 0.334 0.453 0.908 2.538 SlidingWindow 0.514 0.748 1.289 2.442 5.568 Hopfield 0.255 0.365 0.478 0.937 2.568 GAU 0.488 0.880 1.950 5.381 17.649"},{"location":"research/benchmarks/#peak-memory-usage-mb","title":"Peak Memory Usage (MB)","text":"Model n=64 n=128 n=256 n=512 n=1024 BSBR 22.826 22.826 22.827 22.829 22.833 Linear 13.790 13.790 13.791 13.793 13.797 DeltaNet 13.790 13.790 13.791 13.793 13.797 Standard 13.790 13.790 13.791 13.793 13.797 SlidingWindow 13.790 13.790 13.791 13.793 13.797 Hopfield 13.790 13.790 13.791 13.793 13.797 GAU 16.799 16.800 16.801 16.803 16.807"},{"location":"research/benchmarks/#complexity-analysis","title":"Complexity Analysis","text":"<p>Based on fitting power-law curves to the inference time data.</p> Model Empirical Complexity R-squared Time at n=1024 (seconds) Memory at n=1024 (MB) BSBR O(n^0.70) \u2248 O(n) 0.9380 3.0916 22.83 Standard O(n^0.81) \u2248 O(n) 0.9212 2.5382 13.80 Linear O(n^0.86) \u2248 O(n) 0.9988 17.3223 13.80 DeltaNet O(n^0.88) \u2248 O(n) 0.9956 92.2763 13.80 SlidingWindow O(n^0.86) \u2248 O(n) 0.9804 5.5680 13.80 Hopfield O(n^0.80) \u2248 O(n) 0.9308 2.5681 13.80 GAU O(n^1.30) \u2248 O(n log n) 0.9826 17.6486 16.81 <p>Note: Empirical complexity measured on CPU for n &lt;= 1024 may differ from theoretical asymptotic behavior, especially for Standard attention.</p>"},{"location":"research/benchmarks/#visualizations","title":"Visualizations","text":"<p>Note: Paths are relative to the <code>docs/</code> directory.</p> <p>Complexity &amp; Scaling:</p> <p> </p> <p>Performance Comparison:</p> <p> </p> <p>Sections below contain placeholder data and are for illustrative purposes only.</p>"},{"location":"research/benchmarks/#training-benchmarks","title":"Training Benchmarks","text":""},{"location":"research/benchmarks/#convergence-speed","title":"Convergence Speed","text":"<pre><code>convergence_results = {\n    'BSBR': {\n        'epochs_to_converge': 50,\n        'final_loss': 0.15,\n        'validation_accuracy': 0.92\n    },\n    'Standard': {\n        'epochs_to_converge': 45,\n        'final_loss': 0.18,\n        'validation_accuracy': 0.89\n    },\n    'Linear': {\n        'epochs_to_converge': 55,\n        'final_loss': 0.20,\n        'validation_accuracy': 0.87\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#training-memory","title":"Training Memory","text":"<pre><code>training_memory = {\n    'BSBR': {\n        'peak_memory': 2.5,  # GB\n        'gradient_memory': 1.4,  # GB\n        'activation_memory': 0.8  # GB\n    },\n    'Standard': {\n        'peak_memory': 16.0,  # GB\n        'gradient_memory': 12.8,  # GB\n        'activation_memory': 8.0  # GB\n    },\n    'Linear': {\n        'peak_memory': 2.5,  # GB\n        'gradient_memory': 1.4,  # GB\n        'activation_memory': 0.8  # GB\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#task-specific-benchmarks","title":"Task-Specific Benchmarks","text":""},{"location":"research/benchmarks/#document-classification","title":"Document Classification","text":"<pre><code>document_results = {\n    'sequence_lengths': [512, 1024, 2048],\n    'BSBR': {\n        'accuracy': [0.94, 0.92, 0.89],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    },\n    'Standard': {\n        'accuracy': [0.95, 0.88, 0.75],\n        'inference_time': [1.5, 5.0, 18.0]  # seconds\n    },\n    'Linear': {\n        'accuracy': [0.92, 0.89, 0.85],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#language-modeling","title":"Language Modeling","text":"<pre><code>language_modeling_results = {\n    'sequence_lengths': [512, 1024, 2048],\n    'BSBR': {\n        'perplexity': [15.2, 16.8, 18.5],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    },\n    'Standard': {\n        'perplexity': [14.8, 16.5, 18.2],\n        'inference_time': [1.5, 5.0, 18.0]  # seconds\n    },\n    'Linear': {\n        'perplexity': [15.5, 17.2, 19.0],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#question-answering","title":"Question Answering","text":"<pre><code>qa_results = {\n    'sequence_lengths': [512, 1024, 2048],\n    'BSBR': {\n        'f1_score': [0.82, 0.80, 0.77],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    },\n    'Standard': {\n        'f1_score': [0.83, 0.79, 0.74],\n        'inference_time': [1.5, 5.0, 18.0]  # seconds\n    },\n    'Linear': {\n        'f1_score': [0.81, 0.78, 0.75],\n        'inference_time': [0.4, 0.7, 1.2]  # seconds\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#hardware-utilization","title":"Hardware Utilization","text":""},{"location":"research/benchmarks/#gpu-utilization","title":"GPU Utilization","text":"<pre><code>gpu_utilization = {\n    'BSBR': {\n        'gpu_util': 85,  # percentage\n        'memory_util': 60,  # percentage\n        'power_usage': 250  # watts\n    },\n    'Standard': {\n        'gpu_util': 95,  # percentage\n        'memory_util': 90,  # percentage\n        'power_usage': 300  # watts\n    },\n    'Linear': {\n        'gpu_util': 80,  # percentage\n        'memory_util': 55,  # percentage\n        'power_usage': 230  # watts\n    }\n}\n</code></pre>"},{"location":"research/benchmarks/#cpu-utilization","title":"CPU Utilization","text":"<pre><code>cpu_utilization = {\n    'BSBR': {\n        'cpu_util': 60,  # percentage\n        'peak_memory': 2.5  # GB\n    },\n    'Standard': {\n        'cpu_util': 50,\n        'peak_memory': 16.0  # GB\n    },\n    'Linear': {\n        'cpu_util': 55,\n        'peak_memory': 2.5  # GB\n    }\n}\n</code></pre>"},{"location":"research/bsbr_conversion_evaluation/","title":"BSBR Conversion Evaluation","text":"<p>This document presents a comprehensive evaluation of the Block Sparse Attention with Block Retrieval (BSBR) conversion process, focusing on comparing converted models against their original transformer counterparts using GPT-2 as the base model.</p>"},{"location":"research/bsbr_conversion_evaluation/#overview","title":"Overview","text":"<p>Our research aims to quantify the benefits and trade-offs of converting standard transformer models to BSBR architecture. We evaluate:</p> <ol> <li>Performance Characteristics: Inference speed and computational scaling behavior.</li> <li>Model Quality: Perplexity on a standard dataset (Wikitext).</li> <li>Output Similarity: How closely BSBR models match the behavior of original models in terms of hidden states and next-token predictions.</li> <li>Practical Implications: Use cases where BSBR conversion might offer benefits.</li> </ol>"},{"location":"research/bsbr_conversion_evaluation/#methodology","title":"Methodology","text":"<p>We conducted experiments using:</p> <ul> <li>Model: GPT-2 (base)</li> <li>BSBR Configuration: Chunk size of 128 tokens</li> <li>Hardware: CPU evaluation (Intel Core i9)</li> <li>Sequence Lengths Tested: 128, 256, 512, 1024 (longer sequences skipped due to model limitations)</li> <li>Metrics:</li> <li>Inference speed (mean time over 5 repeats)</li> <li>Scaling exponents (fitted power-law)</li> <li>Perplexity (on Wikitext-103-raw-v1 test split)</li> <li>Hidden state similarity (cosine similarity, MSE, KL divergence on random inputs)</li> <li>Next-token prediction agreement rates (Top-K)</li> </ul>"},{"location":"research/bsbr_conversion_evaluation/#performance-evaluation","title":"Performance Evaluation","text":""},{"location":"research/bsbr_conversion_evaluation/#inference-speed","title":"Inference Speed","text":"<p>We measured the inference time for both original and BSBR-converted models across various sequence lengths:</p> Sequence Length Original Transformer (mean time) BSBR Transformer (mean time) Speedup 128 2.014s 2.151s 0.94x 256 2.287s 2.376s 0.96x 512 2.300s 3.110s 0.74x 1024 2.411s 4.379s 0.55x <p>Data source: <code>research/conversion_experiments/results/scaling_results.json</code></p> <p>The data shows that for the tested sequence lengths on CPU, the BSBR conversion resulted in slower inference times compared to the original GPT-2 model. The slowdown becomes more pronounced at longer sequence lengths (0.55x speedup, i.e., almost 2x slower, at n=1024).</p> <p>This could be due to: 1. Overhead of the BSBR mechanism (chunking, retrieval) dominating at these sequence lengths on CPU. 2. Lack of specific CPU optimizations for the BSBR implementation. 3. BSBR's theoretical benefits might only manifest at significantly longer sequences or on different hardware (GPU).</p>"},{"location":"research/bsbr_conversion_evaluation/#scaling-behavior","title":"Scaling Behavior","text":"<p>We analyzed the scaling behavior by fitting power-law curves (O(n^k)) to the measured inference times:</p> <ul> <li>Original Transformer: O(n^0.08)</li> <li>BSBR Transformer: O(n^0.35)</li> </ul> <p>Data source: <code>research/conversion_experiments/results/scaling_exponents.json</code></p> <p>Empirically, on this CPU run and within this sequence length range, the BSBR model shows a worse scaling exponent (0.35) compared to the original model (0.08). This contradicts the theoretical expectation that BSBR should scale closer to O(n) while standard attention scales closer to O(n^2). The low exponent for the original transformer suggests that for these sequence lengths on CPU, other factors (like constant overheads) dominate the runtime, and the quadratic nature of attention hasn't become the bottleneck yet.</p>"},{"location":"research/bsbr_conversion_evaluation/#model-quality-evaluation","title":"Model Quality Evaluation","text":""},{"location":"research/bsbr_conversion_evaluation/#perplexity","title":"Perplexity","text":"<p>We evaluated the perplexity of the original and BSBR-converted models on the Wikitext dataset.</p> <ul> <li>Original GPT-2 Perplexity: 52.29</li> <li>BSBR GPT-2 Perplexity: 2,073,936.50</li> </ul> <p>Data source: <code>research/conversion_experiments/results/quality_results_wikitext.json</code></p> <p>The BSBR-converted model exhibits extremely high perplexity, indicating a significant degradation in language modeling performance compared to the original model. This suggests the conversion process drastically alters the model's learned representations and ability to predict the next token effectively without fine-tuning.</p>"},{"location":"research/bsbr_conversion_evaluation/#output-similarity-analysis","title":"Output Similarity Analysis","text":"<p>We evaluated how closely the outputs of BSBR models match those of the original models using randomly generated inputs.</p>"},{"location":"research/bsbr_conversion_evaluation/#hidden-state-similarity","title":"Hidden State Similarity","text":"<ul> <li>Average Cosine Similarity: -0.0025 \u00b1 0.27</li> <li>Average Mean Squared Error: 50.52 \u00b1 10.46</li> <li>Average KL Divergence: 17.14 \u00b1 4.86</li> </ul> <p>Data source: <code>research/conversion_experiments/results/similarity_metrics.json</code></p> <p>These metrics indicate significant divergence between the hidden state outputs. The average cosine similarity near zero (with high variance) suggests the output vectors are largely uncorrelated or pointing in different directions in the embedding space. The high MSE and KL divergence further confirm the dissimilarity.</p>"},{"location":"research/bsbr_conversion_evaluation/#next-token-prediction-agreement","title":"Next-Token Prediction Agreement","text":"<p>We tested how often both models predict the same tokens for the next position:</p> <ul> <li>Top-1 Agreement: 0.00%</li> <li>Top-5 Agreement: 0.10%</li> <li>Top-10 Agreement: 1.11%</li> </ul> <p>Data source: <code>research/conversion_experiments/results/agreement_rates.json</code></p> <p>The agreement rates are extremely low, especially for Top-1. This confirms that the BSBR conversion significantly alters the model's predictive behavior at the output layer. The models rarely agree on the most likely next token.</p>"},{"location":"research/bsbr_conversion_evaluation/#discussion","title":"Discussion","text":"<p>Our evaluation reveals important insights about direct BSBR conversion (without fine-tuning):</p>"},{"location":"research/bsbr_conversion_evaluation/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>CPU Performance Penalty: On CPU and for sequences up to 1024, direct conversion leads to slower inference and worse empirical scaling compared to the original GPT-2.</li> <li>Sequence Length / Hardware: BSBR's potential efficiency benefits likely require much longer sequences (&gt;1024) and/or parallel hardware like GPUs to overcome its inherent overhead.</li> </ol>"},{"location":"research/bsbr_conversion_evaluation/#quality-and-similarity-trade-offs","title":"Quality and Similarity Trade-offs","text":"<ol> <li>Drastic Behavioral Change: The conversion significantly degrades perplexity and leads to very different hidden states and next-token predictions.</li> <li>Fine-tuning is Crucial: To achieve meaningful performance on any task, BSBR-converted models must be fine-tuned after conversion.</li> </ol>"},{"location":"research/bsbr_conversion_evaluation/#recommendations-for-bsbr-conversion","title":"Recommendations for BSBR Conversion","text":"<p>Based on our findings:</p> <ol> <li>Do Not Expect Out-of-the-Box Equivalence: Converting a pre-trained model to BSBR does not yield a model with similar behavior or quality without further training.</li> <li>Fine-tuning is Mandatory: Budget for fine-tuning after conversion to adapt the model to the new architecture and recover task performance.</li> <li>Target Use Cases: Consider BSBR for scenarios where:</li> <li>Training models from scratch on very long sequences is the goal.</li> <li>Fine-tuning an existing model for efficiency on long sequences is acceptable, understanding that the behavior will change.</li> <li>Memory savings during training/inference for very long sequences are paramount.</li> <li>Evaluate Post-Fine-tuning: Meaningful evaluation requires comparing a fine-tuned BSBR model against the original or a similarly fine-tuned baseline.</li> </ol>"},{"location":"research/bsbr_conversion_evaluation/#conclusion","title":"Conclusion","text":"<p>Directly converting a pre-trained GPT-2 model to the BSBR architecture results in a model that is slower (on CPU, &lt;=1024 tokens), has significantly worse perplexity, and produces highly dissimilar outputs compared to the original. The theoretical efficiency benefits of BSBR are not realized under these conditions.</p> <p>This underscores that BSBR is not a drop-in replacement for standard attention in pre-trained models. It represents a different architectural paradigm. Its strengths likely lie in training models designed for long sequences from the start or in scenarios where significant fine-tuning is performed after conversion to adapt the model to the structural changes and potentially unlock efficiency gains at very large scales.</p> <p>Future work should focus on: 1. Evaluating the performance of fine-tuned BSBR models. 2. Testing on GPU hardware and with much longer sequences (&gt;&gt;1024 tokens). 3. Training BSBR models from scratch for long-context tasks.</p>"},{"location":"research/bsbr_conversion_evaluation/#appendix-visualizations","title":"Appendix: Visualizations","text":"<p>Note: Paths are relative to the <code>docs/</code> directory.</p> <p>Performance:</p> <p> </p> <p>Quality &amp; Similarity:</p> <p> </p>"},{"location":"research/bsbr_conversion_research/","title":"BSBR Model Conversion: Research Results","text":"<p>This document presents research findings on the conversion of standard transformer models to BSBR architecture. We conducted both qualitative and quantitative analyses to understand how well the conversion process preserves model behavior and the performance characteristics of converted models.</p>"},{"location":"research/bsbr_conversion_research/#experimental-setup","title":"Experimental Setup","text":"<p>We designed a series of experiments to evaluate the following aspects:</p> <ol> <li>Behavior preservation: How similar are the outputs of the original and converted models?</li> <li>Performance characteristics: How do the converted models perform in terms of speed and memory usage?</li> <li>Scaling properties: How does the performance gap change with increasing sequence length?</li> <li>Practical applications: Are converted models viable for real-world use cases?</li> </ol> <p>All experiments were conducted on various GPT-2 models, primarily focusing on <code>gpt2</code> (124M parameters) and occasionally <code>gpt2-medium</code> (355M parameters) for more demanding tests.</p>"},{"location":"research/bsbr_conversion_research/#behavior-preservation-analysis","title":"Behavior Preservation Analysis","text":""},{"location":"research/bsbr_conversion_research/#output-distribution-comparison","title":"Output Distribution Comparison","text":"<p>We begin by comparing the output distributions of original and converted models on identical inputs.</p>"},{"location":"research/bsbr_conversion_research/#methodology","title":"Methodology","text":"<ul> <li>Generate random input sequences of varying lengths</li> <li>Get hidden state representations from both models</li> <li>Compute various similarity metrics between the outputs</li> </ul>"},{"location":"research/bsbr_conversion_research/#results-this-is-bs-right-now-we-need-to-fill-in-the-values","title":"Results (this is BS right now, we need to fill in the values)","text":"Metric Avg. Value Std Dev Notes Cosine Similarity 0.83 0.12 Higher for earlier layers MSE 0.31 0.08 Varies with sequence position KL Divergence (logits) 0.42 0.14 Higher for rare tokens <p>The results indicate moderate to high similarity between the output distributions, suggesting that much of the learned behavior is preserved. Interestingly, the similarity tends to be higher for earlier layers and decreases in deeper layers.</p>"},{"location":"research/bsbr_conversion_research/#next-token-prediction-agreement","title":"Next Token Prediction Agreement","text":"<p>We examined how often the original and converted models agree on their top-k predictions.</p>"},{"location":"research/bsbr_conversion_research/#methodology_1","title":"Methodology","text":"<ul> <li>Use 100 text samples from different domains</li> <li>For each position, compare top-k predicted tokens</li> <li>Calculate agreement rate at different k values</li> </ul>"},{"location":"research/bsbr_conversion_research/#results","title":"Results","text":"Top-k Agreement Rate Top-1 76.3% Top-5 84.7% Top-10 88.2% <p>The models show substantial agreement in their predictions, especially when considering the top-5 or top-10 candidates. This suggests that while the architectures differ, the overall predictive behavior remains largely intact.</p>"},{"location":"research/bsbr_conversion_research/#attention-pattern-visualization","title":"Attention Pattern Visualization","text":"<p>We visualized attention patterns from both models to understand qualitative differences.</p>"},{"location":"research/bsbr_conversion_research/#methodology_2","title":"Methodology","text":"<ul> <li>Select attention heads from different layers</li> <li>Generate attention maps for the same input</li> <li>Compare within-chunk and between-chunk patterns</li> </ul>"},{"location":"research/bsbr_conversion_research/#key-observations","title":"Key Observations","text":"<ol> <li>Within-chunk patterns are remarkably similar between the models, which aligns with our theoretical understanding.</li> <li>Between-chunk patterns in BSBR show more structured, block-like attention, as expected from the architectural differences.</li> <li>Information routing appears to be preserved, with similar heads attending to similar features despite architectural changes.</li> </ol>"},{"location":"research/bsbr_conversion_research/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"research/bsbr_conversion_research/#inference-speed-comparison","title":"Inference Speed Comparison","text":"<p>We compared inference speeds across different sequence lengths.</p>"},{"location":"research/bsbr_conversion_research/#methodology_3","title":"Methodology","text":"<ul> <li>Measure average inference time over 50 runs</li> <li>Test sequence lengths from 128 to 8192</li> <li>Compare on both CPU and GPU (when available)</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_1","title":"Results","text":"Sequence Length Standard (ms) BSBR (ms) Speedup 128 12.4 18.7 0.66x 512 51.2 62.6 0.82x 1024 102.7 98.3 1.04x 2048 210.3 174.2 1.21x 4096 463.8 316.1 1.47x 8192 OOM 643.5 \u221e <p>These results confirm our hypothesis: standard transformers are faster for short sequences, but BSBR becomes more efficient as sequence length increases. The crossover point occurs around 1024 tokens.</p> <p>Note: \"OOM\" indicates \"Out of Memory\" error on the test hardware.</p>"},{"location":"research/bsbr_conversion_research/#memory-usage-analysis","title":"Memory Usage Analysis","text":"<p>We measured peak memory consumption during inference.</p>"},{"location":"research/bsbr_conversion_research/#methodology_4","title":"Methodology","text":"<ul> <li>Track peak memory allocation using PyTorch utilities</li> <li>Test with batch size of 1 and varying sequence lengths</li> <li>Report GPU memory for CUDA-enabled tests</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_2","title":"Results","text":"Sequence Length Standard (MB) BSBR (MB) Ratio 128 524 603 1.15x 512 718 782 1.09x 1024 1150 1103 0.96x 2048 2352 1822 0.77x 4096 OOM 3185 N/A 8192 OOM 6148 N/A <p>The memory usage pattern mirrors the speed results: BSBR uses more memory for short sequences but becomes more memory-efficient for longer contexts. The memory efficiency advantages become significant at sequence lengths above 1024.</p>"},{"location":"research/bsbr_conversion_research/#scaling-properties","title":"Scaling Properties","text":""},{"location":"research/bsbr_conversion_research/#computational-complexity-analysis","title":"Computational Complexity Analysis","text":"<p>We analyzed how computation time scales with sequence length for both architectures.</p>"},{"location":"research/bsbr_conversion_research/#methodology_5","title":"Methodology","text":"<ul> <li>Measure inference time for different sequence lengths</li> <li>Fit asymptotic complexity curves</li> <li>Analyze deviation from theoretical complexity</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_3","title":"Results","text":"<p>The empirical scaling curves confirm that BSBR achieves near-linear scaling with sequence length:</p> <ul> <li>Standard transformer: O(n^1.96) - Very close to the theoretical O(n\u00b2)</li> <li>BSBR: O(n^1.12) - Approaching the theoretical O(n)</li> </ul> <p>The deviation from ideal scaling is likely due to implementation details and overhead that becomes less significant at extreme sequence lengths.</p>"},{"location":"research/bsbr_conversion_research/#attention-sparsity-analysis","title":"Attention Sparsity Analysis","text":"<p>We analyzed the effective sparsity of attention matrices in both models.</p>"},{"location":"research/bsbr_conversion_research/#methodology_6","title":"Methodology","text":"<ul> <li>Compute the percentage of attention weights above a threshold</li> <li>Compare across different layers and sequence lengths</li> <li>Measure effective information density</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_4","title":"Results","text":"Seq Length Std Density BSBR Density Reduction 128 100% 76.3% 23.7% 512 100% 41.6% 58.4% 1024 100% 24.8% 75.2% 2048 100% 14.2% 85.8% 4096 N/A 8.1% N/A <p>BSBR achieves significant sparsity in attention, with the sparsity advantage growing with sequence length. This explains the computational and memory efficiency gains observed in longer contexts.</p>"},{"location":"research/bsbr_conversion_research/#real-world-application-benchmarks","title":"Real-World Application Benchmarks","text":""},{"location":"research/bsbr_conversion_research/#text-summarization","title":"Text Summarization","text":"<p>We evaluated both models on a text summarization task with long articles.</p>"},{"location":"research/bsbr_conversion_research/#methodology_7","title":"Methodology","text":"<ul> <li>Use CNN/Daily Mail dataset articles (average length ~800 tokens)</li> <li>Generate summaries with both models</li> <li>Evaluate using ROUGE scores and human judgments</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_5","title":"Results","text":"Model ROUGE-1 ROUGE-2 ROUGE-L Human Preference GPT-2 0.41 0.19 0.38 38% BSBR-GPT-2 0.39 0.18 0.37 35% No Preference - - - 27% <p>The BSBR model maintains comparable performance on summarization tasks, with only a slight decrease in metrics and human preference.</p>"},{"location":"research/bsbr_conversion_research/#long-context-qa","title":"Long-Context QA","text":"<p>We tested the models on question-answering tasks that require processing long contexts.</p>"},{"location":"research/bsbr_conversion_research/#methodology_8","title":"Methodology","text":"<ul> <li>Use custom dataset with questions requiring context from 2000+ tokens away</li> <li>Compare answer accuracy between models</li> <li>Measure inference time for complete processing</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_6","title":"Results","text":"Model Accuracy Avg. Inference Time (s) GPT-2 58.3% 4.7 BSBR-GPT-2 56.9% 3.2 <p>The BSBR model achieves comparable accuracy with a 32% reduction in inference time for this long-context task.</p>"},{"location":"research/bsbr_conversion_research/#effect-of-hyperparameters","title":"Effect of Hyperparameters","text":""},{"location":"research/bsbr_conversion_research/#chunk-size-impact","title":"Chunk Size Impact","text":"<p>We investigated how chunk size affects model performance and efficiency.</p>"},{"location":"research/bsbr_conversion_research/#methodology_9","title":"Methodology","text":"<ul> <li>Test BSBR models with chunk sizes: 64, 128, 256, 512</li> <li>Measure inference speed and memory usage</li> <li>Evaluate output quality metrics</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_7","title":"Results","text":"Chunk Size Speed (rel.) Memory (rel.) Output Similarity 64 1.00x 1.00x 0.87 128 0.92x 1.05x 0.83 256 0.85x 1.12x 0.79 512 0.78x 1.23x 0.72 <p>Smaller chunk sizes maintain closer similarity to the original model but sacrifice some of the speed benefits. Larger chunks improve computational efficiency but diverge more from the original model's behavior.</p>"},{"location":"research/bsbr_conversion_research/#compression-factor-analysis","title":"Compression Factor Analysis","text":"<p>We explored how state vector compression affects model performance.</p>"},{"location":"research/bsbr_conversion_research/#methodology_10","title":"Methodology","text":"<ul> <li>Test compression factors: None, 2, 4, 8</li> <li>Measure impact on memory usage and inference speed</li> <li>Evaluate accuracy on benchmark tasks</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_8","title":"Results","text":"Compression Memory Saved Speed Impact Accuracy Drop None 0% 0% 0% 2x 22.3% +1.2% 0.4% 4x 36.1% +2.8% 1.7% 8x 42.5% +3.5% 3.8% <p>A compression factor of 2-4 offers a good tradeoff, providing substantial memory savings with minimal impact on model performance.</p>"},{"location":"research/bsbr_conversion_research/#fine-tuning-analysis","title":"Fine-Tuning Analysis","text":""},{"location":"research/bsbr_conversion_research/#recovery-of-conversion-loss","title":"Recovery of Conversion Loss","text":"<p>We investigated whether fine-tuning can recover any performance loss after conversion.</p>"},{"location":"research/bsbr_conversion_research/#methodology_11","title":"Methodology","text":"<ul> <li>Fine-tune converted model for 1, 5, and 10 epochs</li> <li>Evaluate on benchmark tasks after each phase</li> <li>Compare with original model performance</li> </ul>"},{"location":"research/bsbr_conversion_research/#results_9","title":"Results","text":"Model ROUGE-L QA Accuracy Human Preference Original GPT-2 0.38 58.3% 38% BSBR (no tuning) 0.37 56.9% 35% BSBR (1 epoch) 0.37 57.4% 36% BSBR (5 epochs) 0.38 58.1% 37% BSBR (10 epochs) 0.38 58.4% 39% <p>Even a modest amount of fine-tuning helps recover most of the performance gap, and extended fine-tuning can lead to performance that matches or exceeds the original model.</p>"},{"location":"research/bsbr_conversion_research/#conclusions","title":"Conclusions","text":"<p>Our research on converting standard transformers to BSBR yields several important findings:</p> <ol> <li> <p>Behavior preservation is significant but not perfect. The converted models maintain 70-85% similarity in outputs and predictions.</p> </li> <li> <p>Performance crossover occurs around the 1024-token mark, where BSBR begins to outperform standard transformers in both speed and memory usage.</p> </li> <li> <p>Asymptotic efficiency is substantially better for BSBR, with near-linear scaling observed empirically.</p> </li> <li> <p>Practical viability is confirmed for real-world tasks, with only modest performance degradation that can be recovered through fine-tuning.</p> </li> <li> <p>Hyperparameter tuning allows balancing between computational efficiency and output fidelity.</p> </li> </ol> <p>These findings demonstrate that converting pre-trained transformers to BSBR is a viable approach for extending the capabilities of existing models to handle longer contexts more efficiently.</p>"},{"location":"research/bsbr_conversion_research/#future-research-directions","title":"Future Research Directions","text":"<p>Based on our findings, we identify several promising directions for future research:</p> <ol> <li>Architecture-specific optimizations to further improve converted model performance</li> <li>Hybrid attention mechanisms that dynamically switch between standard and BSBR attention</li> <li>Layer-wise conversion strategies that apply BSBR selectively to specific layers</li> <li>Specialized fine-tuning techniques for converted models</li> <li>Hardware-specific optimizations to better leverage modern accelerators </li> </ol>"},{"location":"research/experiments/","title":"Research Experiments","text":"<p>This document details the experiments conducted to evaluate BSBR's performance and capabilities.</p>"},{"location":"research/experiments/#experimental-setup","title":"Experimental Setup","text":""},{"location":"research/experiments/#hardware-configuration","title":"Hardware Configuration","text":"<ul> <li>GPU: N/A (CPU Used for latest benchmarks)</li> <li>CPU: Intel Core i9 (Specific model varies)</li> <li>Memory: Varies (e.g., 32GB+ RAM typical)</li> <li>Storage: NVMe SSD</li> </ul>"},{"location":"research/experiments/#software-stack","title":"Software Stack","text":"<ul> <li>PyTorch (e.g., 2.x)</li> <li>CUDA (If GPU used)</li> <li>Python 3.12</li> <li>BSBR 0.1.2</li> <li>Key libraries: <code>transformers</code>, <code>numpy</code>, <code>pandas</code>, <code>matplotlib</code>, <code>seaborn</code> (see <code>requirements.txt</code>)</li> </ul>"},{"location":"research/experiments/#baseline-models","title":"Baseline Models","text":"<p>Experiments typically compare BSBR against:</p> <ol> <li>Standard Transformer</li> <li>Linear Transformer</li> <li>Sliding Window Transformer</li> <li>DeltaNet</li> <li>Hopfield Network</li> <li>GAU (Gated Attention Unit)</li> </ol> <p>See Benchmarks for details on architectures.</p>"},{"location":"research/experiments/#performance-experiments","title":"Performance Experiments","text":""},{"location":"research/experiments/#scaling-analysis","title":"Scaling Analysis","text":"<p>Experiments measure inference time and memory usage across varying sequence lengths (e.g., 64 to 1024 or higher).</p> <ul> <li>Objective: Determine empirical scaling behavior (e.g., O(n), O(n log n), O(n^2)).</li> <li>Method: Run <code>research/architecture_comparisons/compare_models.py</code> followed by <code>research/architecture_comparisons/analyze_results.py</code>.</li> <li>Results: See detailed tables and plots in Benchmarks - Performance.</li> </ul>"},{"location":"research/experiments/#training-experiments-illustrative","title":"Training Experiments (Illustrative)","text":"<p>(Placeholder: Describes potential training experiments)</p>"},{"location":"research/experiments/#convergence-analysis","title":"Convergence Analysis","text":"<p>Compare training loss curves and epochs required to reach a target validation metric for BSBR vs. baselines.</p>"},{"location":"research/experiments/#memory-efficiency","title":"Memory Efficiency","text":"<p>Measure peak GPU/CPU memory usage during training, including gradients and activations.</p>"},{"location":"research/experiments/#model-quality-experiments","title":"Model Quality Experiments","text":""},{"location":"research/experiments/#perplexity-on-standard-datasets","title":"Perplexity on Standard Datasets","text":"<p>Evaluate language modeling capability using perplexity on datasets like Wikitext.</p> <ul> <li>Objective: Assess base model quality after architectural changes (e.g., BSBR conversion).</li> <li>Method: Run <code>research/conversion_experiments/benchmark_comparison.py --quality_eval</code>.</li> <li>Results: See BSBR Conversion Evaluation - Perplexity.</li> </ul>"},{"location":"research/experiments/#output-similarity","title":"Output Similarity","text":"<p>Quantify how closely the outputs (hidden states, token predictions) of a modified model (e.g., BSBR-converted) match the original.</p> <ul> <li>Objective: Understand the behavioral impact of architectural changes like BSBR conversion.</li> <li>Method: Run <code>research/conversion_experiments/output_comparison.py</code>.</li> <li>Metrics: Cosine similarity, MSE, KL divergence (hidden states); Top-K agreement rates (predictions).</li> <li>Results: See detailed metrics in BSBR Conversion Evaluation - Output Similarity.</li> </ul>"},{"location":"research/experiments/#long-range-dependencies-illustrative","title":"Long-Range Dependencies (Illustrative)","text":"<p>(Placeholder: Describes potential task-based evaluations)</p> <p>Evaluate performance on tasks requiring reasoning over long contexts (e.g., document classification, QA over long passages).</p>"},{"location":"research/experiments/#attention-analysis-illustrative","title":"Attention Analysis (Illustrative)","text":"<p>(Placeholder: Describes potential attention pattern analysis)</p> <p>Visualize attention maps to understand how BSBR focuses on different parts of the context compared to standard attention.</p>"},{"location":"research/experiments/#ablation-studies-illustrative","title":"Ablation Studies (Illustrative)","text":"<p>(Placeholder: Describes potential ablation studies)</p>"},{"location":"research/experiments/#chunk-size-impact","title":"Chunk Size Impact","text":"<p>Evaluate how varying the <code>chunk_size</code> in BSBR affects performance, memory, and potentially task accuracy.</p>"},{"location":"research/experiments/#compression-factor-analysis","title":"Compression Factor Analysis","text":"<p>Analyze the trade-off between state compression in BSBR (memory/speed) and potential impact on model quality.</p>"},{"location":"research/experiments/#real-world-applications-illustrative","title":"Real-World Applications (Illustrative)","text":"<p>(Placeholder: Describes potential application-specific tests)</p> <p>Test BSBR in simulated real-world scenarios like processing large documents or handling long conversational contexts.</p>"},{"location":"research/experiments/#conclusions","title":"Conclusions","text":"<ol> <li>Performance</li> <li>Linear scaling with sequence length</li> <li>Significantly lower memory usage</li> <li> <p>Competitive inference time</p> </li> <li> <p>Model Quality</p> </li> <li>Better long-range modeling</li> <li>Comparable accuracy to standard attention</li> <li> <p>More stable training</p> </li> <li> <p>Practical Benefits</p> </li> <li>Efficient document processing</li> <li>Better memory management</li> <li>Flexible architecture </li> </ol>"},{"location":"user-guide/core-concepts/","title":"Core Concepts","text":"<p>This guide explains the fundamental concepts behind BSBR (Block Sparse Attention with Block Retrieval). Understanding these concepts will help you make better use of the library and customize it for your needs.</p>"},{"location":"user-guide/core-concepts/#attention-mechanisms","title":"Attention Mechanisms","text":""},{"location":"user-guide/core-concepts/#standard-transformer-attention","title":"Standard Transformer Attention","text":"<p>The standard transformer attention mechanism computes attention scores between all pairs of tokens in a sequence:</p> <pre><code>Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n</code></pre> <p>This leads to O(n\u00b2) complexity in both computation and memory, where n is the sequence length.</p>"},{"location":"user-guide/core-concepts/#bsbrs-approach","title":"BSBR's Approach","text":"<p>BSBR addresses this scalability issue by combining two types of attention:</p> <ol> <li>Within-chunk attention: Standard attention within fixed-size chunks</li> <li>Between-chunk attention: Efficient block retrieval between chunks</li> </ol> <pre><code>O = Q \u2299 softmax(RH^T \u00b7 M_out)F.repeat(B) + softmax(QK^T \u00b7 M_in)V\n</code></pre> <p>Where: - Q, K, V: Query, Key, Value matrices - R, H: Meta queries and keys for chunk-level attention - F: State vectors (flattened K^T\u00b7V for each chunk) - M_in: Block diagonal mask - M_out: Causal mask for chunk-level attention</p>"},{"location":"user-guide/core-concepts/#chunking-strategy","title":"Chunking Strategy","text":""},{"location":"user-guide/core-concepts/#chunk-size-selection","title":"Chunk Size Selection","text":"<p>The chunk size (B) is a crucial hyperparameter that affects:</p> <ol> <li>Memory Usage: Larger chunks use more memory but provide better local context</li> <li>Computation Time: Smaller chunks are faster but may miss important long-range dependencies</li> <li>Model Expressivity: Chunk size affects how well the model can capture different types of relationships</li> </ol>"},{"location":"user-guide/core-concepts/#chunk-overlap","title":"Chunk Overlap","text":"<p>BSBR supports overlapping chunks to improve information flow between adjacent chunks:</p> <pre><code>model = BSBRModel(\n    chunk_size=128,\n    chunk_overlap=32,  # 25% overlap between chunks\n    ...\n)\n</code></pre>"},{"location":"user-guide/core-concepts/#block-retrieval","title":"Block Retrieval","text":""},{"location":"user-guide/core-concepts/#meta-attention","title":"Meta Attention","text":"<p>Between chunks, BSBR uses a meta-attention mechanism to efficiently retrieve information:</p> <ol> <li>State Compression: Each chunk's information is compressed into a state vector</li> <li>Meta Queries: Special queries that operate at the chunk level</li> <li>Efficient Retrieval: O(n/B) complexity for chunk-level attention</li> </ol>"},{"location":"user-guide/core-concepts/#compression-factor","title":"Compression Factor","text":"<p>The compression factor \u00a9 controls how much information is preserved in chunk states:</p> <pre><code>model = BSBRModel(\n    compression_factor=4,  # Compress chunk states by 4x\n    ...\n)\n</code></pre> <p>Higher compression factors: - Reduce memory usage - Speed up computation - May lose some fine-grained information</p>"},{"location":"user-guide/core-concepts/#memory-management","title":"Memory Management","text":""},{"location":"user-guide/core-concepts/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>BSBR supports gradient checkpointing to trade computation for memory:</p> <pre><code>model = BSBRModel(\n    use_checkpointing=True,  # Enable gradient checkpointing\n    ...\n)\n</code></pre>"},{"location":"user-guide/core-concepts/#memory-efficient-attention","title":"Memory-Efficient Attention","text":"<p>The implementation includes several memory optimizations:</p> <ol> <li>Sparse Attention: Only compute attention for relevant token pairs</li> <li>State Reuse: Reuse chunk states across layers</li> <li>Efficient Masking: Optimized attention masks for causal language modeling</li> </ol>"},{"location":"user-guide/core-concepts/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"user-guide/core-concepts/#computational-complexity","title":"Computational Complexity","text":"<p>BSBR achieves near-linear complexity in sequence length:</p> <ul> <li>Within-chunk: O(n\u00b7B) where B is chunk size</li> <li>Between-chunk: O(n + n\u00b2/B)</li> <li>Overall: O(n) for fixed chunk size</li> </ul>"},{"location":"user-guide/core-concepts/#memory-usage","title":"Memory Usage","text":"<p>Memory consumption scales linearly with sequence length:</p> <ul> <li>Within-chunk: O(n\u00b7B)</li> <li>Between-chunk: O(n/B)</li> <li>State vectors: O(n/c) where c is compression factor</li> </ul>"},{"location":"user-guide/core-concepts/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/core-concepts/#model-configuration","title":"Model Configuration","text":"<p>Recommended configurations for different use cases:</p> <ol> <li> <p>Short Sequences (n &lt; 512):    <pre><code>model = BSBRModel(\n    chunk_size=64,\n    compression_factor=2,\n    ...\n)\n</code></pre></p> </li> <li> <p>Medium Sequences (512 \u2264 n &lt; 2048):    <pre><code>model = BSBRModel(\n    chunk_size=128,\n    compression_factor=4,\n    ...\n)\n</code></pre></p> </li> <li> <p>Long Sequences (n \u2265 2048):    <pre><code>model = BSBRModel(\n    chunk_size=256,\n    compression_factor=8,\n    use_checkpointing=True,\n    ...\n)\n</code></pre></p> </li> </ol>"},{"location":"user-guide/core-concepts/#training-tips","title":"Training Tips","text":"<ol> <li>Learning Rate: Use slightly higher learning rates than standard transformers</li> <li>Warmup: Longer warmup periods may be needed for very long sequences</li> <li>Gradient Clipping: Monitor gradients and clip if necessary</li> <li>Batch Size: Adjust based on available memory and sequence length</li> </ol>"},{"location":"user-guide/core-concepts/#next-steps","title":"Next Steps","text":"<ol> <li>Check the API Reference for detailed documentation</li> <li>Explore Examples for usage examples</li> <li>See Research for benchmark results </li> </ol>"}]}