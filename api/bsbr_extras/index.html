<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Block Sparse Attention with Block Retrieval - An efficient attention mechanism for long-context reasoning"><link href=../bsbr/ rel=prev><link href=../bsbr_transformers/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.9"><title>BSBR Extras - BSBR</title><link rel=stylesheet href=../../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_mkdocstrings.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#bsbr-extras-api class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title=BSBR class="md-header__button md-logo" aria-label=BSBR data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> BSBR </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> BSBR Extras </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../getting-started/installation/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../../user-guide/core-concepts/ class=md-tabs__link> User Guide </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../bsbr/ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=../../examples/basic_usage/ class=md-tabs__link> Examples </a> </li> <li class=md-tabs__item> <a href=../../research/ class=md-tabs__link> Research </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title=BSBR class="md-nav__button md-logo" aria-label=BSBR data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> BSBR </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../getting-started/installation/ class=md-nav__link> <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a href=../../getting-started/quickstart/ class=md-nav__link> <span class=md-ellipsis> Quick Start </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> User Guide </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> User Guide </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../user-guide/core-concepts/ class=md-nav__link> <span class=md-ellipsis> Core Concepts </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> API Reference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../bsbr/ class=md-nav__link> <span class=md-ellipsis> BSBR Core </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> BSBR Extras </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> BSBR Extras </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#bsbr_extras.standard_transformer.StandardTransformerModel class=md-nav__link> <span class=md-ellipsis> StandardTransformerModel </span> </a> <nav class=md-nav aria-label=StandardTransformerModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.standard_transformer.StandardTransformerModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.linear_transformer.LinearTransformerModel class=md-nav__link> <span class=md-ellipsis> LinearTransformerModel </span> </a> <nav class=md-nav aria-label=LinearTransformerModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.linear_transformer.LinearTransformerModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.delta_net.DeltaNetModel class=md-nav__link> <span class=md-ellipsis> DeltaNetModel </span> </a> <nav class=md-nav aria-label=DeltaNetModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.delta_net.DeltaNetModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.gau.GAUModel class=md-nav__link> <span class=md-ellipsis> GAUModel </span> </a> <nav class=md-nav aria-label=GAUModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.gau.GAUModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.hopfield_network.HopfieldNetworkModel class=md-nav__link> <span class=md-ellipsis> HopfieldNetworkModel </span> </a> <nav class=md-nav aria-label=HopfieldNetworkModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.hopfield_network.HopfieldNetworkModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel class=md-nav__link> <span class=md-ellipsis> SlidingWindowTransformerModel </span> </a> <nav class=md-nav aria-label=SlidingWindowTransformerModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../bsbr_transformers/ class=md-nav__link> <span class=md-ellipsis> BSBR Transformers </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Examples </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../examples/basic_usage/ class=md-nav__link> <span class=md-ellipsis> Basic Usage </span> </a> </li> <li class=md-nav__item> <a href=../../examples/advanced_usage/ class=md-nav__link> <span class=md-ellipsis> Advanced Usage </span> </a> </li> <li class=md-nav__item> <a href=../../examples/research_examples/ class=md-nav__link> <span class=md-ellipsis> Research Examples </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Research </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Research </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../research/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../research/background/ class=md-nav__link> <span class=md-ellipsis> Background </span> </a> </li> <li class=md-nav__item> <a href=../../research/benchmarks/ class=md-nav__link> <span class=md-ellipsis> Benchmarks </span> </a> </li> <li class=md-nav__item> <a href=../../research/experiments/ class=md-nav__link> <span class=md-ellipsis> Experiments </span> </a> </li> <li class=md-nav__item> <a href=../../research/bsbr_conversion_research/ class=md-nav__link> <span class=md-ellipsis> Conversion Research </span> </a> </li> <li class=md-nav__item> <a href=../../research/bsbr_conversion_evaluation/ class=md-nav__link> <span class=md-ellipsis> Conversion Evaluation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#bsbr_extras.standard_transformer.StandardTransformerModel class=md-nav__link> <span class=md-ellipsis> StandardTransformerModel </span> </a> <nav class=md-nav aria-label=StandardTransformerModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.standard_transformer.StandardTransformerModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.linear_transformer.LinearTransformerModel class=md-nav__link> <span class=md-ellipsis> LinearTransformerModel </span> </a> <nav class=md-nav aria-label=LinearTransformerModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.linear_transformer.LinearTransformerModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.delta_net.DeltaNetModel class=md-nav__link> <span class=md-ellipsis> DeltaNetModel </span> </a> <nav class=md-nav aria-label=DeltaNetModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.delta_net.DeltaNetModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.gau.GAUModel class=md-nav__link> <span class=md-ellipsis> GAUModel </span> </a> <nav class=md-nav aria-label=GAUModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.gau.GAUModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.hopfield_network.HopfieldNetworkModel class=md-nav__link> <span class=md-ellipsis> HopfieldNetworkModel </span> </a> <nav class=md-nav aria-label=HopfieldNetworkModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.hopfield_network.HopfieldNetworkModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel class=md-nav__link> <span class=md-ellipsis> SlidingWindowTransformerModel </span> </a> <nav class=md-nav aria-label=SlidingWindowTransformerModel> <ul class=md-nav__list> <li class=md-nav__item> <a href=#bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=bsbr-extras-api>BSBR Extras API</h1> <div class="doc doc-object doc-class"> <h2 id=bsbr_extras.standard_transformer.StandardTransformerModel class="doc doc-heading"> <code>bsbr_extras.standard_transformer.StandardTransformerModel</code> </h2> <div class="doc doc-contents first"> <p class="doc doc-class-bases"> Bases: <code><span title=torch.nn.Module>Module</span></code></p> <p>Full Standard Transformer model stacking multiple Standard Transformer layers.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>vocab_size</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Vocabulary size for embedding layer</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>hidden_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Hidden dimension size</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_layers</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of transformer layers</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_heads</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of attention heads</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>ff_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Feed-forward intermediate dimension</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>dropout</code> </td> <td> <code><span title=float>float</span></code> </td> <td> <div class=doc-md-description> <p>Dropout probability</p> </div> </td> <td> <code>0.1</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/standard_transformer.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-156>156</a></span>
<span class=normal><a href=#__codelineno-0-157>157</a></span>
<span class=normal><a href=#__codelineno-0-158>158</a></span>
<span class=normal><a href=#__codelineno-0-159>159</a></span>
<span class=normal><a href=#__codelineno-0-160>160</a></span>
<span class=normal><a href=#__codelineno-0-161>161</a></span>
<span class=normal><a href=#__codelineno-0-162>162</a></span>
<span class=normal><a href=#__codelineno-0-163>163</a></span>
<span class=normal><a href=#__codelineno-0-164>164</a></span>
<span class=normal><a href=#__codelineno-0-165>165</a></span>
<span class=normal><a href=#__codelineno-0-166>166</a></span>
<span class=normal><a href=#__codelineno-0-167>167</a></span>
<span class=normal><a href=#__codelineno-0-168>168</a></span>
<span class=normal><a href=#__codelineno-0-169>169</a></span>
<span class=normal><a href=#__codelineno-0-170>170</a></span>
<span class=normal><a href=#__codelineno-0-171>171</a></span>
<span class=normal><a href=#__codelineno-0-172>172</a></span>
<span class=normal><a href=#__codelineno-0-173>173</a></span>
<span class=normal><a href=#__codelineno-0-174>174</a></span>
<span class=normal><a href=#__codelineno-0-175>175</a></span>
<span class=normal><a href=#__codelineno-0-176>176</a></span>
<span class=normal><a href=#__codelineno-0-177>177</a></span>
<span class=normal><a href=#__codelineno-0-178>178</a></span>
<span class=normal><a href=#__codelineno-0-179>179</a></span>
<span class=normal><a href=#__codelineno-0-180>180</a></span>
<span class=normal><a href=#__codelineno-0-181>181</a></span>
<span class=normal><a href=#__codelineno-0-182>182</a></span>
<span class=normal><a href=#__codelineno-0-183>183</a></span>
<span class=normal><a href=#__codelineno-0-184>184</a></span>
<span class=normal><a href=#__codelineno-0-185>185</a></span>
<span class=normal><a href=#__codelineno-0-186>186</a></span>
<span class=normal><a href=#__codelineno-0-187>187</a></span>
<span class=normal><a href=#__codelineno-0-188>188</a></span>
<span class=normal><a href=#__codelineno-0-189>189</a></span>
<span class=normal><a href=#__codelineno-0-190>190</a></span>
<span class=normal><a href=#__codelineno-0-191>191</a></span>
<span class=normal><a href=#__codelineno-0-192>192</a></span>
<span class=normal><a href=#__codelineno-0-193>193</a></span>
<span class=normal><a href=#__codelineno-0-194>194</a></span>
<span class=normal><a href=#__codelineno-0-195>195</a></span>
<span class=normal><a href=#__codelineno-0-196>196</a></span>
<span class=normal><a href=#__codelineno-0-197>197</a></span>
<span class=normal><a href=#__codelineno-0-198>198</a></span>
<span class=normal><a href=#__codelineno-0-199>199</a></span>
<span class=normal><a href=#__codelineno-0-200>200</a></span>
<span class=normal><a href=#__codelineno-0-201>201</a></span>
<span class=normal><a href=#__codelineno-0-202>202</a></span>
<span class=normal><a href=#__codelineno-0-203>203</a></span>
<span class=normal><a href=#__codelineno-0-204>204</a></span>
<span class=normal><a href=#__codelineno-0-205>205</a></span>
<span class=normal><a href=#__codelineno-0-206>206</a></span>
<span class=normal><a href=#__codelineno-0-207>207</a></span>
<span class=normal><a href=#__codelineno-0-208>208</a></span>
<span class=normal><a href=#__codelineno-0-209>209</a></span>
<span class=normal><a href=#__codelineno-0-210>210</a></span>
<span class=normal><a href=#__codelineno-0-211>211</a></span>
<span class=normal><a href=#__codelineno-0-212>212</a></span>
<span class=normal><a href=#__codelineno-0-213>213</a></span>
<span class=normal><a href=#__codelineno-0-214>214</a></span>
<span class=normal><a href=#__codelineno-0-215>215</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-156 name=__codelineno-0-156></a><span class=k>class</span><span class=w> </span><span class=nc>StandardTransformerModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<a id=__codelineno-0-157 name=__codelineno-0-157></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-158 name=__codelineno-0-158></a><span class=sd>    Full Standard Transformer model stacking multiple Standard Transformer layers.</span>
<a id=__codelineno-0-159 name=__codelineno-0-159></a>
<a id=__codelineno-0-160 name=__codelineno-0-160></a><span class=sd>    Args:</span>
<a id=__codelineno-0-161 name=__codelineno-0-161></a><span class=sd>        vocab_size (int): Vocabulary size for embedding layer</span>
<a id=__codelineno-0-162 name=__codelineno-0-162></a><span class=sd>        hidden_dim (int): Hidden dimension size</span>
<a id=__codelineno-0-163 name=__codelineno-0-163></a><span class=sd>        num_layers (int): Number of transformer layers</span>
<a id=__codelineno-0-164 name=__codelineno-0-164></a><span class=sd>        num_heads (int): Number of attention heads</span>
<a id=__codelineno-0-165 name=__codelineno-0-165></a><span class=sd>        ff_dim (int): Feed-forward intermediate dimension</span>
<a id=__codelineno-0-166 name=__codelineno-0-166></a><span class=sd>        dropout (float): Dropout probability</span>
<a id=__codelineno-0-167 name=__codelineno-0-167></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-168 name=__codelineno-0-168></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span>
<a id=__codelineno-0-169 name=__codelineno-0-169></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-170 name=__codelineno-0-170></a>        <span class=n>vocab_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-171 name=__codelineno-0-171></a>        <span class=n>hidden_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-172 name=__codelineno-0-172></a>        <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-173 name=__codelineno-0-173></a>        <span class=n>num_heads</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-174 name=__codelineno-0-174></a>        <span class=n>ff_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-175 name=__codelineno-0-175></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span>
<a id=__codelineno-0-176 name=__codelineno-0-176></a>    <span class=p>):</span>
<a id=__codelineno-0-177 name=__codelineno-0-177></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
<a id=__codelineno-0-178 name=__codelineno-0-178></a>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-179 name=__codelineno-0-179></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
<a id=__codelineno-0-180 name=__codelineno-0-180></a>
<a id=__codelineno-0-181 name=__codelineno-0-181></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
<a id=__codelineno-0-182 name=__codelineno-0-182></a>            <span class=n>StandardTransformerLayer</span><span class=p>(</span>
<a id=__codelineno-0-183 name=__codelineno-0-183></a>                <span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span>
<a id=__codelineno-0-184 name=__codelineno-0-184></a>                <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>,</span>
<a id=__codelineno-0-185 name=__codelineno-0-185></a>                <span class=n>ff_dim</span><span class=o>=</span><span class=n>ff_dim</span><span class=p>,</span>
<a id=__codelineno-0-186 name=__codelineno-0-186></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span>
<a id=__codelineno-0-187 name=__codelineno-0-187></a>            <span class=p>)</span>
<a id=__codelineno-0-188 name=__codelineno-0-188></a>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)</span>
<a id=__codelineno-0-189 name=__codelineno-0-189></a>        <span class=p>])</span>
<a id=__codelineno-0-190 name=__codelineno-0-190></a>
<a id=__codelineno-0-191 name=__codelineno-0-191></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-192 name=__codelineno-0-192></a>
<a id=__codelineno-0-193 name=__codelineno-0-193></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-194 name=__codelineno-0-194></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-195 name=__codelineno-0-195></a>        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-196 name=__codelineno-0-196></a>        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-197 name=__codelineno-0-197></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
<a id=__codelineno-0-198 name=__codelineno-0-198></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-199 name=__codelineno-0-199></a><span class=sd>        Forward pass for the full Standard Transformer model.</span>
<a id=__codelineno-0-200 name=__codelineno-0-200></a>
<a id=__codelineno-0-201 name=__codelineno-0-201></a><span class=sd>        Args:</span>
<a id=__codelineno-0-202 name=__codelineno-0-202></a><span class=sd>            input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-203 name=__codelineno-0-203></a><span class=sd>            attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-204 name=__codelineno-0-204></a>
<a id=__codelineno-0-205 name=__codelineno-0-205></a><span class=sd>        Returns:</span>
<a id=__codelineno-0-206 name=__codelineno-0-206></a><span class=sd>            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-207 name=__codelineno-0-207></a><span class=sd>        &quot;&quot;&quot;</span>
<a id=__codelineno-0-208 name=__codelineno-0-208></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-209 name=__codelineno-0-209></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-210 name=__codelineno-0-210></a>
<a id=__codelineno-0-211 name=__codelineno-0-211></a>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
<a id=__codelineno-0-212 name=__codelineno-0-212></a>            <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>
<a id=__codelineno-0-213 name=__codelineno-0-213></a>
<a id=__codelineno-0-214 name=__codelineno-0-214></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-215 name=__codelineno-0-215></a>        <span class=k>return</span> <span class=n>hidden_states</span>
</code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=bsbr_extras.standard_transformer.StandardTransformerModel.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> </h3> <div class="doc doc-contents "> <p>Forward pass for the full Standard Transformer model.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>input_ids</code> </td> <td> <code><span title=torch.LongTensor>LongTensor</span></code> </td> <td> <div class=doc-md-description> <p>Token IDs of shape [batch_size, seq_len]</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>attention_mask</code> </td> <td> <code><span title=typing.Optional>Optional</span>[<span title=torch.Tensor>Tensor</span>]</code> </td> <td> <div class=doc-md-description> <p>Optional attention mask of shape [batch_size, seq_len]</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <p><span class=doc-section-title>Returns:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>output</code></td> <td> <code><span title=torch.Tensor>Tensor</span></code> </td> <td> <div class=doc-md-description> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/standard_transformer.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-193>193</a></span>
<span class=normal><a href=#__codelineno-0-194>194</a></span>
<span class=normal><a href=#__codelineno-0-195>195</a></span>
<span class=normal><a href=#__codelineno-0-196>196</a></span>
<span class=normal><a href=#__codelineno-0-197>197</a></span>
<span class=normal><a href=#__codelineno-0-198>198</a></span>
<span class=normal><a href=#__codelineno-0-199>199</a></span>
<span class=normal><a href=#__codelineno-0-200>200</a></span>
<span class=normal><a href=#__codelineno-0-201>201</a></span>
<span class=normal><a href=#__codelineno-0-202>202</a></span>
<span class=normal><a href=#__codelineno-0-203>203</a></span>
<span class=normal><a href=#__codelineno-0-204>204</a></span>
<span class=normal><a href=#__codelineno-0-205>205</a></span>
<span class=normal><a href=#__codelineno-0-206>206</a></span>
<span class=normal><a href=#__codelineno-0-207>207</a></span>
<span class=normal><a href=#__codelineno-0-208>208</a></span>
<span class=normal><a href=#__codelineno-0-209>209</a></span>
<span class=normal><a href=#__codelineno-0-210>210</a></span>
<span class=normal><a href=#__codelineno-0-211>211</a></span>
<span class=normal><a href=#__codelineno-0-212>212</a></span>
<span class=normal><a href=#__codelineno-0-213>213</a></span>
<span class=normal><a href=#__codelineno-0-214>214</a></span>
<span class=normal><a href=#__codelineno-0-215>215</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-193 name=__codelineno-0-193></a><span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-194 name=__codelineno-0-194></a>    <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-195 name=__codelineno-0-195></a>    <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-196 name=__codelineno-0-196></a>    <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-197 name=__codelineno-0-197></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
<a id=__codelineno-0-198 name=__codelineno-0-198></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-199 name=__codelineno-0-199></a><span class=sd>    Forward pass for the full Standard Transformer model.</span>
<a id=__codelineno-0-200 name=__codelineno-0-200></a>
<a id=__codelineno-0-201 name=__codelineno-0-201></a><span class=sd>    Args:</span>
<a id=__codelineno-0-202 name=__codelineno-0-202></a><span class=sd>        input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-203 name=__codelineno-0-203></a><span class=sd>        attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-204 name=__codelineno-0-204></a>
<a id=__codelineno-0-205 name=__codelineno-0-205></a><span class=sd>    Returns:</span>
<a id=__codelineno-0-206 name=__codelineno-0-206></a><span class=sd>        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-207 name=__codelineno-0-207></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-208 name=__codelineno-0-208></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-209 name=__codelineno-0-209></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-210 name=__codelineno-0-210></a>
<a id=__codelineno-0-211 name=__codelineno-0-211></a>    <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
<a id=__codelineno-0-212 name=__codelineno-0-212></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>
<a id=__codelineno-0-213 name=__codelineno-0-213></a>
<a id=__codelineno-0-214 name=__codelineno-0-214></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-215 name=__codelineno-0-215></a>    <span class=k>return</span> <span class=n>hidden_states</span>
</code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=bsbr_extras.linear_transformer.LinearTransformerModel class="doc doc-heading"> <code>bsbr_extras.linear_transformer.LinearTransformerModel</code> </h2> <div class="doc doc-contents first"> <p class="doc doc-class-bases"> Bases: <code><span title=torch.nn.Module>Module</span></code></p> <p>Full Linear Transformer model stacking multiple Linear Transformer layers.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>vocab_size</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Vocabulary size for embedding layer</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>hidden_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Hidden dimension size</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_layers</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of LinearTransformer layers</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_heads</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of attention heads</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>ff_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Feed-forward intermediate dimension</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>dropout</code> </td> <td> <code><span title=float>float</span></code> </td> <td> <div class=doc-md-description> <p>Dropout probability</p> </div> </td> <td> <code>0.1</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/linear_transformer.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-177>177</a></span>
<span class=normal><a href=#__codelineno-0-178>178</a></span>
<span class=normal><a href=#__codelineno-0-179>179</a></span>
<span class=normal><a href=#__codelineno-0-180>180</a></span>
<span class=normal><a href=#__codelineno-0-181>181</a></span>
<span class=normal><a href=#__codelineno-0-182>182</a></span>
<span class=normal><a href=#__codelineno-0-183>183</a></span>
<span class=normal><a href=#__codelineno-0-184>184</a></span>
<span class=normal><a href=#__codelineno-0-185>185</a></span>
<span class=normal><a href=#__codelineno-0-186>186</a></span>
<span class=normal><a href=#__codelineno-0-187>187</a></span>
<span class=normal><a href=#__codelineno-0-188>188</a></span>
<span class=normal><a href=#__codelineno-0-189>189</a></span>
<span class=normal><a href=#__codelineno-0-190>190</a></span>
<span class=normal><a href=#__codelineno-0-191>191</a></span>
<span class=normal><a href=#__codelineno-0-192>192</a></span>
<span class=normal><a href=#__codelineno-0-193>193</a></span>
<span class=normal><a href=#__codelineno-0-194>194</a></span>
<span class=normal><a href=#__codelineno-0-195>195</a></span>
<span class=normal><a href=#__codelineno-0-196>196</a></span>
<span class=normal><a href=#__codelineno-0-197>197</a></span>
<span class=normal><a href=#__codelineno-0-198>198</a></span>
<span class=normal><a href=#__codelineno-0-199>199</a></span>
<span class=normal><a href=#__codelineno-0-200>200</a></span>
<span class=normal><a href=#__codelineno-0-201>201</a></span>
<span class=normal><a href=#__codelineno-0-202>202</a></span>
<span class=normal><a href=#__codelineno-0-203>203</a></span>
<span class=normal><a href=#__codelineno-0-204>204</a></span>
<span class=normal><a href=#__codelineno-0-205>205</a></span>
<span class=normal><a href=#__codelineno-0-206>206</a></span>
<span class=normal><a href=#__codelineno-0-207>207</a></span>
<span class=normal><a href=#__codelineno-0-208>208</a></span>
<span class=normal><a href=#__codelineno-0-209>209</a></span>
<span class=normal><a href=#__codelineno-0-210>210</a></span>
<span class=normal><a href=#__codelineno-0-211>211</a></span>
<span class=normal><a href=#__codelineno-0-212>212</a></span>
<span class=normal><a href=#__codelineno-0-213>213</a></span>
<span class=normal><a href=#__codelineno-0-214>214</a></span>
<span class=normal><a href=#__codelineno-0-215>215</a></span>
<span class=normal><a href=#__codelineno-0-216>216</a></span>
<span class=normal><a href=#__codelineno-0-217>217</a></span>
<span class=normal><a href=#__codelineno-0-218>218</a></span>
<span class=normal><a href=#__codelineno-0-219>219</a></span>
<span class=normal><a href=#__codelineno-0-220>220</a></span>
<span class=normal><a href=#__codelineno-0-221>221</a></span>
<span class=normal><a href=#__codelineno-0-222>222</a></span>
<span class=normal><a href=#__codelineno-0-223>223</a></span>
<span class=normal><a href=#__codelineno-0-224>224</a></span>
<span class=normal><a href=#__codelineno-0-225>225</a></span>
<span class=normal><a href=#__codelineno-0-226>226</a></span>
<span class=normal><a href=#__codelineno-0-227>227</a></span>
<span class=normal><a href=#__codelineno-0-228>228</a></span>
<span class=normal><a href=#__codelineno-0-229>229</a></span>
<span class=normal><a href=#__codelineno-0-230>230</a></span>
<span class=normal><a href=#__codelineno-0-231>231</a></span>
<span class=normal><a href=#__codelineno-0-232>232</a></span>
<span class=normal><a href=#__codelineno-0-233>233</a></span>
<span class=normal><a href=#__codelineno-0-234>234</a></span>
<span class=normal><a href=#__codelineno-0-235>235</a></span>
<span class=normal><a href=#__codelineno-0-236>236</a></span>
<span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span>
<span class=normal><a href=#__codelineno-0-242>242</a></span>
<span class=normal><a href=#__codelineno-0-243>243</a></span>
<span class=normal><a href=#__codelineno-0-244>244</a></span>
<span class=normal><a href=#__codelineno-0-245>245</a></span>
<span class=normal><a href=#__codelineno-0-246>246</a></span>
<span class=normal><a href=#__codelineno-0-247>247</a></span>
<span class=normal><a href=#__codelineno-0-248>248</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-177 name=__codelineno-0-177></a><span class=k>class</span><span class=w> </span><span class=nc>LinearTransformerModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<a id=__codelineno-0-178 name=__codelineno-0-178></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-179 name=__codelineno-0-179></a><span class=sd>    Full Linear Transformer model stacking multiple Linear Transformer layers.</span>
<a id=__codelineno-0-180 name=__codelineno-0-180></a>
<a id=__codelineno-0-181 name=__codelineno-0-181></a><span class=sd>    Args:</span>
<a id=__codelineno-0-182 name=__codelineno-0-182></a><span class=sd>        vocab_size (int): Vocabulary size for embedding layer</span>
<a id=__codelineno-0-183 name=__codelineno-0-183></a><span class=sd>        hidden_dim (int): Hidden dimension size</span>
<a id=__codelineno-0-184 name=__codelineno-0-184></a><span class=sd>        num_layers (int): Number of LinearTransformer layers</span>
<a id=__codelineno-0-185 name=__codelineno-0-185></a><span class=sd>        num_heads (int): Number of attention heads</span>
<a id=__codelineno-0-186 name=__codelineno-0-186></a><span class=sd>        ff_dim (int): Feed-forward intermediate dimension</span>
<a id=__codelineno-0-187 name=__codelineno-0-187></a><span class=sd>        dropout (float): Dropout probability</span>
<a id=__codelineno-0-188 name=__codelineno-0-188></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-189 name=__codelineno-0-189></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span>
<a id=__codelineno-0-190 name=__codelineno-0-190></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-191 name=__codelineno-0-191></a>        <span class=n>vocab_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-192 name=__codelineno-0-192></a>        <span class=n>hidden_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-193 name=__codelineno-0-193></a>        <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-194 name=__codelineno-0-194></a>        <span class=n>num_heads</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-195 name=__codelineno-0-195></a>        <span class=n>ff_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-196 name=__codelineno-0-196></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span>
<a id=__codelineno-0-197 name=__codelineno-0-197></a>    <span class=p>):</span>
<a id=__codelineno-0-198 name=__codelineno-0-198></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
<a id=__codelineno-0-199 name=__codelineno-0-199></a>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-200 name=__codelineno-0-200></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
<a id=__codelineno-0-201 name=__codelineno-0-201></a>
<a id=__codelineno-0-202 name=__codelineno-0-202></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
<a id=__codelineno-0-203 name=__codelineno-0-203></a>            <span class=n>LinearTransformerLayer</span><span class=p>(</span>
<a id=__codelineno-0-204 name=__codelineno-0-204></a>                <span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span>
<a id=__codelineno-0-205 name=__codelineno-0-205></a>                <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>,</span>
<a id=__codelineno-0-206 name=__codelineno-0-206></a>                <span class=n>ff_dim</span><span class=o>=</span><span class=n>ff_dim</span><span class=p>,</span>
<a id=__codelineno-0-207 name=__codelineno-0-207></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span>
<a id=__codelineno-0-208 name=__codelineno-0-208></a>            <span class=p>)</span>
<a id=__codelineno-0-209 name=__codelineno-0-209></a>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)</span>
<a id=__codelineno-0-210 name=__codelineno-0-210></a>        <span class=p>])</span>
<a id=__codelineno-0-211 name=__codelineno-0-211></a>
<a id=__codelineno-0-212 name=__codelineno-0-212></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-213 name=__codelineno-0-213></a>        <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span> <span class=o>=</span> <span class=n>num_layers</span>
<a id=__codelineno-0-214 name=__codelineno-0-214></a>
<a id=__codelineno-0-215 name=__codelineno-0-215></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-216 name=__codelineno-0-216></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-217 name=__codelineno-0-217></a>        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-218 name=__codelineno-0-218></a>        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
<a id=__codelineno-0-219 name=__codelineno-0-219></a>        <span class=n>states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>list</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-220 name=__codelineno-0-220></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>list</span><span class=p>]:</span>
<a id=__codelineno-0-221 name=__codelineno-0-221></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-222 name=__codelineno-0-222></a><span class=sd>        Forward pass for the full Linear Transformer model.</span>
<a id=__codelineno-0-223 name=__codelineno-0-223></a>
<a id=__codelineno-0-224 name=__codelineno-0-224></a><span class=sd>        Args:</span>
<a id=__codelineno-0-225 name=__codelineno-0-225></a><span class=sd>            input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-226 name=__codelineno-0-226></a><span class=sd>            attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-227 name=__codelineno-0-227></a><span class=sd>            states: Optional previous state list for each layer</span>
<a id=__codelineno-0-228 name=__codelineno-0-228></a>
<a id=__codelineno-0-229 name=__codelineno-0-229></a><span class=sd>        Returns:</span>
<a id=__codelineno-0-230 name=__codelineno-0-230></a><span class=sd>            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-231 name=__codelineno-0-231></a><span class=sd>            new_states: Updated state list for each layer</span>
<a id=__codelineno-0-232 name=__codelineno-0-232></a><span class=sd>        &quot;&quot;&quot;</span>
<a id=__codelineno-0-233 name=__codelineno-0-233></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-234 name=__codelineno-0-234></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-235 name=__codelineno-0-235></a>
<a id=__codelineno-0-236 name=__codelineno-0-236></a>        <span class=c1># Initialize states if not provided</span>
<a id=__codelineno-0-237 name=__codelineno-0-237></a>        <span class=k>if</span> <span class=n>states</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
<a id=__codelineno-0-238 name=__codelineno-0-238></a>            <span class=n>states</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span>
<a id=__codelineno-0-239 name=__codelineno-0-239></a>
<a id=__codelineno-0-240 name=__codelineno-0-240></a>        <span class=n>new_states</span> <span class=o>=</span> <span class=p>[]</span>
<a id=__codelineno-0-241 name=__codelineno-0-241></a>
<a id=__codelineno-0-242 name=__codelineno-0-242></a>        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
<a id=__codelineno-0-243 name=__codelineno-0-243></a>            <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_state</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>states</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
<a id=__codelineno-0-244 name=__codelineno-0-244></a>            <span class=n>new_states</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>new_state</span><span class=p>)</span>
<a id=__codelineno-0-245 name=__codelineno-0-245></a>
<a id=__codelineno-0-246 name=__codelineno-0-246></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-247 name=__codelineno-0-247></a>
<a id=__codelineno-0-248 name=__codelineno-0-248></a>        <span class=k>return</span> <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_states</span>
</code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=bsbr_extras.linear_transformer.LinearTransformerModel.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>states</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> </h3> <div class="doc doc-contents "> <p>Forward pass for the full Linear Transformer model.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>input_ids</code> </td> <td> <code><span title=torch.LongTensor>LongTensor</span></code> </td> <td> <div class=doc-md-description> <p>Token IDs of shape [batch_size, seq_len]</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>attention_mask</code> </td> <td> <code><span title=typing.Optional>Optional</span>[<span title=torch.Tensor>Tensor</span>]</code> </td> <td> <div class=doc-md-description> <p>Optional attention mask of shape [batch_size, seq_len]</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td> <code>states</code> </td> <td> <code><span title=typing.Optional>Optional</span>[<span title=list>list</span>]</code> </td> <td> <div class=doc-md-description> <p>Optional previous state list for each layer</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <p><span class=doc-section-title>Returns:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>output</code></td> <td> <code><span title=torch.Tensor>Tensor</span></code> </td> <td> <div class=doc-md-description> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> </div> </td> </tr> <tr class=doc-section-item> <td><code>new_states</code></td> <td> <code><span title=list>list</span></code> </td> <td> <div class=doc-md-description> <p>Updated state list for each layer</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/linear_transformer.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-215>215</a></span>
<span class=normal><a href=#__codelineno-0-216>216</a></span>
<span class=normal><a href=#__codelineno-0-217>217</a></span>
<span class=normal><a href=#__codelineno-0-218>218</a></span>
<span class=normal><a href=#__codelineno-0-219>219</a></span>
<span class=normal><a href=#__codelineno-0-220>220</a></span>
<span class=normal><a href=#__codelineno-0-221>221</a></span>
<span class=normal><a href=#__codelineno-0-222>222</a></span>
<span class=normal><a href=#__codelineno-0-223>223</a></span>
<span class=normal><a href=#__codelineno-0-224>224</a></span>
<span class=normal><a href=#__codelineno-0-225>225</a></span>
<span class=normal><a href=#__codelineno-0-226>226</a></span>
<span class=normal><a href=#__codelineno-0-227>227</a></span>
<span class=normal><a href=#__codelineno-0-228>228</a></span>
<span class=normal><a href=#__codelineno-0-229>229</a></span>
<span class=normal><a href=#__codelineno-0-230>230</a></span>
<span class=normal><a href=#__codelineno-0-231>231</a></span>
<span class=normal><a href=#__codelineno-0-232>232</a></span>
<span class=normal><a href=#__codelineno-0-233>233</a></span>
<span class=normal><a href=#__codelineno-0-234>234</a></span>
<span class=normal><a href=#__codelineno-0-235>235</a></span>
<span class=normal><a href=#__codelineno-0-236>236</a></span>
<span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span>
<span class=normal><a href=#__codelineno-0-242>242</a></span>
<span class=normal><a href=#__codelineno-0-243>243</a></span>
<span class=normal><a href=#__codelineno-0-244>244</a></span>
<span class=normal><a href=#__codelineno-0-245>245</a></span>
<span class=normal><a href=#__codelineno-0-246>246</a></span>
<span class=normal><a href=#__codelineno-0-247>247</a></span>
<span class=normal><a href=#__codelineno-0-248>248</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-215 name=__codelineno-0-215></a><span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-216 name=__codelineno-0-216></a>    <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-217 name=__codelineno-0-217></a>    <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-218 name=__codelineno-0-218></a>    <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
<a id=__codelineno-0-219 name=__codelineno-0-219></a>    <span class=n>states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>list</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-220 name=__codelineno-0-220></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>list</span><span class=p>]:</span>
<a id=__codelineno-0-221 name=__codelineno-0-221></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-222 name=__codelineno-0-222></a><span class=sd>    Forward pass for the full Linear Transformer model.</span>
<a id=__codelineno-0-223 name=__codelineno-0-223></a>
<a id=__codelineno-0-224 name=__codelineno-0-224></a><span class=sd>    Args:</span>
<a id=__codelineno-0-225 name=__codelineno-0-225></a><span class=sd>        input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-226 name=__codelineno-0-226></a><span class=sd>        attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-227 name=__codelineno-0-227></a><span class=sd>        states: Optional previous state list for each layer</span>
<a id=__codelineno-0-228 name=__codelineno-0-228></a>
<a id=__codelineno-0-229 name=__codelineno-0-229></a><span class=sd>    Returns:</span>
<a id=__codelineno-0-230 name=__codelineno-0-230></a><span class=sd>        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-231 name=__codelineno-0-231></a><span class=sd>        new_states: Updated state list for each layer</span>
<a id=__codelineno-0-232 name=__codelineno-0-232></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-233 name=__codelineno-0-233></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-234 name=__codelineno-0-234></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-235 name=__codelineno-0-235></a>
<a id=__codelineno-0-236 name=__codelineno-0-236></a>    <span class=c1># Initialize states if not provided</span>
<a id=__codelineno-0-237 name=__codelineno-0-237></a>    <span class=k>if</span> <span class=n>states</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
<a id=__codelineno-0-238 name=__codelineno-0-238></a>        <span class=n>states</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span>
<a id=__codelineno-0-239 name=__codelineno-0-239></a>
<a id=__codelineno-0-240 name=__codelineno-0-240></a>    <span class=n>new_states</span> <span class=o>=</span> <span class=p>[]</span>
<a id=__codelineno-0-241 name=__codelineno-0-241></a>
<a id=__codelineno-0-242 name=__codelineno-0-242></a>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
<a id=__codelineno-0-243 name=__codelineno-0-243></a>        <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_state</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>states</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
<a id=__codelineno-0-244 name=__codelineno-0-244></a>        <span class=n>new_states</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>new_state</span><span class=p>)</span>
<a id=__codelineno-0-245 name=__codelineno-0-245></a>
<a id=__codelineno-0-246 name=__codelineno-0-246></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-247 name=__codelineno-0-247></a>
<a id=__codelineno-0-248 name=__codelineno-0-248></a>    <span class=k>return</span> <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_states</span>
</code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=bsbr_extras.delta_net.DeltaNetModel class="doc doc-heading"> <code>bsbr_extras.delta_net.DeltaNetModel</code> </h2> <div class="doc doc-contents first"> <p class="doc doc-class-bases"> Bases: <code><span title=torch.nn.Module>Module</span></code></p> <p>Full DeltaNet model stacking multiple DeltaNet layers.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>vocab_size</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Vocabulary size for embedding layer</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>hidden_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Hidden dimension size</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_layers</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of DeltaNet layers</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_heads</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of attention heads</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>ff_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Feed-forward intermediate dimension</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>beta</code> </td> <td> <code><span title=float>float</span></code> </td> <td> <div class=doc-md-description> <p>Forgetting/update rate parameter (β in the paper)</p> </div> </td> <td> <code>0.9</code> </td> </tr> <tr class=doc-section-item> <td> <code>dropout</code> </td> <td> <code><span title=float>float</span></code> </td> <td> <div class=doc-md-description> <p>Dropout probability</p> </div> </td> <td> <code>0.1</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/delta_net.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span>
<span class=normal><a href=#__codelineno-0-242>242</a></span>
<span class=normal><a href=#__codelineno-0-243>243</a></span>
<span class=normal><a href=#__codelineno-0-244>244</a></span>
<span class=normal><a href=#__codelineno-0-245>245</a></span>
<span class=normal><a href=#__codelineno-0-246>246</a></span>
<span class=normal><a href=#__codelineno-0-247>247</a></span>
<span class=normal><a href=#__codelineno-0-248>248</a></span>
<span class=normal><a href=#__codelineno-0-249>249</a></span>
<span class=normal><a href=#__codelineno-0-250>250</a></span>
<span class=normal><a href=#__codelineno-0-251>251</a></span>
<span class=normal><a href=#__codelineno-0-252>252</a></span>
<span class=normal><a href=#__codelineno-0-253>253</a></span>
<span class=normal><a href=#__codelineno-0-254>254</a></span>
<span class=normal><a href=#__codelineno-0-255>255</a></span>
<span class=normal><a href=#__codelineno-0-256>256</a></span>
<span class=normal><a href=#__codelineno-0-257>257</a></span>
<span class=normal><a href=#__codelineno-0-258>258</a></span>
<span class=normal><a href=#__codelineno-0-259>259</a></span>
<span class=normal><a href=#__codelineno-0-260>260</a></span>
<span class=normal><a href=#__codelineno-0-261>261</a></span>
<span class=normal><a href=#__codelineno-0-262>262</a></span>
<span class=normal><a href=#__codelineno-0-263>263</a></span>
<span class=normal><a href=#__codelineno-0-264>264</a></span>
<span class=normal><a href=#__codelineno-0-265>265</a></span>
<span class=normal><a href=#__codelineno-0-266>266</a></span>
<span class=normal><a href=#__codelineno-0-267>267</a></span>
<span class=normal><a href=#__codelineno-0-268>268</a></span>
<span class=normal><a href=#__codelineno-0-269>269</a></span>
<span class=normal><a href=#__codelineno-0-270>270</a></span>
<span class=normal><a href=#__codelineno-0-271>271</a></span>
<span class=normal><a href=#__codelineno-0-272>272</a></span>
<span class=normal><a href=#__codelineno-0-273>273</a></span>
<span class=normal><a href=#__codelineno-0-274>274</a></span>
<span class=normal><a href=#__codelineno-0-275>275</a></span>
<span class=normal><a href=#__codelineno-0-276>276</a></span>
<span class=normal><a href=#__codelineno-0-277>277</a></span>
<span class=normal><a href=#__codelineno-0-278>278</a></span>
<span class=normal><a href=#__codelineno-0-279>279</a></span>
<span class=normal><a href=#__codelineno-0-280>280</a></span>
<span class=normal><a href=#__codelineno-0-281>281</a></span>
<span class=normal><a href=#__codelineno-0-282>282</a></span>
<span class=normal><a href=#__codelineno-0-283>283</a></span>
<span class=normal><a href=#__codelineno-0-284>284</a></span>
<span class=normal><a href=#__codelineno-0-285>285</a></span>
<span class=normal><a href=#__codelineno-0-286>286</a></span>
<span class=normal><a href=#__codelineno-0-287>287</a></span>
<span class=normal><a href=#__codelineno-0-288>288</a></span>
<span class=normal><a href=#__codelineno-0-289>289</a></span>
<span class=normal><a href=#__codelineno-0-290>290</a></span>
<span class=normal><a href=#__codelineno-0-291>291</a></span>
<span class=normal><a href=#__codelineno-0-292>292</a></span>
<span class=normal><a href=#__codelineno-0-293>293</a></span>
<span class=normal><a href=#__codelineno-0-294>294</a></span>
<span class=normal><a href=#__codelineno-0-295>295</a></span>
<span class=normal><a href=#__codelineno-0-296>296</a></span>
<span class=normal><a href=#__codelineno-0-297>297</a></span>
<span class=normal><a href=#__codelineno-0-298>298</a></span>
<span class=normal><a href=#__codelineno-0-299>299</a></span>
<span class=normal><a href=#__codelineno-0-300>300</a></span>
<span class=normal><a href=#__codelineno-0-301>301</a></span>
<span class=normal><a href=#__codelineno-0-302>302</a></span>
<span class=normal><a href=#__codelineno-0-303>303</a></span>
<span class=normal><a href=#__codelineno-0-304>304</a></span>
<span class=normal><a href=#__codelineno-0-305>305</a></span>
<span class=normal><a href=#__codelineno-0-306>306</a></span>
<span class=normal><a href=#__codelineno-0-307>307</a></span>
<span class=normal><a href=#__codelineno-0-308>308</a></span>
<span class=normal><a href=#__codelineno-0-309>309</a></span>
<span class=normal><a href=#__codelineno-0-310>310</a></span>
<span class=normal><a href=#__codelineno-0-311>311</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-237 name=__codelineno-0-237></a><span class=k>class</span><span class=w> </span><span class=nc>DeltaNetModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<a id=__codelineno-0-238 name=__codelineno-0-238></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-239 name=__codelineno-0-239></a><span class=sd>    Full DeltaNet model stacking multiple DeltaNet layers.</span>
<a id=__codelineno-0-240 name=__codelineno-0-240></a>
<a id=__codelineno-0-241 name=__codelineno-0-241></a><span class=sd>    Args:</span>
<a id=__codelineno-0-242 name=__codelineno-0-242></a><span class=sd>        vocab_size (int): Vocabulary size for embedding layer</span>
<a id=__codelineno-0-243 name=__codelineno-0-243></a><span class=sd>        hidden_dim (int): Hidden dimension size</span>
<a id=__codelineno-0-244 name=__codelineno-0-244></a><span class=sd>        num_layers (int): Number of DeltaNet layers</span>
<a id=__codelineno-0-245 name=__codelineno-0-245></a><span class=sd>        num_heads (int): Number of attention heads</span>
<a id=__codelineno-0-246 name=__codelineno-0-246></a><span class=sd>        ff_dim (int): Feed-forward intermediate dimension</span>
<a id=__codelineno-0-247 name=__codelineno-0-247></a><span class=sd>        beta (float): Forgetting/update rate parameter (β in the paper)</span>
<a id=__codelineno-0-248 name=__codelineno-0-248></a><span class=sd>        dropout (float): Dropout probability</span>
<a id=__codelineno-0-249 name=__codelineno-0-249></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-250 name=__codelineno-0-250></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span>
<a id=__codelineno-0-251 name=__codelineno-0-251></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-252 name=__codelineno-0-252></a>        <span class=n>vocab_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-253 name=__codelineno-0-253></a>        <span class=n>hidden_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-254 name=__codelineno-0-254></a>        <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-255 name=__codelineno-0-255></a>        <span class=n>num_heads</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-256 name=__codelineno-0-256></a>        <span class=n>ff_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-257 name=__codelineno-0-257></a>        <span class=n>beta</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.9</span><span class=p>,</span>
<a id=__codelineno-0-258 name=__codelineno-0-258></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span>
<a id=__codelineno-0-259 name=__codelineno-0-259></a>    <span class=p>):</span>
<a id=__codelineno-0-260 name=__codelineno-0-260></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
<a id=__codelineno-0-261 name=__codelineno-0-261></a>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-262 name=__codelineno-0-262></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
<a id=__codelineno-0-263 name=__codelineno-0-263></a>
<a id=__codelineno-0-264 name=__codelineno-0-264></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
<a id=__codelineno-0-265 name=__codelineno-0-265></a>            <span class=n>DeltaNetLayer</span><span class=p>(</span>
<a id=__codelineno-0-266 name=__codelineno-0-266></a>                <span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span>
<a id=__codelineno-0-267 name=__codelineno-0-267></a>                <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>,</span>
<a id=__codelineno-0-268 name=__codelineno-0-268></a>                <span class=n>ff_dim</span><span class=o>=</span><span class=n>ff_dim</span><span class=p>,</span>
<a id=__codelineno-0-269 name=__codelineno-0-269></a>                <span class=n>beta</span><span class=o>=</span><span class=n>beta</span><span class=p>,</span>
<a id=__codelineno-0-270 name=__codelineno-0-270></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span>
<a id=__codelineno-0-271 name=__codelineno-0-271></a>            <span class=p>)</span>
<a id=__codelineno-0-272 name=__codelineno-0-272></a>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)</span>
<a id=__codelineno-0-273 name=__codelineno-0-273></a>        <span class=p>])</span>
<a id=__codelineno-0-274 name=__codelineno-0-274></a>
<a id=__codelineno-0-275 name=__codelineno-0-275></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-276 name=__codelineno-0-276></a>        <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span> <span class=o>=</span> <span class=n>num_layers</span>
<a id=__codelineno-0-277 name=__codelineno-0-277></a>
<a id=__codelineno-0-278 name=__codelineno-0-278></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-279 name=__codelineno-0-279></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-280 name=__codelineno-0-280></a>        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-281 name=__codelineno-0-281></a>        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
<a id=__codelineno-0-282 name=__codelineno-0-282></a>        <span class=n>states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>list</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-283 name=__codelineno-0-283></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>list</span><span class=p>]:</span>
<a id=__codelineno-0-284 name=__codelineno-0-284></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-285 name=__codelineno-0-285></a><span class=sd>        Forward pass for the full DeltaNet model.</span>
<a id=__codelineno-0-286 name=__codelineno-0-286></a>
<a id=__codelineno-0-287 name=__codelineno-0-287></a><span class=sd>        Args:</span>
<a id=__codelineno-0-288 name=__codelineno-0-288></a><span class=sd>            input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-289 name=__codelineno-0-289></a><span class=sd>            attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-290 name=__codelineno-0-290></a><span class=sd>            states: Optional previous state list for each layer</span>
<a id=__codelineno-0-291 name=__codelineno-0-291></a>
<a id=__codelineno-0-292 name=__codelineno-0-292></a><span class=sd>        Returns:</span>
<a id=__codelineno-0-293 name=__codelineno-0-293></a><span class=sd>            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-294 name=__codelineno-0-294></a><span class=sd>            new_states: Updated state list for each layer</span>
<a id=__codelineno-0-295 name=__codelineno-0-295></a><span class=sd>        &quot;&quot;&quot;</span>
<a id=__codelineno-0-296 name=__codelineno-0-296></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-297 name=__codelineno-0-297></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-298 name=__codelineno-0-298></a>
<a id=__codelineno-0-299 name=__codelineno-0-299></a>        <span class=c1># Initialize states if not provided</span>
<a id=__codelineno-0-300 name=__codelineno-0-300></a>        <span class=k>if</span> <span class=n>states</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
<a id=__codelineno-0-301 name=__codelineno-0-301></a>            <span class=n>states</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span>
<a id=__codelineno-0-302 name=__codelineno-0-302></a>
<a id=__codelineno-0-303 name=__codelineno-0-303></a>        <span class=n>new_states</span> <span class=o>=</span> <span class=p>[]</span>
<a id=__codelineno-0-304 name=__codelineno-0-304></a>
<a id=__codelineno-0-305 name=__codelineno-0-305></a>        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
<a id=__codelineno-0-306 name=__codelineno-0-306></a>            <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_state</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>states</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
<a id=__codelineno-0-307 name=__codelineno-0-307></a>            <span class=n>new_states</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>new_state</span><span class=p>)</span>
<a id=__codelineno-0-308 name=__codelineno-0-308></a>
<a id=__codelineno-0-309 name=__codelineno-0-309></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-310 name=__codelineno-0-310></a>
<a id=__codelineno-0-311 name=__codelineno-0-311></a>        <span class=k>return</span> <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_states</span>
</code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=bsbr_extras.delta_net.DeltaNetModel.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>states</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> </h3> <div class="doc doc-contents "> <p>Forward pass for the full DeltaNet model.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>input_ids</code> </td> <td> <code><span title=torch.LongTensor>LongTensor</span></code> </td> <td> <div class=doc-md-description> <p>Token IDs of shape [batch_size, seq_len]</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>attention_mask</code> </td> <td> <code><span title=typing.Optional>Optional</span>[<span title=torch.Tensor>Tensor</span>]</code> </td> <td> <div class=doc-md-description> <p>Optional attention mask of shape [batch_size, seq_len]</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td> <code>states</code> </td> <td> <code><span title=typing.Optional>Optional</span>[<span title=list>list</span>]</code> </td> <td> <div class=doc-md-description> <p>Optional previous state list for each layer</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <p><span class=doc-section-title>Returns:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>output</code></td> <td> <code><span title=torch.Tensor>Tensor</span></code> </td> <td> <div class=doc-md-description> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> </div> </td> </tr> <tr class=doc-section-item> <td><code>new_states</code></td> <td> <code><span title=list>list</span></code> </td> <td> <div class=doc-md-description> <p>Updated state list for each layer</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/delta_net.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-278>278</a></span>
<span class=normal><a href=#__codelineno-0-279>279</a></span>
<span class=normal><a href=#__codelineno-0-280>280</a></span>
<span class=normal><a href=#__codelineno-0-281>281</a></span>
<span class=normal><a href=#__codelineno-0-282>282</a></span>
<span class=normal><a href=#__codelineno-0-283>283</a></span>
<span class=normal><a href=#__codelineno-0-284>284</a></span>
<span class=normal><a href=#__codelineno-0-285>285</a></span>
<span class=normal><a href=#__codelineno-0-286>286</a></span>
<span class=normal><a href=#__codelineno-0-287>287</a></span>
<span class=normal><a href=#__codelineno-0-288>288</a></span>
<span class=normal><a href=#__codelineno-0-289>289</a></span>
<span class=normal><a href=#__codelineno-0-290>290</a></span>
<span class=normal><a href=#__codelineno-0-291>291</a></span>
<span class=normal><a href=#__codelineno-0-292>292</a></span>
<span class=normal><a href=#__codelineno-0-293>293</a></span>
<span class=normal><a href=#__codelineno-0-294>294</a></span>
<span class=normal><a href=#__codelineno-0-295>295</a></span>
<span class=normal><a href=#__codelineno-0-296>296</a></span>
<span class=normal><a href=#__codelineno-0-297>297</a></span>
<span class=normal><a href=#__codelineno-0-298>298</a></span>
<span class=normal><a href=#__codelineno-0-299>299</a></span>
<span class=normal><a href=#__codelineno-0-300>300</a></span>
<span class=normal><a href=#__codelineno-0-301>301</a></span>
<span class=normal><a href=#__codelineno-0-302>302</a></span>
<span class=normal><a href=#__codelineno-0-303>303</a></span>
<span class=normal><a href=#__codelineno-0-304>304</a></span>
<span class=normal><a href=#__codelineno-0-305>305</a></span>
<span class=normal><a href=#__codelineno-0-306>306</a></span>
<span class=normal><a href=#__codelineno-0-307>307</a></span>
<span class=normal><a href=#__codelineno-0-308>308</a></span>
<span class=normal><a href=#__codelineno-0-309>309</a></span>
<span class=normal><a href=#__codelineno-0-310>310</a></span>
<span class=normal><a href=#__codelineno-0-311>311</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-278 name=__codelineno-0-278></a><span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-279 name=__codelineno-0-279></a>    <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-280 name=__codelineno-0-280></a>    <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-281 name=__codelineno-0-281></a>    <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
<a id=__codelineno-0-282 name=__codelineno-0-282></a>    <span class=n>states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>list</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-283 name=__codelineno-0-283></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>list</span><span class=p>]:</span>
<a id=__codelineno-0-284 name=__codelineno-0-284></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-285 name=__codelineno-0-285></a><span class=sd>    Forward pass for the full DeltaNet model.</span>
<a id=__codelineno-0-286 name=__codelineno-0-286></a>
<a id=__codelineno-0-287 name=__codelineno-0-287></a><span class=sd>    Args:</span>
<a id=__codelineno-0-288 name=__codelineno-0-288></a><span class=sd>        input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-289 name=__codelineno-0-289></a><span class=sd>        attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-290 name=__codelineno-0-290></a><span class=sd>        states: Optional previous state list for each layer</span>
<a id=__codelineno-0-291 name=__codelineno-0-291></a>
<a id=__codelineno-0-292 name=__codelineno-0-292></a><span class=sd>    Returns:</span>
<a id=__codelineno-0-293 name=__codelineno-0-293></a><span class=sd>        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-294 name=__codelineno-0-294></a><span class=sd>        new_states: Updated state list for each layer</span>
<a id=__codelineno-0-295 name=__codelineno-0-295></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-296 name=__codelineno-0-296></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-297 name=__codelineno-0-297></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-298 name=__codelineno-0-298></a>
<a id=__codelineno-0-299 name=__codelineno-0-299></a>    <span class=c1># Initialize states if not provided</span>
<a id=__codelineno-0-300 name=__codelineno-0-300></a>    <span class=k>if</span> <span class=n>states</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
<a id=__codelineno-0-301 name=__codelineno-0-301></a>        <span class=n>states</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span>
<a id=__codelineno-0-302 name=__codelineno-0-302></a>
<a id=__codelineno-0-303 name=__codelineno-0-303></a>    <span class=n>new_states</span> <span class=o>=</span> <span class=p>[]</span>
<a id=__codelineno-0-304 name=__codelineno-0-304></a>
<a id=__codelineno-0-305 name=__codelineno-0-305></a>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
<a id=__codelineno-0-306 name=__codelineno-0-306></a>        <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_state</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>states</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
<a id=__codelineno-0-307 name=__codelineno-0-307></a>        <span class=n>new_states</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>new_state</span><span class=p>)</span>
<a id=__codelineno-0-308 name=__codelineno-0-308></a>
<a id=__codelineno-0-309 name=__codelineno-0-309></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-310 name=__codelineno-0-310></a>
<a id=__codelineno-0-311 name=__codelineno-0-311></a>    <span class=k>return</span> <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_states</span>
</code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=bsbr_extras.gau.GAUModel class="doc doc-heading"> <code>bsbr_extras.gau.GAUModel</code> </h2> <div class="doc doc-contents first"> <p class="doc doc-class-bases"> Bases: <code><span title=torch.nn.Module>Module</span></code></p> <p>Full Gated Attention Unit model stacking multiple GAU layers.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>vocab_size</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Vocabulary size for embedding layer</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>hidden_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Hidden dimension size</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_layers</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of GAU layers</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>chunk_size</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Size of chunks for parallel processing</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>ff_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Feed-forward intermediate dimension</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>expansion_factor</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Expansion factor for GAU</p> </div> </td> <td> <code>2</code> </td> </tr> <tr class=doc-section-item> <td> <code>dropout</code> </td> <td> <code><span title=float>float</span></code> </td> <td> <div class=doc-md-description> <p>Dropout probability</p> </div> </td> <td> <code>0.1</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/gau.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-247>247</a></span>
<span class=normal><a href=#__codelineno-0-248>248</a></span>
<span class=normal><a href=#__codelineno-0-249>249</a></span>
<span class=normal><a href=#__codelineno-0-250>250</a></span>
<span class=normal><a href=#__codelineno-0-251>251</a></span>
<span class=normal><a href=#__codelineno-0-252>252</a></span>
<span class=normal><a href=#__codelineno-0-253>253</a></span>
<span class=normal><a href=#__codelineno-0-254>254</a></span>
<span class=normal><a href=#__codelineno-0-255>255</a></span>
<span class=normal><a href=#__codelineno-0-256>256</a></span>
<span class=normal><a href=#__codelineno-0-257>257</a></span>
<span class=normal><a href=#__codelineno-0-258>258</a></span>
<span class=normal><a href=#__codelineno-0-259>259</a></span>
<span class=normal><a href=#__codelineno-0-260>260</a></span>
<span class=normal><a href=#__codelineno-0-261>261</a></span>
<span class=normal><a href=#__codelineno-0-262>262</a></span>
<span class=normal><a href=#__codelineno-0-263>263</a></span>
<span class=normal><a href=#__codelineno-0-264>264</a></span>
<span class=normal><a href=#__codelineno-0-265>265</a></span>
<span class=normal><a href=#__codelineno-0-266>266</a></span>
<span class=normal><a href=#__codelineno-0-267>267</a></span>
<span class=normal><a href=#__codelineno-0-268>268</a></span>
<span class=normal><a href=#__codelineno-0-269>269</a></span>
<span class=normal><a href=#__codelineno-0-270>270</a></span>
<span class=normal><a href=#__codelineno-0-271>271</a></span>
<span class=normal><a href=#__codelineno-0-272>272</a></span>
<span class=normal><a href=#__codelineno-0-273>273</a></span>
<span class=normal><a href=#__codelineno-0-274>274</a></span>
<span class=normal><a href=#__codelineno-0-275>275</a></span>
<span class=normal><a href=#__codelineno-0-276>276</a></span>
<span class=normal><a href=#__codelineno-0-277>277</a></span>
<span class=normal><a href=#__codelineno-0-278>278</a></span>
<span class=normal><a href=#__codelineno-0-279>279</a></span>
<span class=normal><a href=#__codelineno-0-280>280</a></span>
<span class=normal><a href=#__codelineno-0-281>281</a></span>
<span class=normal><a href=#__codelineno-0-282>282</a></span>
<span class=normal><a href=#__codelineno-0-283>283</a></span>
<span class=normal><a href=#__codelineno-0-284>284</a></span>
<span class=normal><a href=#__codelineno-0-285>285</a></span>
<span class=normal><a href=#__codelineno-0-286>286</a></span>
<span class=normal><a href=#__codelineno-0-287>287</a></span>
<span class=normal><a href=#__codelineno-0-288>288</a></span>
<span class=normal><a href=#__codelineno-0-289>289</a></span>
<span class=normal><a href=#__codelineno-0-290>290</a></span>
<span class=normal><a href=#__codelineno-0-291>291</a></span>
<span class=normal><a href=#__codelineno-0-292>292</a></span>
<span class=normal><a href=#__codelineno-0-293>293</a></span>
<span class=normal><a href=#__codelineno-0-294>294</a></span>
<span class=normal><a href=#__codelineno-0-295>295</a></span>
<span class=normal><a href=#__codelineno-0-296>296</a></span>
<span class=normal><a href=#__codelineno-0-297>297</a></span>
<span class=normal><a href=#__codelineno-0-298>298</a></span>
<span class=normal><a href=#__codelineno-0-299>299</a></span>
<span class=normal><a href=#__codelineno-0-300>300</a></span>
<span class=normal><a href=#__codelineno-0-301>301</a></span>
<span class=normal><a href=#__codelineno-0-302>302</a></span>
<span class=normal><a href=#__codelineno-0-303>303</a></span>
<span class=normal><a href=#__codelineno-0-304>304</a></span>
<span class=normal><a href=#__codelineno-0-305>305</a></span>
<span class=normal><a href=#__codelineno-0-306>306</a></span>
<span class=normal><a href=#__codelineno-0-307>307</a></span>
<span class=normal><a href=#__codelineno-0-308>308</a></span>
<span class=normal><a href=#__codelineno-0-309>309</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-247 name=__codelineno-0-247></a><span class=k>class</span><span class=w> </span><span class=nc>GAUModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<a id=__codelineno-0-248 name=__codelineno-0-248></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-249 name=__codelineno-0-249></a><span class=sd>    Full Gated Attention Unit model stacking multiple GAU layers.</span>
<a id=__codelineno-0-250 name=__codelineno-0-250></a>
<a id=__codelineno-0-251 name=__codelineno-0-251></a><span class=sd>    Args:</span>
<a id=__codelineno-0-252 name=__codelineno-0-252></a><span class=sd>        vocab_size (int): Vocabulary size for embedding layer</span>
<a id=__codelineno-0-253 name=__codelineno-0-253></a><span class=sd>        hidden_dim (int): Hidden dimension size</span>
<a id=__codelineno-0-254 name=__codelineno-0-254></a><span class=sd>        num_layers (int): Number of GAU layers</span>
<a id=__codelineno-0-255 name=__codelineno-0-255></a><span class=sd>        chunk_size (int): Size of chunks for parallel processing</span>
<a id=__codelineno-0-256 name=__codelineno-0-256></a><span class=sd>        ff_dim (int): Feed-forward intermediate dimension</span>
<a id=__codelineno-0-257 name=__codelineno-0-257></a><span class=sd>        expansion_factor (int): Expansion factor for GAU</span>
<a id=__codelineno-0-258 name=__codelineno-0-258></a><span class=sd>        dropout (float): Dropout probability</span>
<a id=__codelineno-0-259 name=__codelineno-0-259></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-260 name=__codelineno-0-260></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span>
<a id=__codelineno-0-261 name=__codelineno-0-261></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-262 name=__codelineno-0-262></a>        <span class=n>vocab_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-263 name=__codelineno-0-263></a>        <span class=n>hidden_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-264 name=__codelineno-0-264></a>        <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-265 name=__codelineno-0-265></a>        <span class=n>chunk_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-266 name=__codelineno-0-266></a>        <span class=n>ff_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-267 name=__codelineno-0-267></a>        <span class=n>expansion_factor</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>2</span><span class=p>,</span>
<a id=__codelineno-0-268 name=__codelineno-0-268></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span>
<a id=__codelineno-0-269 name=__codelineno-0-269></a>    <span class=p>):</span>
<a id=__codelineno-0-270 name=__codelineno-0-270></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
<a id=__codelineno-0-271 name=__codelineno-0-271></a>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-272 name=__codelineno-0-272></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
<a id=__codelineno-0-273 name=__codelineno-0-273></a>
<a id=__codelineno-0-274 name=__codelineno-0-274></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
<a id=__codelineno-0-275 name=__codelineno-0-275></a>            <span class=n>GAULayer</span><span class=p>(</span>
<a id=__codelineno-0-276 name=__codelineno-0-276></a>                <span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span>
<a id=__codelineno-0-277 name=__codelineno-0-277></a>                <span class=n>chunk_size</span><span class=o>=</span><span class=n>chunk_size</span><span class=p>,</span>
<a id=__codelineno-0-278 name=__codelineno-0-278></a>                <span class=n>ff_dim</span><span class=o>=</span><span class=n>ff_dim</span><span class=p>,</span>
<a id=__codelineno-0-279 name=__codelineno-0-279></a>                <span class=n>expansion_factor</span><span class=o>=</span><span class=n>expansion_factor</span><span class=p>,</span>
<a id=__codelineno-0-280 name=__codelineno-0-280></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span>
<a id=__codelineno-0-281 name=__codelineno-0-281></a>            <span class=p>)</span>
<a id=__codelineno-0-282 name=__codelineno-0-282></a>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)</span>
<a id=__codelineno-0-283 name=__codelineno-0-283></a>        <span class=p>])</span>
<a id=__codelineno-0-284 name=__codelineno-0-284></a>
<a id=__codelineno-0-285 name=__codelineno-0-285></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-286 name=__codelineno-0-286></a>
<a id=__codelineno-0-287 name=__codelineno-0-287></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-288 name=__codelineno-0-288></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-289 name=__codelineno-0-289></a>        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-290 name=__codelineno-0-290></a>        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-291 name=__codelineno-0-291></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
<a id=__codelineno-0-292 name=__codelineno-0-292></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-293 name=__codelineno-0-293></a><span class=sd>        Forward pass for the full GAU model.</span>
<a id=__codelineno-0-294 name=__codelineno-0-294></a>
<a id=__codelineno-0-295 name=__codelineno-0-295></a><span class=sd>        Args:</span>
<a id=__codelineno-0-296 name=__codelineno-0-296></a><span class=sd>            input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-297 name=__codelineno-0-297></a><span class=sd>            attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-298 name=__codelineno-0-298></a>
<a id=__codelineno-0-299 name=__codelineno-0-299></a><span class=sd>        Returns:</span>
<a id=__codelineno-0-300 name=__codelineno-0-300></a><span class=sd>            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-301 name=__codelineno-0-301></a><span class=sd>        &quot;&quot;&quot;</span>
<a id=__codelineno-0-302 name=__codelineno-0-302></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-303 name=__codelineno-0-303></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-304 name=__codelineno-0-304></a>
<a id=__codelineno-0-305 name=__codelineno-0-305></a>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
<a id=__codelineno-0-306 name=__codelineno-0-306></a>            <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>
<a id=__codelineno-0-307 name=__codelineno-0-307></a>
<a id=__codelineno-0-308 name=__codelineno-0-308></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-309 name=__codelineno-0-309></a>        <span class=k>return</span> <span class=n>hidden_states</span>
</code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=bsbr_extras.gau.GAUModel.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> </h3> <div class="doc doc-contents "> <p>Forward pass for the full GAU model.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>input_ids</code> </td> <td> <code><span title=torch.LongTensor>LongTensor</span></code> </td> <td> <div class=doc-md-description> <p>Token IDs of shape [batch_size, seq_len]</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>attention_mask</code> </td> <td> <code><span title=typing.Optional>Optional</span>[<span title=torch.Tensor>Tensor</span>]</code> </td> <td> <div class=doc-md-description> <p>Optional attention mask of shape [batch_size, seq_len]</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <p><span class=doc-section-title>Returns:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>output</code></td> <td> <code><span title=torch.Tensor>Tensor</span></code> </td> <td> <div class=doc-md-description> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/gau.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-287>287</a></span>
<span class=normal><a href=#__codelineno-0-288>288</a></span>
<span class=normal><a href=#__codelineno-0-289>289</a></span>
<span class=normal><a href=#__codelineno-0-290>290</a></span>
<span class=normal><a href=#__codelineno-0-291>291</a></span>
<span class=normal><a href=#__codelineno-0-292>292</a></span>
<span class=normal><a href=#__codelineno-0-293>293</a></span>
<span class=normal><a href=#__codelineno-0-294>294</a></span>
<span class=normal><a href=#__codelineno-0-295>295</a></span>
<span class=normal><a href=#__codelineno-0-296>296</a></span>
<span class=normal><a href=#__codelineno-0-297>297</a></span>
<span class=normal><a href=#__codelineno-0-298>298</a></span>
<span class=normal><a href=#__codelineno-0-299>299</a></span>
<span class=normal><a href=#__codelineno-0-300>300</a></span>
<span class=normal><a href=#__codelineno-0-301>301</a></span>
<span class=normal><a href=#__codelineno-0-302>302</a></span>
<span class=normal><a href=#__codelineno-0-303>303</a></span>
<span class=normal><a href=#__codelineno-0-304>304</a></span>
<span class=normal><a href=#__codelineno-0-305>305</a></span>
<span class=normal><a href=#__codelineno-0-306>306</a></span>
<span class=normal><a href=#__codelineno-0-307>307</a></span>
<span class=normal><a href=#__codelineno-0-308>308</a></span>
<span class=normal><a href=#__codelineno-0-309>309</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-287 name=__codelineno-0-287></a><span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-288 name=__codelineno-0-288></a>    <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-289 name=__codelineno-0-289></a>    <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-290 name=__codelineno-0-290></a>    <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-291 name=__codelineno-0-291></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
<a id=__codelineno-0-292 name=__codelineno-0-292></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-293 name=__codelineno-0-293></a><span class=sd>    Forward pass for the full GAU model.</span>
<a id=__codelineno-0-294 name=__codelineno-0-294></a>
<a id=__codelineno-0-295 name=__codelineno-0-295></a><span class=sd>    Args:</span>
<a id=__codelineno-0-296 name=__codelineno-0-296></a><span class=sd>        input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-297 name=__codelineno-0-297></a><span class=sd>        attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-298 name=__codelineno-0-298></a>
<a id=__codelineno-0-299 name=__codelineno-0-299></a><span class=sd>    Returns:</span>
<a id=__codelineno-0-300 name=__codelineno-0-300></a><span class=sd>        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-301 name=__codelineno-0-301></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-302 name=__codelineno-0-302></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-303 name=__codelineno-0-303></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-304 name=__codelineno-0-304></a>
<a id=__codelineno-0-305 name=__codelineno-0-305></a>    <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
<a id=__codelineno-0-306 name=__codelineno-0-306></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>
<a id=__codelineno-0-307 name=__codelineno-0-307></a>
<a id=__codelineno-0-308 name=__codelineno-0-308></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-309 name=__codelineno-0-309></a>    <span class=k>return</span> <span class=n>hidden_states</span>
</code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=bsbr_extras.hopfield_network.HopfieldNetworkModel class="doc doc-heading"> <code>bsbr_extras.hopfield_network.HopfieldNetworkModel</code> </h2> <div class="doc doc-contents first"> <p class="doc doc-class-bases"> Bases: <code><span title=torch.nn.Module>Module</span></code></p> <p>Full Hopfield Network model stacking multiple Hopfield Network layers.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>vocab_size</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Vocabulary size for embedding layer</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>hidden_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Hidden dimension size</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_layers</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of Hopfield Network layers</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_heads</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of attention heads</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>ff_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Feed-forward intermediate dimension</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>temperature</code> </td> <td> <code><span title=float>float</span></code> </td> <td> <div class=doc-md-description> <p>Temperature parameter for the Hopfield energy function</p> </div> </td> <td> <code>1.0</code> </td> </tr> <tr class=doc-section-item> <td> <code>dropout</code> </td> <td> <code><span title=float>float</span></code> </td> <td> <div class=doc-md-description> <p>Dropout probability</p> </div> </td> <td> <code>0.1</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/hopfield_network.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-195>195</a></span>
<span class=normal><a href=#__codelineno-0-196>196</a></span>
<span class=normal><a href=#__codelineno-0-197>197</a></span>
<span class=normal><a href=#__codelineno-0-198>198</a></span>
<span class=normal><a href=#__codelineno-0-199>199</a></span>
<span class=normal><a href=#__codelineno-0-200>200</a></span>
<span class=normal><a href=#__codelineno-0-201>201</a></span>
<span class=normal><a href=#__codelineno-0-202>202</a></span>
<span class=normal><a href=#__codelineno-0-203>203</a></span>
<span class=normal><a href=#__codelineno-0-204>204</a></span>
<span class=normal><a href=#__codelineno-0-205>205</a></span>
<span class=normal><a href=#__codelineno-0-206>206</a></span>
<span class=normal><a href=#__codelineno-0-207>207</a></span>
<span class=normal><a href=#__codelineno-0-208>208</a></span>
<span class=normal><a href=#__codelineno-0-209>209</a></span>
<span class=normal><a href=#__codelineno-0-210>210</a></span>
<span class=normal><a href=#__codelineno-0-211>211</a></span>
<span class=normal><a href=#__codelineno-0-212>212</a></span>
<span class=normal><a href=#__codelineno-0-213>213</a></span>
<span class=normal><a href=#__codelineno-0-214>214</a></span>
<span class=normal><a href=#__codelineno-0-215>215</a></span>
<span class=normal><a href=#__codelineno-0-216>216</a></span>
<span class=normal><a href=#__codelineno-0-217>217</a></span>
<span class=normal><a href=#__codelineno-0-218>218</a></span>
<span class=normal><a href=#__codelineno-0-219>219</a></span>
<span class=normal><a href=#__codelineno-0-220>220</a></span>
<span class=normal><a href=#__codelineno-0-221>221</a></span>
<span class=normal><a href=#__codelineno-0-222>222</a></span>
<span class=normal><a href=#__codelineno-0-223>223</a></span>
<span class=normal><a href=#__codelineno-0-224>224</a></span>
<span class=normal><a href=#__codelineno-0-225>225</a></span>
<span class=normal><a href=#__codelineno-0-226>226</a></span>
<span class=normal><a href=#__codelineno-0-227>227</a></span>
<span class=normal><a href=#__codelineno-0-228>228</a></span>
<span class=normal><a href=#__codelineno-0-229>229</a></span>
<span class=normal><a href=#__codelineno-0-230>230</a></span>
<span class=normal><a href=#__codelineno-0-231>231</a></span>
<span class=normal><a href=#__codelineno-0-232>232</a></span>
<span class=normal><a href=#__codelineno-0-233>233</a></span>
<span class=normal><a href=#__codelineno-0-234>234</a></span>
<span class=normal><a href=#__codelineno-0-235>235</a></span>
<span class=normal><a href=#__codelineno-0-236>236</a></span>
<span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span>
<span class=normal><a href=#__codelineno-0-242>242</a></span>
<span class=normal><a href=#__codelineno-0-243>243</a></span>
<span class=normal><a href=#__codelineno-0-244>244</a></span>
<span class=normal><a href=#__codelineno-0-245>245</a></span>
<span class=normal><a href=#__codelineno-0-246>246</a></span>
<span class=normal><a href=#__codelineno-0-247>247</a></span>
<span class=normal><a href=#__codelineno-0-248>248</a></span>
<span class=normal><a href=#__codelineno-0-249>249</a></span>
<span class=normal><a href=#__codelineno-0-250>250</a></span>
<span class=normal><a href=#__codelineno-0-251>251</a></span>
<span class=normal><a href=#__codelineno-0-252>252</a></span>
<span class=normal><a href=#__codelineno-0-253>253</a></span>
<span class=normal><a href=#__codelineno-0-254>254</a></span>
<span class=normal><a href=#__codelineno-0-255>255</a></span>
<span class=normal><a href=#__codelineno-0-256>256</a></span>
<span class=normal><a href=#__codelineno-0-257>257</a></span>
<span class=normal><a href=#__codelineno-0-258>258</a></span>
<span class=normal><a href=#__codelineno-0-259>259</a></span>
<span class=normal><a href=#__codelineno-0-260>260</a></span>
<span class=normal><a href=#__codelineno-0-261>261</a></span>
<span class=normal><a href=#__codelineno-0-262>262</a></span>
<span class=normal><a href=#__codelineno-0-263>263</a></span>
<span class=normal><a href=#__codelineno-0-264>264</a></span>
<span class=normal><a href=#__codelineno-0-265>265</a></span>
<span class=normal><a href=#__codelineno-0-266>266</a></span>
<span class=normal><a href=#__codelineno-0-267>267</a></span>
<span class=normal><a href=#__codelineno-0-268>268</a></span>
<span class=normal><a href=#__codelineno-0-269>269</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-195 name=__codelineno-0-195></a><span class=k>class</span><span class=w> </span><span class=nc>HopfieldNetworkModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<a id=__codelineno-0-196 name=__codelineno-0-196></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-197 name=__codelineno-0-197></a><span class=sd>    Full Hopfield Network model stacking multiple Hopfield Network layers.</span>
<a id=__codelineno-0-198 name=__codelineno-0-198></a>
<a id=__codelineno-0-199 name=__codelineno-0-199></a><span class=sd>    Args:</span>
<a id=__codelineno-0-200 name=__codelineno-0-200></a><span class=sd>        vocab_size (int): Vocabulary size for embedding layer</span>
<a id=__codelineno-0-201 name=__codelineno-0-201></a><span class=sd>        hidden_dim (int): Hidden dimension size</span>
<a id=__codelineno-0-202 name=__codelineno-0-202></a><span class=sd>        num_layers (int): Number of Hopfield Network layers</span>
<a id=__codelineno-0-203 name=__codelineno-0-203></a><span class=sd>        num_heads (int): Number of attention heads</span>
<a id=__codelineno-0-204 name=__codelineno-0-204></a><span class=sd>        ff_dim (int): Feed-forward intermediate dimension</span>
<a id=__codelineno-0-205 name=__codelineno-0-205></a><span class=sd>        temperature (float): Temperature parameter for the Hopfield energy function</span>
<a id=__codelineno-0-206 name=__codelineno-0-206></a><span class=sd>        dropout (float): Dropout probability</span>
<a id=__codelineno-0-207 name=__codelineno-0-207></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-208 name=__codelineno-0-208></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span>
<a id=__codelineno-0-209 name=__codelineno-0-209></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-210 name=__codelineno-0-210></a>        <span class=n>vocab_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-211 name=__codelineno-0-211></a>        <span class=n>hidden_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-212 name=__codelineno-0-212></a>        <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-213 name=__codelineno-0-213></a>        <span class=n>num_heads</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-214 name=__codelineno-0-214></a>        <span class=n>ff_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-215 name=__codelineno-0-215></a>        <span class=n>temperature</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>,</span>
<a id=__codelineno-0-216 name=__codelineno-0-216></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span>
<a id=__codelineno-0-217 name=__codelineno-0-217></a>    <span class=p>):</span>
<a id=__codelineno-0-218 name=__codelineno-0-218></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
<a id=__codelineno-0-219 name=__codelineno-0-219></a>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-220 name=__codelineno-0-220></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
<a id=__codelineno-0-221 name=__codelineno-0-221></a>
<a id=__codelineno-0-222 name=__codelineno-0-222></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
<a id=__codelineno-0-223 name=__codelineno-0-223></a>            <span class=n>HopfieldNetworkLayer</span><span class=p>(</span>
<a id=__codelineno-0-224 name=__codelineno-0-224></a>                <span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span>
<a id=__codelineno-0-225 name=__codelineno-0-225></a>                <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>,</span>
<a id=__codelineno-0-226 name=__codelineno-0-226></a>                <span class=n>ff_dim</span><span class=o>=</span><span class=n>ff_dim</span><span class=p>,</span>
<a id=__codelineno-0-227 name=__codelineno-0-227></a>                <span class=n>temperature</span><span class=o>=</span><span class=n>temperature</span><span class=p>,</span>
<a id=__codelineno-0-228 name=__codelineno-0-228></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span>
<a id=__codelineno-0-229 name=__codelineno-0-229></a>            <span class=p>)</span>
<a id=__codelineno-0-230 name=__codelineno-0-230></a>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)</span>
<a id=__codelineno-0-231 name=__codelineno-0-231></a>        <span class=p>])</span>
<a id=__codelineno-0-232 name=__codelineno-0-232></a>
<a id=__codelineno-0-233 name=__codelineno-0-233></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-234 name=__codelineno-0-234></a>        <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span> <span class=o>=</span> <span class=n>num_layers</span>
<a id=__codelineno-0-235 name=__codelineno-0-235></a>
<a id=__codelineno-0-236 name=__codelineno-0-236></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-237 name=__codelineno-0-237></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-238 name=__codelineno-0-238></a>        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-239 name=__codelineno-0-239></a>        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
<a id=__codelineno-0-240 name=__codelineno-0-240></a>        <span class=n>states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>list</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-241 name=__codelineno-0-241></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>list</span><span class=p>]:</span>
<a id=__codelineno-0-242 name=__codelineno-0-242></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-243 name=__codelineno-0-243></a><span class=sd>        Forward pass for the full Hopfield Network model.</span>
<a id=__codelineno-0-244 name=__codelineno-0-244></a>
<a id=__codelineno-0-245 name=__codelineno-0-245></a><span class=sd>        Args:</span>
<a id=__codelineno-0-246 name=__codelineno-0-246></a><span class=sd>            input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-247 name=__codelineno-0-247></a><span class=sd>            attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-248 name=__codelineno-0-248></a><span class=sd>            states: Optional previous states list for each layer</span>
<a id=__codelineno-0-249 name=__codelineno-0-249></a>
<a id=__codelineno-0-250 name=__codelineno-0-250></a><span class=sd>        Returns:</span>
<a id=__codelineno-0-251 name=__codelineno-0-251></a><span class=sd>            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-252 name=__codelineno-0-252></a><span class=sd>            new_states: Updated states list for each layer</span>
<a id=__codelineno-0-253 name=__codelineno-0-253></a><span class=sd>        &quot;&quot;&quot;</span>
<a id=__codelineno-0-254 name=__codelineno-0-254></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-255 name=__codelineno-0-255></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-256 name=__codelineno-0-256></a>
<a id=__codelineno-0-257 name=__codelineno-0-257></a>        <span class=c1># Initialize states if not provided</span>
<a id=__codelineno-0-258 name=__codelineno-0-258></a>        <span class=k>if</span> <span class=n>states</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
<a id=__codelineno-0-259 name=__codelineno-0-259></a>            <span class=n>states</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span>
<a id=__codelineno-0-260 name=__codelineno-0-260></a>
<a id=__codelineno-0-261 name=__codelineno-0-261></a>        <span class=n>new_states</span> <span class=o>=</span> <span class=p>[]</span>
<a id=__codelineno-0-262 name=__codelineno-0-262></a>
<a id=__codelineno-0-263 name=__codelineno-0-263></a>        <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
<a id=__codelineno-0-264 name=__codelineno-0-264></a>            <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_state</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>states</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
<a id=__codelineno-0-265 name=__codelineno-0-265></a>            <span class=n>new_states</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>new_state</span><span class=p>)</span>
<a id=__codelineno-0-266 name=__codelineno-0-266></a>
<a id=__codelineno-0-267 name=__codelineno-0-267></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-268 name=__codelineno-0-268></a>
<a id=__codelineno-0-269 name=__codelineno-0-269></a>        <span class=k>return</span> <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_states</span>
</code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=bsbr_extras.hopfield_network.HopfieldNetworkModel.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>states</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> </h3> <div class="doc doc-contents "> <p>Forward pass for the full Hopfield Network model.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>input_ids</code> </td> <td> <code><span title=torch.LongTensor>LongTensor</span></code> </td> <td> <div class=doc-md-description> <p>Token IDs of shape [batch_size, seq_len]</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>attention_mask</code> </td> <td> <code><span title=typing.Optional>Optional</span>[<span title=torch.Tensor>Tensor</span>]</code> </td> <td> <div class=doc-md-description> <p>Optional attention mask of shape [batch_size, seq_len]</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td> <code>states</code> </td> <td> <code><span title=typing.Optional>Optional</span>[<span title=list>list</span>]</code> </td> <td> <div class=doc-md-description> <p>Optional previous states list for each layer</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <p><span class=doc-section-title>Returns:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>output</code></td> <td> <code><span title=torch.Tensor>Tensor</span></code> </td> <td> <div class=doc-md-description> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> </div> </td> </tr> <tr class=doc-section-item> <td><code>new_states</code></td> <td> <code><span title=list>list</span></code> </td> <td> <div class=doc-md-description> <p>Updated states list for each layer</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/hopfield_network.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-236>236</a></span>
<span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span>
<span class=normal><a href=#__codelineno-0-242>242</a></span>
<span class=normal><a href=#__codelineno-0-243>243</a></span>
<span class=normal><a href=#__codelineno-0-244>244</a></span>
<span class=normal><a href=#__codelineno-0-245>245</a></span>
<span class=normal><a href=#__codelineno-0-246>246</a></span>
<span class=normal><a href=#__codelineno-0-247>247</a></span>
<span class=normal><a href=#__codelineno-0-248>248</a></span>
<span class=normal><a href=#__codelineno-0-249>249</a></span>
<span class=normal><a href=#__codelineno-0-250>250</a></span>
<span class=normal><a href=#__codelineno-0-251>251</a></span>
<span class=normal><a href=#__codelineno-0-252>252</a></span>
<span class=normal><a href=#__codelineno-0-253>253</a></span>
<span class=normal><a href=#__codelineno-0-254>254</a></span>
<span class=normal><a href=#__codelineno-0-255>255</a></span>
<span class=normal><a href=#__codelineno-0-256>256</a></span>
<span class=normal><a href=#__codelineno-0-257>257</a></span>
<span class=normal><a href=#__codelineno-0-258>258</a></span>
<span class=normal><a href=#__codelineno-0-259>259</a></span>
<span class=normal><a href=#__codelineno-0-260>260</a></span>
<span class=normal><a href=#__codelineno-0-261>261</a></span>
<span class=normal><a href=#__codelineno-0-262>262</a></span>
<span class=normal><a href=#__codelineno-0-263>263</a></span>
<span class=normal><a href=#__codelineno-0-264>264</a></span>
<span class=normal><a href=#__codelineno-0-265>265</a></span>
<span class=normal><a href=#__codelineno-0-266>266</a></span>
<span class=normal><a href=#__codelineno-0-267>267</a></span>
<span class=normal><a href=#__codelineno-0-268>268</a></span>
<span class=normal><a href=#__codelineno-0-269>269</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-236 name=__codelineno-0-236></a><span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-237 name=__codelineno-0-237></a>    <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-238 name=__codelineno-0-238></a>    <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-239 name=__codelineno-0-239></a>    <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
<a id=__codelineno-0-240 name=__codelineno-0-240></a>    <span class=n>states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>list</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-241 name=__codelineno-0-241></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>list</span><span class=p>]:</span>
<a id=__codelineno-0-242 name=__codelineno-0-242></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-243 name=__codelineno-0-243></a><span class=sd>    Forward pass for the full Hopfield Network model.</span>
<a id=__codelineno-0-244 name=__codelineno-0-244></a>
<a id=__codelineno-0-245 name=__codelineno-0-245></a><span class=sd>    Args:</span>
<a id=__codelineno-0-246 name=__codelineno-0-246></a><span class=sd>        input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-247 name=__codelineno-0-247></a><span class=sd>        attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-248 name=__codelineno-0-248></a><span class=sd>        states: Optional previous states list for each layer</span>
<a id=__codelineno-0-249 name=__codelineno-0-249></a>
<a id=__codelineno-0-250 name=__codelineno-0-250></a><span class=sd>    Returns:</span>
<a id=__codelineno-0-251 name=__codelineno-0-251></a><span class=sd>        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-252 name=__codelineno-0-252></a><span class=sd>        new_states: Updated states list for each layer</span>
<a id=__codelineno-0-253 name=__codelineno-0-253></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-254 name=__codelineno-0-254></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-255 name=__codelineno-0-255></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-256 name=__codelineno-0-256></a>
<a id=__codelineno-0-257 name=__codelineno-0-257></a>    <span class=c1># Initialize states if not provided</span>
<a id=__codelineno-0-258 name=__codelineno-0-258></a>    <span class=k>if</span> <span class=n>states</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
<a id=__codelineno-0-259 name=__codelineno-0-259></a>        <span class=n>states</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>]</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_layers</span>
<a id=__codelineno-0-260 name=__codelineno-0-260></a>
<a id=__codelineno-0-261 name=__codelineno-0-261></a>    <span class=n>new_states</span> <span class=o>=</span> <span class=p>[]</span>
<a id=__codelineno-0-262 name=__codelineno-0-262></a>
<a id=__codelineno-0-263 name=__codelineno-0-263></a>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>layer</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>):</span>
<a id=__codelineno-0-264 name=__codelineno-0-264></a>        <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_state</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>states</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
<a id=__codelineno-0-265 name=__codelineno-0-265></a>        <span class=n>new_states</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>new_state</span><span class=p>)</span>
<a id=__codelineno-0-266 name=__codelineno-0-266></a>
<a id=__codelineno-0-267 name=__codelineno-0-267></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-268 name=__codelineno-0-268></a>
<a id=__codelineno-0-269 name=__codelineno-0-269></a>    <span class=k>return</span> <span class=n>hidden_states</span><span class=p>,</span> <span class=n>new_states</span>
</code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel class="doc doc-heading"> <code>bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel</code> </h2> <div class="doc doc-contents first"> <p class="doc doc-class-bases"> Bases: <code><span title=torch.nn.Module>Module</span></code></p> <p>Full Sliding Window Transformer model stacking multiple transformer layers.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>vocab_size</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Vocabulary size for embedding layer</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>hidden_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Hidden dimension size</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_layers</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of transformer layers</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>num_heads</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Number of attention heads</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>window_size</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Size of the attention window</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>ff_dim</code> </td> <td> <code><span title=int>int</span></code> </td> <td> <div class=doc-md-description> <p>Feed-forward intermediate dimension</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>dropout</code> </td> <td> <code><span title=float>float</span></code> </td> <td> <div class=doc-md-description> <p>Dropout probability</p> </div> </td> <td> <code>0.1</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/sliding_window_transformer.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-182>182</a></span>
<span class=normal><a href=#__codelineno-0-183>183</a></span>
<span class=normal><a href=#__codelineno-0-184>184</a></span>
<span class=normal><a href=#__codelineno-0-185>185</a></span>
<span class=normal><a href=#__codelineno-0-186>186</a></span>
<span class=normal><a href=#__codelineno-0-187>187</a></span>
<span class=normal><a href=#__codelineno-0-188>188</a></span>
<span class=normal><a href=#__codelineno-0-189>189</a></span>
<span class=normal><a href=#__codelineno-0-190>190</a></span>
<span class=normal><a href=#__codelineno-0-191>191</a></span>
<span class=normal><a href=#__codelineno-0-192>192</a></span>
<span class=normal><a href=#__codelineno-0-193>193</a></span>
<span class=normal><a href=#__codelineno-0-194>194</a></span>
<span class=normal><a href=#__codelineno-0-195>195</a></span>
<span class=normal><a href=#__codelineno-0-196>196</a></span>
<span class=normal><a href=#__codelineno-0-197>197</a></span>
<span class=normal><a href=#__codelineno-0-198>198</a></span>
<span class=normal><a href=#__codelineno-0-199>199</a></span>
<span class=normal><a href=#__codelineno-0-200>200</a></span>
<span class=normal><a href=#__codelineno-0-201>201</a></span>
<span class=normal><a href=#__codelineno-0-202>202</a></span>
<span class=normal><a href=#__codelineno-0-203>203</a></span>
<span class=normal><a href=#__codelineno-0-204>204</a></span>
<span class=normal><a href=#__codelineno-0-205>205</a></span>
<span class=normal><a href=#__codelineno-0-206>206</a></span>
<span class=normal><a href=#__codelineno-0-207>207</a></span>
<span class=normal><a href=#__codelineno-0-208>208</a></span>
<span class=normal><a href=#__codelineno-0-209>209</a></span>
<span class=normal><a href=#__codelineno-0-210>210</a></span>
<span class=normal><a href=#__codelineno-0-211>211</a></span>
<span class=normal><a href=#__codelineno-0-212>212</a></span>
<span class=normal><a href=#__codelineno-0-213>213</a></span>
<span class=normal><a href=#__codelineno-0-214>214</a></span>
<span class=normal><a href=#__codelineno-0-215>215</a></span>
<span class=normal><a href=#__codelineno-0-216>216</a></span>
<span class=normal><a href=#__codelineno-0-217>217</a></span>
<span class=normal><a href=#__codelineno-0-218>218</a></span>
<span class=normal><a href=#__codelineno-0-219>219</a></span>
<span class=normal><a href=#__codelineno-0-220>220</a></span>
<span class=normal><a href=#__codelineno-0-221>221</a></span>
<span class=normal><a href=#__codelineno-0-222>222</a></span>
<span class=normal><a href=#__codelineno-0-223>223</a></span>
<span class=normal><a href=#__codelineno-0-224>224</a></span>
<span class=normal><a href=#__codelineno-0-225>225</a></span>
<span class=normal><a href=#__codelineno-0-226>226</a></span>
<span class=normal><a href=#__codelineno-0-227>227</a></span>
<span class=normal><a href=#__codelineno-0-228>228</a></span>
<span class=normal><a href=#__codelineno-0-229>229</a></span>
<span class=normal><a href=#__codelineno-0-230>230</a></span>
<span class=normal><a href=#__codelineno-0-231>231</a></span>
<span class=normal><a href=#__codelineno-0-232>232</a></span>
<span class=normal><a href=#__codelineno-0-233>233</a></span>
<span class=normal><a href=#__codelineno-0-234>234</a></span>
<span class=normal><a href=#__codelineno-0-235>235</a></span>
<span class=normal><a href=#__codelineno-0-236>236</a></span>
<span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span>
<span class=normal><a href=#__codelineno-0-242>242</a></span>
<span class=normal><a href=#__codelineno-0-243>243</a></span>
<span class=normal><a href=#__codelineno-0-244>244</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-182 name=__codelineno-0-182></a><span class=k>class</span><span class=w> </span><span class=nc>SlidingWindowTransformerModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
<a id=__codelineno-0-183 name=__codelineno-0-183></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-184 name=__codelineno-0-184></a><span class=sd>    Full Sliding Window Transformer model stacking multiple transformer layers.</span>
<a id=__codelineno-0-185 name=__codelineno-0-185></a>
<a id=__codelineno-0-186 name=__codelineno-0-186></a><span class=sd>    Args:</span>
<a id=__codelineno-0-187 name=__codelineno-0-187></a><span class=sd>        vocab_size (int): Vocabulary size for embedding layer</span>
<a id=__codelineno-0-188 name=__codelineno-0-188></a><span class=sd>        hidden_dim (int): Hidden dimension size</span>
<a id=__codelineno-0-189 name=__codelineno-0-189></a><span class=sd>        num_layers (int): Number of transformer layers</span>
<a id=__codelineno-0-190 name=__codelineno-0-190></a><span class=sd>        num_heads (int): Number of attention heads</span>
<a id=__codelineno-0-191 name=__codelineno-0-191></a><span class=sd>        window_size (int): Size of the attention window</span>
<a id=__codelineno-0-192 name=__codelineno-0-192></a><span class=sd>        ff_dim (int): Feed-forward intermediate dimension</span>
<a id=__codelineno-0-193 name=__codelineno-0-193></a><span class=sd>        dropout (float): Dropout probability</span>
<a id=__codelineno-0-194 name=__codelineno-0-194></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-195 name=__codelineno-0-195></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span>
<a id=__codelineno-0-196 name=__codelineno-0-196></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-197 name=__codelineno-0-197></a>        <span class=n>vocab_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-198 name=__codelineno-0-198></a>        <span class=n>hidden_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-199 name=__codelineno-0-199></a>        <span class=n>num_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-200 name=__codelineno-0-200></a>        <span class=n>num_heads</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-201 name=__codelineno-0-201></a>        <span class=n>window_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-202 name=__codelineno-0-202></a>        <span class=n>ff_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
<a id=__codelineno-0-203 name=__codelineno-0-203></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span>
<a id=__codelineno-0-204 name=__codelineno-0-204></a>    <span class=p>):</span>
<a id=__codelineno-0-205 name=__codelineno-0-205></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
<a id=__codelineno-0-206 name=__codelineno-0-206></a>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-207 name=__codelineno-0-207></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
<a id=__codelineno-0-208 name=__codelineno-0-208></a>
<a id=__codelineno-0-209 name=__codelineno-0-209></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span>
<a id=__codelineno-0-210 name=__codelineno-0-210></a>            <span class=n>SlidingWindowTransformerLayer</span><span class=p>(</span>
<a id=__codelineno-0-211 name=__codelineno-0-211></a>                <span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span>
<a id=__codelineno-0-212 name=__codelineno-0-212></a>                <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>,</span>
<a id=__codelineno-0-213 name=__codelineno-0-213></a>                <span class=n>window_size</span><span class=o>=</span><span class=n>window_size</span><span class=p>,</span>
<a id=__codelineno-0-214 name=__codelineno-0-214></a>                <span class=n>ff_dim</span><span class=o>=</span><span class=n>ff_dim</span><span class=p>,</span>
<a id=__codelineno-0-215 name=__codelineno-0-215></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span>
<a id=__codelineno-0-216 name=__codelineno-0-216></a>            <span class=p>)</span>
<a id=__codelineno-0-217 name=__codelineno-0-217></a>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)</span>
<a id=__codelineno-0-218 name=__codelineno-0-218></a>        <span class=p>])</span>
<a id=__codelineno-0-219 name=__codelineno-0-219></a>
<a id=__codelineno-0-220 name=__codelineno-0-220></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>)</span>
<a id=__codelineno-0-221 name=__codelineno-0-221></a>
<a id=__codelineno-0-222 name=__codelineno-0-222></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-223 name=__codelineno-0-223></a>        <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-224 name=__codelineno-0-224></a>        <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-225 name=__codelineno-0-225></a>        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-226 name=__codelineno-0-226></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
<a id=__codelineno-0-227 name=__codelineno-0-227></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-228 name=__codelineno-0-228></a><span class=sd>        Forward pass for the full Sliding Window Transformer model.</span>
<a id=__codelineno-0-229 name=__codelineno-0-229></a>
<a id=__codelineno-0-230 name=__codelineno-0-230></a><span class=sd>        Args:</span>
<a id=__codelineno-0-231 name=__codelineno-0-231></a><span class=sd>            input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-232 name=__codelineno-0-232></a><span class=sd>            attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-233 name=__codelineno-0-233></a>
<a id=__codelineno-0-234 name=__codelineno-0-234></a><span class=sd>        Returns:</span>
<a id=__codelineno-0-235 name=__codelineno-0-235></a><span class=sd>            output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-236 name=__codelineno-0-236></a><span class=sd>        &quot;&quot;&quot;</span>
<a id=__codelineno-0-237 name=__codelineno-0-237></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-238 name=__codelineno-0-238></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-239 name=__codelineno-0-239></a>
<a id=__codelineno-0-240 name=__codelineno-0-240></a>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
<a id=__codelineno-0-241 name=__codelineno-0-241></a>            <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>
<a id=__codelineno-0-242 name=__codelineno-0-242></a>
<a id=__codelineno-0-243 name=__codelineno-0-243></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-244 name=__codelineno-0-244></a>        <span class=k>return</span> <span class=n>hidden_states</span>
</code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=bsbr_extras.sliding_window_transformer.SlidingWindowTransformerModel.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> </h3> <div class="doc doc-contents "> <p>Forward pass for the full Sliding Window Transformer model.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td> <code>input_ids</code> </td> <td> <code><span title=torch.LongTensor>LongTensor</span></code> </td> <td> <div class=doc-md-description> <p>Token IDs of shape [batch_size, seq_len]</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td> <code>attention_mask</code> </td> <td> <code><span title=typing.Optional>Optional</span>[<span title=torch.Tensor>Tensor</span>]</code> </td> <td> <div class=doc-md-description> <p>Optional attention mask of shape [batch_size, seq_len]</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <p><span class=doc-section-title>Returns:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>output</code></td> <td> <code><span title=torch.Tensor>Tensor</span></code> </td> <td> <div class=doc-md-description> <p>Processed tensor of shape [batch_size, seq_len, hidden_dim]</p> </div> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/bsbr_extras/sliding_window_transformer.py</code></summary> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-222>222</a></span>
<span class=normal><a href=#__codelineno-0-223>223</a></span>
<span class=normal><a href=#__codelineno-0-224>224</a></span>
<span class=normal><a href=#__codelineno-0-225>225</a></span>
<span class=normal><a href=#__codelineno-0-226>226</a></span>
<span class=normal><a href=#__codelineno-0-227>227</a></span>
<span class=normal><a href=#__codelineno-0-228>228</a></span>
<span class=normal><a href=#__codelineno-0-229>229</a></span>
<span class=normal><a href=#__codelineno-0-230>230</a></span>
<span class=normal><a href=#__codelineno-0-231>231</a></span>
<span class=normal><a href=#__codelineno-0-232>232</a></span>
<span class=normal><a href=#__codelineno-0-233>233</a></span>
<span class=normal><a href=#__codelineno-0-234>234</a></span>
<span class=normal><a href=#__codelineno-0-235>235</a></span>
<span class=normal><a href=#__codelineno-0-236>236</a></span>
<span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span>
<span class=normal><a href=#__codelineno-0-242>242</a></span>
<span class=normal><a href=#__codelineno-0-243>243</a></span>
<span class=normal><a href=#__codelineno-0-244>244</a></span></pre></div></td><td class=code><div><pre><span></span><code><a id=__codelineno-0-222 name=__codelineno-0-222></a><span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span>
<a id=__codelineno-0-223 name=__codelineno-0-223></a>    <span class=bp>self</span><span class=p>,</span>
<a id=__codelineno-0-224 name=__codelineno-0-224></a>    <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>,</span>
<a id=__codelineno-0-225 name=__codelineno-0-225></a>    <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
<a id=__codelineno-0-226 name=__codelineno-0-226></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
<a id=__codelineno-0-227 name=__codelineno-0-227></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<a id=__codelineno-0-228 name=__codelineno-0-228></a><span class=sd>    Forward pass for the full Sliding Window Transformer model.</span>
<a id=__codelineno-0-229 name=__codelineno-0-229></a>
<a id=__codelineno-0-230 name=__codelineno-0-230></a><span class=sd>    Args:</span>
<a id=__codelineno-0-231 name=__codelineno-0-231></a><span class=sd>        input_ids: Token IDs of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-232 name=__codelineno-0-232></a><span class=sd>        attention_mask: Optional attention mask of shape [batch_size, seq_len]</span>
<a id=__codelineno-0-233 name=__codelineno-0-233></a>
<a id=__codelineno-0-234 name=__codelineno-0-234></a><span class=sd>    Returns:</span>
<a id=__codelineno-0-235 name=__codelineno-0-235></a><span class=sd>        output: Processed tensor of shape [batch_size, seq_len, hidden_dim]</span>
<a id=__codelineno-0-236 name=__codelineno-0-236></a><span class=sd>    &quot;&quot;&quot;</span>
<a id=__codelineno-0-237 name=__codelineno-0-237></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>input_ids</span><span class=p>)</span>
<a id=__codelineno-0-238 name=__codelineno-0-238></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoding</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-239 name=__codelineno-0-239></a>
<a id=__codelineno-0-240 name=__codelineno-0-240></a>    <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
<a id=__codelineno-0-241 name=__codelineno-0-241></a>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>)</span>
<a id=__codelineno-0-242 name=__codelineno-0-242></a>
<a id=__codelineno-0-243 name=__codelineno-0-243></a>    <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
<a id=__codelineno-0-244 name=__codelineno-0-244></a>    <span class=k>return</span> <span class=n>hidden_states</span>
</code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="March 29, 2025 08:54:51">March 29, 2025 08:54:51</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../assets/javascripts/bundle.c8b220af.min.js></script> </body> </html>