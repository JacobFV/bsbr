@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{katharopoulos2020transformers,
  title={Transformers are RNNs: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  journal={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  publisher={PMLR}
}

@article{ramsauer2021hopfield,
  title={Hopfield networks is all you need},
  author={Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and others},
  journal={arXiv preprint arXiv:2008.02217},
  year={2021}
}

@article{hua2022transformer,
  title={Transformer quality in linear time},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  journal={International Conference on Machine Learning},
  pages={9099--9117},
  year={2022},
  publisher={PMLR}
}

@article{hu2024deltanet,
  title={DeltaNet: Efficient attention with linear complexity},
  author={Hu, Shengding and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2406.06484},
  year={2024}
}

@article{hua2022gated,
  title={Gated attention unit: A novel attention mechanism for transformers},
  author={Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  journal={arXiv preprint arXiv:2202.10447},
  year={2022}
}

@article{hu2025streaming,
  title={Streaming models for efficient long-context reasoning},
  author={Hu, Shengding},
  journal={arXiv preprint arXiv:2403.xxxxx},
  year={2025}
} 